<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Consulting Statistician on Consulting Statistician</title>
    <link>/</link>
    <description>Recent content in Consulting Statistician on Consulting Statistician</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Week 9 : French Train Delays</title>
      <link>/post/tidytuesday2019/week9/week9/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week9/week9/</guid>
      <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(gganimate)
library(ggalluvial)
library(geomnet)
library(ggthemr)

# load the theme
ggthemr(&amp;quot;fresh&amp;quot;)

# load the data
small_trains &amp;lt;- read_csv(&amp;quot;small_trains.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week9&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26&#34;&gt;Data set&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;When Journey Average  Time increases the Total Number of Trips will decrease. Obviously. Code : &lt;a href=&#34;https://t.co/qY2l10OraS&#34;&gt;https://t.co/qY2l10OraS&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/ZZn0l7E7WZ&#34;&gt;pic.twitter.com/ZZn0l7E7WZ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1101039210298003456?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;network-graph-for-the-french-city-trains&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Network Graph for the French City Trains&lt;/h1&gt;
&lt;p&gt;Simply drawing a network graph to understand which french cities are mainly urban with capacity to trains arriving and leaving. Cities such as Paris Lyon, Paris Montparnasse, Paris Nord and Paris Est could be cities of concern with much for traffic with related to trains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(small_trains,aes(from_id=departure_station,to_id=arrival_station))+
          geom_net(directed = TRUE,labelon = TRUE,size=0.5,labelcolour = &amp;quot;black&amp;quot;,
                   repel = FALSE,ecolour = &amp;quot;grey70&amp;quot;, arrowsize = 0.75,
                   linewidth = 0.5,layout.alg = &amp;quot;fruchtermanreingold&amp;quot;)+
          theme_net()+
          ggtitle(&amp;quot;Network Graph Showing from City to City of French Trains&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/network%20graph-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;paris-montparnasse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Paris Montparnasse&lt;/h1&gt;
&lt;p&gt;Let me focus on Montparnasse which has lot of trains coming towards and leaving outwards according to the network map. Not all are cities of France according to my observations, where I can see Madrid, Zurich and Barcelona.&lt;/p&gt;
&lt;div id=&#34;chosen-city-with-total-number-of-trips&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Total Number of Trips&lt;/h2&gt;
&lt;p&gt;Total number of trips from Paris Montparnasse station to other cities is noted here. Four years of data with more accuracy by months is considered in this plot. There is clear variation in this data for cities.&lt;/p&gt;
&lt;p&gt;Close to 800 trips have been recorded towards the Bordeaux St Jean city but not clearly the same pattern for all years or months as well. Further, St Malo city has the lowest amount of trips compared to other cities in all fours but follows a centered pattern around the 100 mark.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y=total_num_trips,color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       scale_y_continuous(breaks = seq(0,800,100),labels=seq(0,800,100))+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Total Number of Trips&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Stations&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20total%20num%20of%20trips-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chosen-city-with-average-journey-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Average Journey Time&lt;/h2&gt;
&lt;p&gt;Except year 2017 all cities has similar and centered data points. In this exceptional year of 2017 we can see a difference between the first six months and rest. Where most of the Average journey times have been reduced, it is clear according to year 2018 points.&lt;/p&gt;
&lt;p&gt;City of Toulouse Matabiau has the highest Average Journey Time, while lowest time goes to the city of Le Mans. So what happened after mid of year 2017.&lt;/p&gt;
&lt;p&gt;Maximum Average Journey time before mid of year 2017 is close to 325 but after this period it is centered around 275. The Minimum Average Journey time before mid of year 2017 and after also it is close to 50.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y=journey_time_avg,color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       scale_y_continuous(breaks=seq(0,350,25),labels=seq(0,350,25))+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Average Journey Time&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Station&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20Journey%20time%20average-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chosen-city-with-average-delay-with-all-departing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Average Delay with All Departing&lt;/h2&gt;
&lt;p&gt;There is no clear pattern in perspective of months or years because data points are spread all over the place. Yet there is an odd occurring of negative values for average delay with all departing for some cities after mid of year 2017.&lt;/p&gt;
&lt;p&gt;Well none of these negative values does not exceed -2.5, where the maximum average delay in all departing is close to 5.5. I believe unit measured is in minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y= avg_delay_all_departing,color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       scale_y_continuous(breaks=seq(-2.5,5.5,0.5),labels=seq(-2.5,5.5,0.5))+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       geom_hline(yintercept = 0,color=&amp;quot;red&amp;quot;)+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Average Delay All Departing&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Station&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20avg%20delay%20all%20departing-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chosen-city-with-average-delay-with-all-arriving&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Average Delay with All Arriving&lt;/h2&gt;
&lt;p&gt;Highest delay could occur close to 18 minutes for average delay in Arriving and the lowest is close to -3. only in 2018 we see such negative values. These negative values occurs for the city of St Malo. Also there is no clear pattern in any city with relative to year or months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y=avg_delay_all_arriving,color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       geom_hline(yintercept = 0,color=&amp;quot;red&amp;quot;)+
       scale_y_continuous(breaks=seq(-3,18),labels=seq(-3,18))+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Average Delay All Arriving&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Station&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20avg%20delay%20all%20arriving-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chosen-city-with-number-of-late-departures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Number of Late Departures&lt;/h2&gt;
&lt;p&gt;Number of Late departures over the years increases for all cities. It is more clear for Bordeaux St Jean where the counts go beyond 150 and close to 200 in the year of 2018, but in 2015 the highest point is close to 75 for the same city.&lt;/p&gt;
&lt;p&gt;St Malo has the lowest number of late departures where it fails to reach the count of 30 in all four years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y=num_late_at_departure,
           color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       scale_y_continuous(breaks=seq(0,200,25),labels=seq(0,200,25))+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Number of Lates at Departure&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Station&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20num%20late%20at%20departure-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chosen-city-with-number-of-late-arrivals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chosen City with Number of Late Arrivals&lt;/h2&gt;
&lt;p&gt;City of St Malo has the lowest amount of late arrivals for all fours in general. Most amount of highest late arrivals occur in the city of Bordeaux St Jean. In year 2015 most of these data points are centered towards their specific values. In the next few years we can see that is not the case and they are with a lot of variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(small_trains,departure_station==&amp;quot;PARIS MONTPARNASSE&amp;quot;),
       aes(x=str_wrap(arrival_station,20),y=num_arriving_late,
           color=month))+
       geom_jitter()+coord_flip()+ labs(color=&amp;quot;Month&amp;quot;)+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       scale_y_continuous(breaks=seq(0,200,20),labels=seq(0,200,20))+
       xlab(&amp;quot;Arrival Station&amp;quot;)+ylab(&amp;quot;Number of Lates at Arriving&amp;quot;)+
       ggtitle(&amp;quot;Paris Montparnasse and its arrival Station&amp;quot; ,subtitle =&amp;quot;Year :{frame_time}&amp;quot;)

animate(p,nframes=4,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Paris%20Montparnasse%20num%20arriving%20late-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;departure-station-with-average-journey-time-and-total-number-of-trips&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Departure Station with Average Journey Time and Total Number of Trips&lt;/h1&gt;
&lt;p&gt;Summary of this below plot is that when Average Journey Time increases clearly Total Number of Trips will decrease.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(small_trains,aes(x=journey_time_avg,y=total_num_trips,color=month))+
      geom_point()+transition_states(departure_station)+labs(color=&amp;quot;Month&amp;quot;)+
      ggtitle(&amp;quot;Average Journey Time and Total Number of Trips&amp;quot;,
              subtitle=&amp;quot;Departure Station : {closest_state}&amp;quot;)+
      scale_y_continuous(breaks=seq(0,900,50),labels=seq(0,900,50))+
      scale_x_continuous(breaks=seq(0,500,50),labels=seq(0,500,50))+  
      xlab(&amp;quot;Average Journey Time&amp;quot;)+ylab(&amp;quot;Total Number of Trips&amp;quot;)+
      shadow_mark()

animate(p,nframes=59,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/departure%20station%20with%20journey%20time%20avg%20and%20total%20num%20trips-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;departure-station-with-average-delay-all-departing-and-number-of-late-at-departures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Departure Station with Average Delay All Departing and Number of Late at Departures&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(small_trains,aes(x=num_late_at_departure,y=avg_delay_all_departing,
                           color=month))+
      geom_point()+transition_states(departure_station)+labs(color=&amp;quot;Month&amp;quot;)+
      ggtitle(&amp;quot;Average Delay at All Departing and Number of Lates at Departure&amp;quot;,
              subtitle=&amp;quot;Departure Station : {closest_state}&amp;quot;)+
      geom_vline(xintercept = 0,color=&amp;quot;red&amp;quot;)+
      geom_hline(yintercept = 0,color=&amp;quot;red&amp;quot;)+
      scale_y_continuous(breaks=seq(-5,175,5),labels=seq(-5,175,5))+
      scale_x_continuous(breaks=seq(0,500,50),labels=seq(0,500,50))+  
      xlab(&amp;quot;Number of Lates at Departure&amp;quot;)+ylab(&amp;quot;Average Delays at all Departing&amp;quot;)+
      shadow_mark()

animate(p,nframes=59,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/departure%20station%20with%20Number%20of%20late%20and%20average%20Delay-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;departure-station-with-average-delay-all-arriving-and-number-of-arriving-late&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Departure Station with Average Delay All Arriving and Number of Arriving Late&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(small_trains,aes(x=num_arriving_late,y=avg_delay_all_arriving,
                           color=month))+
      geom_point()+transition_states(departure_station)+labs(color=&amp;quot;Month&amp;quot;)+
      ggtitle(&amp;quot;Average Delay at All Arriving and Number of Lates at Arriving&amp;quot;,
              subtitle=&amp;quot;Departure Station : {closest_state}&amp;quot;)+
      geom_vline(xintercept = 0,color=&amp;quot;red&amp;quot;)+
      geom_hline(yintercept = 0,color=&amp;quot;red&amp;quot;)+
      scale_y_continuous(breaks=seq(-150,40,5),labels=seq(-150,40,5))+
      scale_x_continuous(breaks=seq(0,250,25),labels=seq(0,250,25))+  
      xlab(&amp;quot;Number of Lates at Arriving&amp;quot;)+ylab(&amp;quot;Average Delays at all Arriving&amp;quot;)+
      shadow_mark()

animate(p,nframes=59,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/departure%20station%20with%20Number%20of%20Arriving%20late%20and%20average%20Delay%20arriving-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;delayed-cause-and-delayed-number&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Delayed Cause and Delayed Number&lt;/h1&gt;
&lt;p&gt;Delays caused by the travelers is very less likely to happen, where most of these delays are caused by external causes. Other causes such as rolling stock and rail infrastructure also has effect but not as much from external cause. Station management has limited amount of affect but higher than travelers in causes for delaying trains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_trains %&amp;gt;%
    mutate(delay_cause = str_remove(delay_cause,&amp;quot;delay_cause_&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=delay_cause,y=delayed_number))+
      xlab(&amp;quot;Delay Cause&amp;quot;)+ylab(&amp;quot;Delayed Number&amp;quot;)+
      ggtitle(&amp;quot;Delayed Causes and Delayed Number as percentage&amp;quot;)+
      geom_jitter()+coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week9/index_files/figure-html/Delayed%20No%20and%20Delayed%20cause-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 8 : Phds Awarded in USA between 2008 and 2017</title>
      <link>/post/tidytuesday2019/week8/week8/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week8/week8/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#broad-field&#34;&gt;Broad Field&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#all-fields&#34;&gt;All fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropping-psychology-and-social-sciences&#34;&gt;Dropping Psychology and Social Sciences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#major-field&#34;&gt;Major Field&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#major-fields-with-box-plot&#34;&gt;Major Fields with Box plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#major-fields-without-psychology-but-still-in-a-boxplot&#34;&gt;Major Fields without Psychology but still in a Boxplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mathematics-and-computer-sciences&#34;&gt;Mathematics and Computer Sciences&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mathematics-and-computer-science-as-a-broad-field&#34;&gt;Mathematics and Computer Science as a Broad field&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mathematics-and-computer-science-as-a-major-field&#34;&gt;Mathematics and Computer Science as a Major Field&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#major-field-of-mathematics-and-computer-science-but-now-all-fields&#34;&gt;Major Field of Mathematics and Computer Science but now all Fields&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#major-field-field-and-year-for-mathematics-and-computer-sciences&#34;&gt;Major Field, Field and Year For Mathematics and Computer Sciences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(tidyverse)
library(ggthemr)
library(readr)
library(gganimate)
library(ggridges)
library(ggalluvial)

ggthemr(&amp;quot;flat&amp;quot;)

#load the data
phdlist &amp;lt;- read_csv(&amp;quot;phd_by_field.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Five variables are representing this entire data-set and three of them are factors while one column represents the year and the final column is for counts. There are few missing values. We can focus on Phds awarded from 2008 to 2017 in perspective of Broad Field, Major Field and Field.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-19&#34;&gt;Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week8&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Computer science PhDs more than others is that the boom of AI. beginning from 2008 itself !!!!!. My first ridge plot and alluvial diagram.&lt;br&gt;Code: &lt;a href=&#34;https://t.co/TtWLzNk1ga&#34;&gt;https://t.co/TtWLzNk1ga&lt;/a&gt;     &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/AhB01IPuv6&#34;&gt;pic.twitter.com/AhB01IPuv6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1097868689385418752?ref_src=twsrc%5Etfw&#34;&gt;February 19, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Broad Field and Major Field are considered specially but not the column Field as it would be difficult to plot based on the amount of categories.&lt;/p&gt;
&lt;div id=&#34;broad-field&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Broad Field&lt;/h1&gt;
&lt;p&gt;Broad field has 7 categories and clearly Psychology and social sciences has produced more than 4000 Phds each and every year. Which is twice comparing to other categories. If we drop Psychology and Social sciences, now the changes over the years for other categories are clear.&lt;/p&gt;
&lt;p&gt;There are more outliers in the field of Life sciences where some programs produce more than 1000 Phds each year comparatively to the rest categories. Except Life sciences other categories tend to behave rarely as above producing more than 1000 Phds.&lt;/p&gt;
&lt;p&gt;Engineering field has the lowest distribution with relative to other categories according to the box plot in every year.&lt;/p&gt;
&lt;div id=&#34;all-fields&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;All fields&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(phdlist,aes(x=str_wrap(broad_field,20),y=n_phds))+
          geom_boxplot()+
          xlab(&amp;quot;Broad Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Broad Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
          theme(axis.text.x = element_text(hjust=1,angle = 90))

animate(p,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/Broad%20field%20boxplot-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dropping-psychology-and-social-sciences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dropping Psychology and Social Sciences&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(phdlist,broad_field != &amp;quot;Psychology and social sciences&amp;quot;),
          aes(x=str_wrap(broad_field,20),y=n_phds))+
          geom_boxplot()+
          xlab(&amp;quot;Broad Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Broad Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
          theme(axis.text.x = element_text(hjust=1,angle = 90))

animate(p,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/Broad%20field%20boxplot%20without%20psy%20and%20soc%20sciences-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;major-field&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Major Field&lt;/h1&gt;
&lt;p&gt;Focus now solely switches towards the Major Field column and here also we can see the strong outlier in Psychology over the years. Further, Physics and Astronomy field also has a very strong outlier where over the years it reaches 2000 Phds.&lt;/p&gt;
&lt;p&gt;Without dropping Psychology we can see the odd behavior from the fields “Education Research”, “Economics” and “Computer and Information Sciences”. Specially the gradual decrease of “Education Research” from 2008 to 2017.&lt;/p&gt;
&lt;p&gt;Also in “Computer and Information Sciences” field there is an odd increase in 2012.&lt;/p&gt;
&lt;p&gt;After dropping the “Psychology” field we can now clearly see how other Major fields behave over the years.&lt;/p&gt;
&lt;div id=&#34;major-fields-with-box-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Major Fields with Box plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(phdlist,aes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+
          geom_boxplot()+coord_flip()+
          xlab(&amp;quot;Major Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Major Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
          theme(axis.text.x = element_text(hjust=1,angle = 90),
                legend.position = &amp;quot;bottom&amp;quot;)+
          labs(fill=&amp;quot;Broad Field&amp;quot;)

animate(p,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/major%20field%20boxplot-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;major-fields-without-psychology-but-still-in-a-boxplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Major Fields without Psychology but still in a Boxplot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q&amp;lt;-ggplot(subset(phdlist,major_field != &amp;quot;Psychology&amp;quot;),
          aes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+
          geom_boxplot()+coord_flip()+
          xlab(&amp;quot;Major Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Major Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
          theme(axis.text.x = element_text(hjust=1,angle = 90),
                legend.position = &amp;quot;bottom&amp;quot;)+
          labs(fill=&amp;quot;Broad Field&amp;quot;)

animate(q,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/Major%20field%20without%20psy-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematics-and-computer-sciences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mathematics and Computer Sciences&lt;/h1&gt;
&lt;p&gt;I am a Statistics student with a glimpse of Computer science background, therefore my next intention is to focus on the Broad Field “Mathematics and Compute Sciences”.&lt;/p&gt;
&lt;div id=&#34;mathematics-and-computer-science-as-a-broad-field&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematics and Computer Science as a Broad field&lt;/h2&gt;
&lt;p&gt;Mathematics and Statistics has a gradual increase until 2012, but wavers higher and lower in the next years, but in 2016 there is a sudden increase of which would lead to around 700 Phds awarded. Next year this decreases to 500 Phds.&lt;/p&gt;
&lt;p&gt;Comparing the 2 major fields “Computer and Information Sciences” with “Mathematics and Statistics” indicate the strong gap between them awarding Phds. “Computer and Information Sciences” award more than twice the amount of Phds what “Mathematics and Statistics” award each year.&lt;/p&gt;
&lt;p&gt;“Computer and Information Sciences” also hold a clear pattern with the Phds awarded.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(phdlist,broad_field == &amp;quot;Mathematics and computer sciences&amp;quot;) %&amp;gt;%
      
ggplot(.,aes(x=factor(year),y=n_phds,fill=major_field))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,position = &amp;quot;dodge&amp;quot;)+
       theme(legend.position = &amp;quot;bottom&amp;quot;)+
       xlab(&amp;quot;Major Field&amp;quot;)+ylab(&amp;quot;Number of Phds&amp;quot;)+
       ggtitle(&amp;quot;Number of Phds awarded under Mathematics and CS&amp;quot;,
               subtitle = &amp;quot;Year : 2008 to 2017&amp;quot;)+
      scale_y_continuous(breaks=seq(0,1700,100),labels=seq(0,1700,100))+
          labs(fill=&amp;quot;Major Field&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/mathematics%20and%20cs%20bar%20chart-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematics-and-computer-science-as-a-major-field&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematics and Computer Science as a Major Field&lt;/h2&gt;
&lt;p&gt;Box plot indicates the clear variation among these two major fields over years which could be used for comparison. The sudden peak in year 2012 for “Computer and Information Sciences” interests me alot. It should be noted that “Mathematics and Statistics” has more outliers than “Computer and Information Sciences”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(phdlist,broad_field == &amp;quot;Mathematics and computer sciences&amp;quot;),
          aes(x=str_wrap(major_field,20),y=n_phds))+
          geom_boxplot()+
          xlab(&amp;quot;Major Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Major Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)

animate(p,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/major%20field%20boxplot%20with%20maths%20and%20cs-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Below is a ridge plot to describe the same thing which would clearly indicate the data spread.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(subset(phdlist,broad_field == &amp;quot;Mathematics and computer sciences&amp;quot;),
          aes(y=str_wrap(major_field,20),x=n_phds))+
          geom_density_ridges()+
          xlab(&amp;quot;No of Phds&amp;quot;)+ ylab(&amp;quot;Major Field&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;)+
          ggtitle(&amp;quot;Ridge plot for Major Fields in Mathematics and Computer Sciences&amp;quot;,
                  subtitle = &amp;quot;Year : 2008 to 2017&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/ridge%20major%20fields-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;major-field-of-mathematics-and-computer-science-but-now-all-fields&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Major Field of Mathematics and Computer Science but now all Fields&lt;/h2&gt;
&lt;p&gt;Considering the sub categories of the chosen broad field in a box plot did not work quite well, but clearly this plot indicates the Computer Science Phds being awarded with highest amount over the years. Would that mean the boom of Artificial Intelligence in Computer Science.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(phdlist,broad_field == &amp;quot;Mathematics and computer sciences&amp;quot;),
          aes(x=str_wrap(field,20),y=n_phds,fill=major_field))+
          geom_boxplot()+coord_flip()+
          xlab(&amp;quot;Field&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
          transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
          ggtitle(&amp;quot;Boxplot to Number of Phds in Field&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;)+
          labs(fill=&amp;quot;Major Field&amp;quot;)

animate(p,nframes=9,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/field%20boxplot%20with%20maths%20and%20cs-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;To get a clear view here is the ridge plot, where Computer Science is very strong for “Computer and Information Sciences”. It should be noted though there is only three other fields in this major field which are “Information Science systems”, “Computer and Information Sciences, other” and “Computer and Information sciences, general”.&lt;/p&gt;
&lt;p&gt;More than 10 fields for the Major field “Mathematics and Statistics”, where higher counts occur to “Statistics(Mathematics)”, “Applied mathematics” and “Mathematics and Statistics,general”. Still non of these fields have passed the 1000 Phds awarded mark.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(subset(phdlist,broad_field == &amp;quot;Mathematics and computer sciences&amp;quot;),
          aes(y=str_wrap(field,20),x=n_phds,fill=major_field))+
          geom_density_ridges()+
          xlab(&amp;quot;No of Phds&amp;quot;)+ ylab(&amp;quot;Field&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;)+
          ggtitle(&amp;quot;Ridge plot for Fields in Mathematics and Computer Sciences&amp;quot;,
                  subtitle = &amp;quot;Year : 2008 to 2017&amp;quot;)+
          labs(fill=&amp;quot;Major Field&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/ridge%20plot%20fields-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;major-field-field-and-year-for-mathematics-and-computer-sciences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Major Field, Field and Year For Mathematics and Computer Sciences&lt;/h1&gt;
&lt;p&gt;Finally an alluvial diagram just to look at the impact of Computer science field with respective to each year, which is very strong.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(subset(phdlist,broad_field==&amp;quot;Mathematics and computer sciences&amp;quot;)) %&amp;gt;%
           na.omit() %&amp;gt;%
ggplot(.,aes(axis2=factor(str_wrap(year,10)), axis1= factor(str_wrap(major_field,10)), 
             axis3= factor(field), y=as.numeric(n_phds)))+
       scale_x_discrete(limits=c(&amp;quot;Major Field&amp;quot;,&amp;quot;Year&amp;quot;,&amp;quot;Field&amp;quot;),expand = c(.05, .05))+
       geom_alluvium(aes(fill=factor(major_field)),width = 1/2)+
       geom_stratum(width=1/2,fill=&amp;quot;white&amp;quot;,color=&amp;quot;grey&amp;quot;)+
       geom_text(stat = &amp;quot;stratum&amp;quot;, label.strata = TRUE)+
       theme(legend.position = &amp;quot;bottom&amp;quot;)+ylab(&amp;quot;No of Phds&amp;quot;)+
       ggtitle(&amp;quot;Major Field and Fields For Years 2008 to 2017&amp;quot;,
               subtitle=&amp;quot;Mathematics and Computer Science&amp;quot;)+
          labs(fill=&amp;quot;Major Field&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week8/index_files/figure-html/broad%20and%20major%20and%20field-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sri Lanka and its affect on/in Journal Articles </title>
      <link>/post/slandjournal/slandjournal/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/slandjournal/slandjournal/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#packages-and-ideas&#34;&gt;Packages and Ideas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#country-information&#34;&gt;Country Information&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ranking-of-sri-lanka-from-1996-to-2017&#34;&gt;Ranking of Sri Lanka from 1996 to 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ranking-of-south-asian-region-countries-from-1996-to-2017&#34;&gt;Ranking of South Asian Region Countries from 1996 to 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#documents-citable-documents-and-self-citations-from-sri-lanka-journals&#34;&gt;Documents , Citable Documents and Self Citations from Sri Lanka Journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#citations-per-document-from-sri-lankan-journals&#34;&gt;Citations per Document from Sri Lankan Journals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#journals-from-sri-lanka&#34;&gt;Journals from Sri Lanka&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rank-changes-for-sri-lankan-journals&#34;&gt;Rank changes for Sri Lankan Journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sjr-values-changing-for-sri-lankan-journals&#34;&gt;SJR values changing for Sri Lankan Journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references-per-document-changing-for-sri-lankan-journals&#34;&gt;References per Document changing for Sri Lankan Journals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#publishers-and-journals-from-sri-lanka&#34;&gt;Publishers and Journals from Sri Lanka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#total-documents-and-references-for-journals-from-sri-lanka&#34;&gt;Total Documents and References for Journals from Sri Lanka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#three-year-time-period-for-journals-from-sri-lanka&#34;&gt;Three year time period for Journals from Sri Lanka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#journals-and-categories&#34;&gt;Journals and categories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;“SCImago Journal and Country Rank provides valuable estimates of academic journals’ prestige.”, this is according to its GitHub repository which could be found &lt;a href=&#34;https://github.com/ikashnitsky/sjrdata&#34;&gt;here.&lt;/a&gt; I completely agree with this statement, last year(2018) while searching for journals to publish a research of my interest and contribution this &lt;a href=&#34;https://www.scimagojr.com/&#34;&gt;website&lt;/a&gt; came in useful comparative to most of my Google searches.&lt;/p&gt;
&lt;p&gt;It is crucial to find a journal which summarizes the research work we intend to contribute with more relative information such as ranking, h-index, citations and etc which could be compared for our benefit. Usually people tend to collect information by themselves and might miss a valuable and more suitable opportunity to publish because they did not know where to find them. That will not occur if you use SciMagoJr.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;packages-and-ideas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Packages and Ideas&lt;/h1&gt;
&lt;p&gt;As a Researcher from Sri Lanka I am much more interested in how we as a country has made progress in related to journals or research publishing in perspective of our neighboring countries and fields of research interest.&lt;/p&gt;
&lt;p&gt;The package sjrdata has three data-sets and they provide information regarding to journals which is mostly seeked by researchers to find an appropriate place to publish their research or find places which would benefit them in finding a unique research problem.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data
library(sjrdata)

# Packages
library(tidyverse)
library(gganimate)
library(ggrepel)
library(magrittr)
library(lubridate)
library(splitstackshape)
library(qdap)
library(ggthemr)
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The three data-sets in concern are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;sjr_countries - Contains information regarding h_index over the years for countries and their ranks with other information related to documents.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sjr_journals - Records for type of document with their basic information which can be connected to country of origin, publisher and fields of interest and much more.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sjr_countries_1996_2017 - Similar to sjr_countries but this is an accumulation of the data-set which would indicate the overall performance as a country from 1996 to 2017.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;country-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Country Information&lt;/h1&gt;
&lt;p&gt;This section will simply scrutinize how Sri Lanka over the years has performed and it should be noted that only the ranks of South Asian Region countries will be compared for the purpose of understanding the progress as a whole community.&lt;/p&gt;
&lt;div id=&#34;ranking-of-sri-lanka-from-1996-to-2017&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking of Sri Lanka from 1996 to 2017&lt;/h2&gt;
&lt;p&gt;Sri Lanka(SL) had its ups and downs in the rankings where the worst place was to be in 85th and the best place to be in 75th. As of in 2017 it has reached 78th which is good considering the previous three years(2014-2016) the ranks were higher.&lt;/p&gt;
&lt;p&gt;In 1996 SL was holding 85th and this occurs again in the years 2011 and 2014. Also it reached the 75th ranking position in year 2007. I sincerely hope that in the coming years SL would reach better ranks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(sjr_countries,country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
  mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=year,y=rank,label=rank))+
  geom_point()+geom_line()+
  geom_text_repel()+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rank&amp;quot;)+
  ggtitle(&amp;quot;Rank of Sri Lanka Changing from 1996 to 2017&amp;quot;)+
  theme(axis.text.x =element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/sjr%20Sri%20Lanka%20and%20rank-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ranking-of-south-asian-region-countries-from-1996-to-2017&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ranking of South Asian Region Countries from 1996 to 2017&lt;/h2&gt;
&lt;p&gt;India is way ahead of other seven countries in the South Asian region where it has gradually moved from 13th position to 5th over the years. Sri Lanka is currently behind India, Pakistan and Nepal.&lt;/p&gt;
&lt;p&gt;Further Bhutan, Maldives and Afghanistan have ranks above 130 for the period of time given, while Maldives has stayed in the range of 213 and 167. Afghanistan and Bhutan have made improvements where now(2017) they are holding ranks of 136 and 159 respectively while in 1996 they had ranks of 207 and 186. These three countries tend to oscillate very frequently among ranks than the other 5 countries in the region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(sjr_countries,
       country ==&amp;quot;Sri Lanka&amp;quot; | country==&amp;quot;India&amp;quot;| country==&amp;quot;Pakistan&amp;quot;|
       country ==&amp;quot;Nepal&amp;quot; | country== &amp;quot;Maldives&amp;quot; | country ==&amp;quot;Afghanistan&amp;quot;|
       country == &amp;quot;Bhutan&amp;quot; | country==&amp;quot;Bangladesh&amp;quot;) %&amp;gt;%
       mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=year,y=rank,label=rank,color=country))+
  geom_point()+geom_line()+
  geom_text_repel()+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rank&amp;quot;)+
  ggtitle(&amp;quot;Rank of South Asian Region Changing from 1996 to 2017&amp;quot;)+
  theme(axis.text.x =element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/sjr%20South%20Asia%20and%20rank-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;documents-citable-documents-and-self-citations-from-sri-lanka-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Documents , Citable Documents and Self Citations from Sri Lanka Journals&lt;/h2&gt;
&lt;p&gt;Over the years Self citations were higher than citable documents and documents in counts, but this is not the case after the year 2012. Self citations reached its highest peak of 1331 in 2012 but in the next few years they gradually decreased to 175 counts in year 2017. Even though in 1996 the self citations count was 275 which is higher than current counts in 2017 for self citation.&lt;/p&gt;
&lt;p&gt;This is not the case for citable documents and documents where they start with counts of close to 200. Over the years they rapidly increase and reach the close to thousand mark 2012. This form of improvement continues further and reaches the close to two thousand mark in the year 2017, which I feel is impressive. The gap between documents and citable documents is very thin in the early 2000s but while counts increase the gap also considerably increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sjr_countries%&amp;gt;%
  gather(citing,values,&amp;quot;documents&amp;quot;,&amp;quot;citable_documents&amp;quot;,&amp;quot;self_citations&amp;quot;) %&amp;gt;%
  subset(country==&amp;quot;Sri Lanka&amp;quot;,select=c(citing,values,year)) %&amp;gt;%
  mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=year,y=values,label=values,color=citing,shape=citing))+
  geom_point()+geom_line()+
  geom_text_repel()+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Counts&amp;quot;)+
  scale_x_continuous(breaks=seq(1996,2017),labels=seq(1996,2017))+
  scale_y_continuous(breaks=seq(0,2000,100),labels=seq(0,2000,100))+
  ggtitle(&amp;quot;Document Counts Changing from 1996 to 2017 for Sri Lanka&amp;quot;)+
  theme(axis.text.x =element_text(angle = 90, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/sjr%20Sri%20Lanka%20and%20documents%20citable%20documents%20self%20citations-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citations-per-document-from-sri-lankan-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Citations per Document from Sri Lankan Journals&lt;/h2&gt;
&lt;p&gt;Citations per document has a steady rise from 14.28 to 24.29 in the years of 1996 to 2000, while there is a sudden decline next year it manages to increases and reach the peak point of 29.29 in year 2003. After the millenium we can clearly see a clear steep in numbers over the next two decades, where now in year 2017 the citations per document has dropped to 0.61. It is very alarming even though we have alot of citable documents in the same year according to the previous plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sjr_countries %&amp;gt;%
   subset(country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
   mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=year,y=citations_per_document,
          label=citations_per_document))+
  geom_point()+geom_line()+
  geom_text_repel()+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Citations Per Document&amp;quot;)+
  scale_x_continuous(breaks=seq(1996,2017),labels=seq(1996,2017))+
  scale_y_continuous(breaks=seq(0,30),labels=seq(0,30))+
  ggtitle(&amp;quot;Citations Per Document from 1996 to 2017 for Sri Lanka&amp;quot;)+
  theme(axis.text.x =element_text(angle = 90, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/sjr%20citations%20per%20document-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;journals-from-sri-lanka&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Journals from Sri Lanka&lt;/h1&gt;
&lt;p&gt;9 titles are listed and considered here from Sri Lanka where some began publishing recently.&lt;/p&gt;
&lt;div id=&#34;rank-changes-for-sri-lankan-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rank changes for Sri Lankan Journals&lt;/h2&gt;
&lt;p&gt;Journals which started close to year 2000 have ranks close to ten thousand, while recent publications has ranks leading upto the rank of higher than thirty thousand as well. Journal of the National Science Foundation of Sri Lanka and Ceylon Medical Journal has published more than other publications and they do have better rankings through out the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(sjr_journals,country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
   mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
   ggplot(.,aes(x=str_wrap(factor(title),30),y=rank,color=year))+
         geom_jitter()+coord_flip()+
         shadow_mark()+
         transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
         xlab(&amp;quot;Title of Journals&amp;quot;)+ylab(&amp;quot;Rank&amp;quot;)+
         ggtitle(&amp;quot;Ranks Changing over time for Sri Lankan Journals from 1999 to 2017&amp;quot;,
                 subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)

animate(p,fps=1,nframes=18)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/rank%20changes-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sjr-values-changing-for-sri-lankan-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SJR values changing for Sri Lankan Journals&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(sjr_journals,country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
   mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
   ggplot(.,aes(x=str_wrap(factor(title),30),y=sjr,color=year,shape=sjr_best_quartile))+
         geom_jitter()+coord_flip()+
         shadow_mark()+
         labs(shape=&amp;quot;SJR Best Q&amp;quot;)+
         transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
         xlab(&amp;quot;Title of Journals&amp;quot;)+ylab(&amp;quot;SJR values&amp;quot;)+
         ggtitle(&amp;quot;SJR values Changing over time for Sri Lankan Journals from 1999 to 2017&amp;quot;,
                 subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
         theme(legend.position = &amp;quot;bottom&amp;quot;)

animate(p,fps=1,nframes=18)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/sjr%20changes-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references-per-document-changing-for-sri-lankan-journals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References per Document changing for Sri Lankan Journals&lt;/h2&gt;
&lt;p&gt;Highest amount of reference per document is close to 100 while the minimum is close to zero. There is no specific pattern over the years which indicate an increase or decrease.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(sjr_journals,country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
   mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
   ggplot(.,aes(x=str_wrap(factor(title),30),y=ref_doc,color=year))+
         geom_jitter()+coord_flip()+
         shadow_mark()+
         transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
         xlab(&amp;quot;Title of Journals&amp;quot;)+ylab(&amp;quot;References per Document&amp;quot;)+
         ggtitle(&amp;quot;References per Document Changing over time for Sri Lankan Journals from 1999 to 2017&amp;quot;,
                 subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)

animate(p,fps=1,nframes=18)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/ref%20per%20doc%20changes-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publishers-and-journals-from-sri-lanka&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Publishers and Journals from Sri Lanka&lt;/h2&gt;
&lt;p&gt;Without any hesitation I could say most of these publications are from the field of medicine and institutions related to medicine. There are some publishers such as Internations Centre for Ethnic Studies and International Irrigation Managament Institue, but they have published very few publications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(sjr_journals,country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
   mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
   ggplot(.,aes(x=str_wrap(factor(title),30),y=str_wrap(factor(publisher),30),color=year))+
   geom_jitter()+
   transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
   shadow_mark()+
   xlab(&amp;quot;Title&amp;quot;)+ylab(&amp;quot;Publisher&amp;quot;)+
   theme(axis.text.x =element_text(angle = 90, hjust = 1))+
   ggtitle(&amp;quot;Title and Publishers changing over time for Sri Lankan Journals from 1999 to 2017&amp;quot;,
           subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)

animate(p,fps=1,nframes=18) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/publisher%20and%20Journals-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;total-documents-and-references-for-journals-from-sri-lanka&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Total Documents and References for Journals from Sri Lanka&lt;/h2&gt;
&lt;p&gt;The Gaps between Total documents per year and Total references are compared here. Obviously in most of the cases Total documents per year is very low considering to Total references. There are anomalies such as Ceylon Medical Journal but only in the earlier publications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-sjr_journals %&amp;gt;%
    gather(Group,values,&amp;quot;total_docs_year&amp;quot;,&amp;quot;total_refs&amp;quot;) %&amp;gt;%
    subset(country==&amp;quot;Sri Lanka&amp;quot;,select=c(Group,values,year,title)) %&amp;gt;%
    mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=str_wrap(factor(title),30),y=values,fill=Group,group=Group))+
      geom_col(position = &amp;quot;dodge&amp;quot;)+
      coord_flip()+
      xlab(&amp;quot;Title&amp;quot;)+ylab(&amp;quot;Count&amp;quot;)+
      transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
      ggtitle(&amp;quot;Counts changing for Sri Lankan Journals from 1999 to 2017&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
      theme(legend.position=&amp;quot;bottom&amp;quot;)

animate(p,nframes=18,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/tot%20doc%20and%20ref-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;three-year-time-period-for-journals-from-sri-lanka&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Three year time period for Journals from Sri Lanka&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-sjr_journals %&amp;gt;%
    gather(Group,values,&amp;quot;total_docs_3years&amp;quot;,&amp;quot;total_cites_3years&amp;quot;,&amp;quot;citable_docs_3years&amp;quot;) %&amp;gt;%
    subset(country==&amp;quot;Sri Lanka&amp;quot;,select=c(Group,values,year,title)) %&amp;gt;%
    mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=str_wrap(factor(title),30),y=values,fill=Group,group=Group))+
      geom_col(position=&amp;quot;dodge&amp;quot;)+coord_flip()+
      xlab(&amp;quot;Title&amp;quot;)+ylab(&amp;quot;Count for 3 years&amp;quot;)+
      transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
      ggtitle(&amp;quot;Counts changing for Sri Lankan Journals from 1999 to 2017&amp;quot;,
              subtitle = &amp;quot;Year : {round(frame_time)}&amp;quot;)+
      theme(legend.position=&amp;quot;bottom&amp;quot;)

animate(p,nframes=18,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/3%20doc%20cite%20citable%20year-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;journals-and-categories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Journals and categories&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-sjr_journals %&amp;gt;%
      subset(country==&amp;quot;Sri Lanka&amp;quot;) %&amp;gt;%
      select(year,title,categories) %&amp;gt;%
      cSplit(&amp;quot;categories&amp;quot;,sep=&amp;quot;;&amp;quot;) %&amp;gt;%
      gather(Groups,Categories,&amp;quot;categories_1&amp;quot;,&amp;quot;categories_2&amp;quot;,&amp;quot;categories_3&amp;quot;,&amp;quot;categories_4&amp;quot;) %&amp;gt;%
      mutate(Categories=genX(Categories,&amp;quot;(Q&amp;quot;,&amp;quot;)&amp;quot;)) %&amp;gt;%
      mutate(year=year(as.Date(year,&amp;quot;%Y&amp;quot;))) %&amp;gt;%
ggplot(.,aes(x=str_wrap(factor(title),30),y=str_wrap(Categories,30),shape=Groups,color=year)) +
      geom_jitter()+ 
      theme(axis.text.x =element_text(angle = 90, hjust = 1),
            legend.position=&amp;quot;bottom&amp;quot;)+
      transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
      xlab(&amp;quot;Title&amp;quot;)+ ylab(&amp;quot;Categories&amp;quot;)+shadow_mark()+
      ggtitle(&amp;quot;Title and Categories between the years 1999 to 2017&amp;quot;,
              subtitle=&amp;quot;Year :{round(frame_time)}&amp;quot;)

animate(p,nframes=18,fps=1)             &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/SLandJournal/index_files/figure-html/journals%20and%20categories-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tree of Binomial Distribution</title>
      <link>/post/binomialdistribution/binomialdistribution/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/binomialdistribution/binomialdistribution/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-distribution&#34;&gt;Binomial Distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-mixture-distributions&#34;&gt;Binomial Mixture Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#main-groups-for-binomial-distribution&#34;&gt;Main Groups for Binomial Distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#other-with-no-sub-groups&#34;&gt;Other With No Sub-Groups&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#grassia-binomial-distribution&#34;&gt;Grassia Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#zero-modified-binomial-distribution&#34;&gt;Zero Modified Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dandekars-modified-binomial-distribution&#34;&gt;Dandekar’s Modified Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simplex-binomial-mixture-model&#34;&gt;Simplex Binomial Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#double-binomial-distribution&#34;&gt;Double Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finite-biomial-mixtures&#34;&gt;Finite Biomial Mixtures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-distribution-of-order-k&#34;&gt;Binomial Distribution of order K&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#truncated-binomial-distribution&#34;&gt;Truncated Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighted-binomial-distribution&#34;&gt;Weighted Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-not-parent&#34;&gt;Binomial Not Parent&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hypergeometric-binomial&#34;&gt;Hypergeometric + Binomial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#negative-binomial-binomial&#34;&gt;Negative Binomial + Binomial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-binomial&#34;&gt;Poisson + Binomial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#alternate-binomial-distributions&#34;&gt;Alternate Binomial Distributions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#additive-binomial-distribution&#34;&gt;Additive Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beta-correlated-binomial-distribution&#34;&gt;Beta-Correlated Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#com-poisson-binomial-distribution&#34;&gt;COM-Poisson Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlated-binomial-distribution&#34;&gt;Correlated Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplicative-binomial-distribution&#34;&gt;Multiplicative Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neyman-type-a-distribution&#34;&gt;Neyman Type A Distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-binomial-beta&#34;&gt;Poisson + Binomial + Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-poisson-binomial-beta-or-poisson-binomial-poisson-beta&#34;&gt;Poisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hermite-distribution&#34;&gt;Hermite Distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-poisson&#34;&gt;Binomial + Poisson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#poisson-binomial-1&#34;&gt;Poisson + Binomial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-parent&#34;&gt;Binomial Parent&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nn-k-and-y-mixtures&#34;&gt;N/n , K and Y Mixtures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#p-transformed-binomial&#34;&gt;p Transformed Binomial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#log-inverse-distribution-01-domain&#34;&gt;Log Inverse Distribution [0,1] Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cumulative-distribution-function&#34;&gt;Cumulative Distribution Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#p-binomial&#34;&gt;p Binomial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Binomial Distribution&lt;/h1&gt;
&lt;p&gt;The binomial distribution can be defined, using the binomial expansion&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ (q+p)^n = \sum_{x=0}^{n} {n \choose k} p^k q^{(n-k)} = \sum_{x=0}^{n} \frac{n!} {k! (n-k)!} p^k q^{(n-k)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as the distribution of a random variable X for which&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr[X=x] = {n \choose k} p^k q^{(n-k)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x=0,1,2,...,n.\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(q+p=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p,q&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is a positive integer. When &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, the distribution is known as the Bernoulli distribution. The mean and variance are &lt;span class=&#34;math inline&#34;&gt;\(\mu=np\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_2 = npq\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent trials are made and in each there is probability &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; that the outcome &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; will occur, then the number of trials in which &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; occurs can be represented by a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; having the binomial distribution with parameters &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This situation occurs when a sample of fixed size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is taken from an infinite population where each element in the population has an &lt;strong&gt;“equal”&lt;/strong&gt; and &lt;strong&gt;“independent”&lt;/strong&gt; probability &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; of possession of a specified attribute. The situation also arises when a sample of fixed size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is taken from a infinite population where each element in the population has an &lt;strong&gt;“equal”&lt;/strong&gt; and &lt;strong&gt;“independent”&lt;/strong&gt; probability &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; of having a specified attribute and elements are sampled independently and sequentially with replacement.&lt;/p&gt;
&lt;p&gt;The distribution was derived by James Bernoulli (in his treatise Ars Conjectandi, published in 1713), for the case &lt;span class=&#34;math inline&#34;&gt;\(p = r/(r+s)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; are positive integers.&lt;/p&gt;
&lt;p&gt;The binomial distribution is of such importance in applied probability and statistics that it is frequently necessary to calculate probabilities based on this distribution. Although the calculation of sums of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{x} {n \choose x} p^x q^{(n-x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is straightforward, it can be tedious, especially when n and x are large and when there are a large number of terms in the summation. It is not surprising that a great deal of attention and ingenuity have been applied to constructing useful approximations for sums of this kind.&lt;/p&gt;
&lt;div id=&#34;applications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applications&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The binomial distribution arises whenever underlying events have two possible outcomes, the chances of which remain constant. The importance of the distribution has extended from its original application in gaming to many other areas.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Its use in genetics arises because the inheritance of biological characteristics depends on genes that occur in pairs; see, for example, Fisher and Matheras (1936) analysis of data on straight versus wavy hair in mice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More recent application in genetics is the study of the number of nucleotides that are in the same state in two DNA sequences (Kaplan and Risko, 1982).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The number of defectives found in random samples of size n from a stable production process is a binomial variable; acceptance sampling is a very important application of the test for the mean of a binomial sample against a hypothetical value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Seber (1982b) has given a number of instances of the use of the binomial distribution in animal ecology, for example, in mark-recapture estimation of the size of an animal population. Boswell, Ord, and Patil (1979) gave applications in plant ecology.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although appealing in their simplicity, the assumptions of independence and constant probability for the binomial distribution are not often precisely satisfied. Published critical appraisals of the extent of departure from these assumptions in actual situations are rather rare.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-mixture-distributions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Binomial Mixture Distributions&lt;/h1&gt;
&lt;p&gt;The binomial distribution has two parameters, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, and either or both of these may be supposed to have a probability distribution. We will not discuss cases in which both &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; vary, though it is easy to construct such examples.&lt;/p&gt;
&lt;p&gt;In most cases discussed in the statistical literature, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; has a continuous distribution, while &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is discrete. The latter restriction is necessary, but the former is not.&lt;/p&gt;
&lt;p&gt;However, discrete distributions for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; have not been found to be useful and have not attracted much attention from a theoretical point of view.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-groups-for-binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Main Groups for Binomial Distribution&lt;/h1&gt;
&lt;p&gt;More than 50 distributions are in this blog post but I shall not explain them even briefly. This blog post is to introduce them and let you know the reader about these distributions with article references.&lt;/p&gt;
&lt;p&gt;In this section I introduce the main groups, where the number of groups are 6. Further, the Binomial Parent group has a very broad approach. “Binomial Parent” means, Where Binomial Distribution is the Parent distribution and by mixing them with other distributions we can produce a new distribution which would satisfy real data but not fully satisfy the condition of Binomial Distribution.&lt;/p&gt;
&lt;p&gt;Clearly with the vast number of distributions and sub groups it is not possible to draw them in one diagram, therefore I have divided them in sub groups and sub topics in the following sections.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:1000px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Not Parent];\n            A--&gt;D[Other with &lt;br&gt; no sub groups];\n            A--&gt;C[Binomial &lt;br&gt; Parent];\n            A--&gt;M[Alternate Binomial &lt;br&gt; Distribution];\n            A--&gt;N[Neyman Type A &lt;br&gt; Distribution];\n            A--&gt;O[Hermite Distribution];\n            C--&gt;CA[Mixing Parameter];\n            CA--&gt;CAA[N/n &lt;br&gt; Binomial];\n            CA--&gt;CAB[K &lt;br&gt; Binomial];\n            CA--&gt;CAC[Y &lt;br&gt; Binomial];\n            CA--&gt;CAE[p transformed &lt;br&gt; Binomial];\n            CA--&gt;CAF[Cumulative Distribution &lt;br&gt; Function];\n            CA--&gt;CAD[p &lt;br&gt; Binomial];\n            CA--&gt;CAG[Log Inverse &lt;br&gt; Distribution &lt;br&gt; 0,1 Domain]\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;other-with-no-sub-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other With No Sub-Groups&lt;/h2&gt;
&lt;p&gt;First sub group in discussion is “Other With No Sub-Groups”, which is an abbreviation for Binomial distributions which were direcly created for the purpose of satisfying specific real world situations and theoretical concepts.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:1000px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph TB;\n            B[Binomial &lt;br&gt; Distribution]--&gt;A[Other with &lt;br&gt; no sub groups];\n            A--&gt;D[Grassia Binomial &lt;br&gt; Distribution];\n            A--&gt;E[Zero Modified &lt;br&gt; Binomial Distribution];\n            A--&gt;F[Dandekar&#39;s Modified &lt;br&gt; Binomial Distribution];\n            A--&gt;G[Simplex Binomial &lt;br&gt; Mixture Model];\n            A--&gt;H[Double Binomial &lt;br&gt; Distribution];\n            A--&gt;I[Finite Binomial &lt;br&gt; Mixtures];\n            A--&gt;J[Binomial Distribution &lt;br&gt; of order K];\n            A--&gt;K[Truncated Binomial &lt;br&gt; Distribution];\n            A--&gt;L[Weighted Binomial &lt;br&gt; Distribution];\n            &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Below is the list of Distributions in this sub group with article references.&lt;/p&gt;
&lt;div id=&#34;grassia-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grassia Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Kemp, A. W., and Kemp, C. D (2004). Factorial moment characterizations for certain binomial-type distributions, Communications in Statistics-Theory and Methods, 33, 3059-3068.&lt;/p&gt;
&lt;p&gt;Harkness, W. L. (1970). The classical occupancy problem revisited, Random Counts in Scientific Work, Vol. 3:Random Counts in Physical Science, Geo Science, and Business, G. P. Patil (editor), 107-126. University Park: Pennsylvania State University Press.&lt;/p&gt;
&lt;p&gt;Weiss, G. H. (1965). A model for the spread of epidemics by carriers, Biometrics, 21, 481-490.&lt;/p&gt;
&lt;p&gt;Dietz, K. (1966). On the model of Weiss for the spread of epidemics by carriers, Journal of Applied Probability, 3, 375-382.&lt;/p&gt;
&lt;p&gt;Downton, F. (1967). Epidemics with carriers: A note on a paper by Dietz, Journal of Applied Probability, 4, 264-270.&lt;/p&gt;
&lt;p&gt;Daley, D. J., and Gani, J. (1999). Epidemic Modelling:An Introduction,Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;p&gt;Grassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformation of the gamma and derived compound distributions, Australian Journal of Statistics, 19, 108-114.&lt;/p&gt;
&lt;p&gt;Alanko, T., and Duffy, J. C. (1996). Compound binomial distributions for modelling consumption data, The Statistician, 45, 269-286.&lt;/p&gt;
&lt;p&gt;Chatfield, C., and Goodhardt, G. J. (1970). The beta-binomial model for consumer purchasing behaviour, Applied Statistics, 19, 240-250.&lt;/p&gt;
&lt;p&gt;Consul, P. C., and Jain, G. C. (1971). On the log-gamma distribution and its properties, Statistische Hefte, 12, 100-106.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-modified-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Zero Modified Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Dowling, M. M., and Nakamura, M. (1997). Estimating parameters for discrete distributions via the empirical probability generating function, Communications in Statistics-Simulation and Computation, 26, 301-313.&lt;/p&gt;
&lt;p&gt;Khatri, C. G. (1961). On the distributions obtained by varying the number of trials in a binomial distribution, Annals of the Institute of Statistical Mathematics, Tokyo, 13, 47-51.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dandekars-modified-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dandekar’s Modified Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Patil, G. P., Boswell, M. T., Joshi, S. W., and Ratnaparkhi, M. V. (1984). Dictionary and Bibliography of Statistical Distributions in Scientific Work, Vol. 1: Discrete Models, Fairland, MD: International Co-operative Publishing House.&lt;/p&gt;
&lt;p&gt;Dandekar, V. M. (1955). Certain modified forms of binomial and Poisson distributions, Sankhya, 15, 237-250.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simplex-binomial-mixture-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simplex Binomial Mixture Model&lt;/h3&gt;
&lt;p&gt;Barndorff-Neilsen, O. E., and Jorgensen, B. (1991). Some parametric models on the simplex, Journal of Multivariate Analysis, 39, 106-116.&lt;/p&gt;
&lt;p&gt;Jorgensen, B. (1997). The Theory of Regression Models, London: Chapman &amp;amp; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;double-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Double Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Lindsey, J. K. (1995). Modelling Frequency and Count Data, Oxford: Oxford University Press.&lt;/p&gt;
&lt;p&gt;Efron, B. (1986). Double exponential families and their use in generalized linear regression, Journal of the American Statistical Association, 81, 709-721.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finite-biomial-mixtures&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finite Biomial Mixtures&lt;/h3&gt;
&lt;p&gt;Teicher, H. (1961). Identifiability of mixtures, Annals of Mathematical Statistics, 32, 244-248.&lt;/p&gt;
&lt;p&gt;Blischke, W. R. (1962). Moment estimation for the parameters of a mixture of two binomial distributions, Annals of Mathematical Statistics, 33, 444-454.&lt;/p&gt;
&lt;p&gt;Blischke, W. R. (1964). Estimating the parameters of mixtures of binomial distributions, Journal of the American Statistical Association, 59, 510-528.&lt;/p&gt;
&lt;p&gt;Blischke, W. R. (1965). Mixtures of discrete distributions, Classical and Contagious Discrete Distributions, G. P. Patil (editor), 351-372. Calcutta: Statistical Publishing Society; Oxford: Pergamon.&lt;/p&gt;
&lt;p&gt;Everitt, B. S., and Hand, D. J. (1981). Finite Mixture Distributions, London: Chapman &amp;amp; Hall.&lt;/p&gt;
&lt;p&gt;Bondesson, L. (1988). On the gain by spreading seeds: A statistical analysis of sowing experiments, Scandinavian Journal of Forest Research, 305-314.&lt;/p&gt;
&lt;p&gt;Gelfand, A. E., and Soloman, H. (1975). Analysing the decision making process of the American jury, Journal of the American Statistical Association, 70, 305-310.&lt;/p&gt;
&lt;p&gt;Hasselblad, V. (1969). Estimation of finite mixtures of distributions from the exponential family, Journal of the American Statistical Association, 64, 1459-1471.&lt;/p&gt;
&lt;p&gt;Rider, P. R. (1962a). Estimating the parameters of mixed Poisson, binomial and Weibull distributions, Bulletin of the International Statistical Institute, 39(2), 225-232.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-distribution-of-order-k&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial Distribution of order K&lt;/h3&gt;
&lt;p&gt;Ling, K. D. (1988). On binomial distributions of order k, Statistics and Probability Letters, 6, 371-376.&lt;/p&gt;
&lt;p&gt;Shanthikumar, J. G. (1985). Discrete random variate generation using uniformization, European Journal of Operational Research, 21, 387-398.&lt;/p&gt;
&lt;p&gt;Chiang, D., and Niu, S. C. (1981). Reliability of consecutive-k-out-of-n:F systems, IEEE Transactions on Reliability, R-30, 87-89.&lt;/p&gt;
&lt;p&gt;Bollinger, R. C., and Salvia, A. A. (1982). Consecutive-k-out-of-n: F networks, IEEE Transactions on Reliability, R-31, 53-55.&lt;/p&gt;
&lt;p&gt;Hirano, K. (1986). Some properties of the distributions of order k, Fibonacci Numbers and Their Applications, A. N. Philippou, G. E. Bergum, and A. F. Horadam (editors), 43-53. Dordrecht: Reidel.&lt;/p&gt;
&lt;p&gt;Philippou, A. N., and Makri, F. S. (1986). Success runs and longest runs, Statistics and Probability Letters, 4, 101-105 (corrected version 211-215).&lt;/p&gt;
&lt;p&gt;Feller, W. (1957). An Introduction to Probability Theory and Its Applications (second edition), Vol. 1, New York: Wiley.&lt;/p&gt;
&lt;p&gt;Aki, S., and Hirano, K. (1988). Some characteristics of the binomial distribution of order k and related distributions, Statistical Theory and Data Analysis, Vol. 2, K. Matusita (editor), 211-222. Amsterdam: Elsevier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;truncated-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Truncated Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Stephan, F. F. (1945). The expected value and variance of the reciprocal and other negative powers of a positive Bernoullian variate, Annals of Mathematical Statistics, 16, 50-61.&lt;/p&gt;
&lt;p&gt;Shah, S. M. (1966). On estimating the parameter of a doubly truncated binomial distribution, Journal of the American Statistical Association, 61, 259-263.&lt;/p&gt;
&lt;p&gt;Newell, D. J. (1965). Unusual frequency distributions, Biometrics, 21, 159-168.&lt;/p&gt;
&lt;p&gt;Grab, E. L., and Savage, I. R. (1954). Tables of the expected value of 1/x for positive Bernoulli and Poisson variables, Journal of the American Statistical Association, 49, 169-177.&lt;/p&gt;
&lt;p&gt;Finney, D. J. (1949). The truncated binomial distribution, Annals of Eugenics, London, 14, 319-328.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weighted Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Patil, G. P., Rao, C. R., and Zelen, M. (1986). A Computerized Bibliography of Weighted Distributions and Related Weighted Methods for Statistical Analysis and Interpretations of Encountered Data, Observational Studies, Representativeness Issues, and Resulting Inferences, University Park, PA: Centre for Statistical Ecology and Environmental Statistics, Pennsylvania State University.&lt;/p&gt;
&lt;p&gt;Patil, G. P., Rao, C. R., and Ratnaparkhi, M. V. (1986). On discrete weighted distributions and their use in model choice for observed data, Communications in Statistics-Theory and Methods, 15, 907-918.&lt;/p&gt;
&lt;p&gt;Rao, C. R. (1965). On discrete distributions arising out of methods of ascertainment, Classical and Contagious Discrete Distributions,G. P Patil (editor), 320-332. Calcutta: Statistical Publishing Society; Oxford: Pergamon. (Republished Sankhya, A27, 1965, 311-324.)&lt;/p&gt;
&lt;p&gt;Rao, C. R. (1985). Weighted distributions arising out of methods of ascertainment: What populations does a sample represent?, A Celebration of Statistics: ISI Centenary Volume, A. C. Atkinson and S. E. Fienberg (editors), 543-569. New York: SpringerVerlag.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-not-parent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial Not Parent&lt;/h2&gt;
&lt;p&gt;This is the sub group where Binomial Distribution is not the parent but rather some other different distribution like Hypergeometric or Negative Binomial or Poisson.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:800px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n            graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial Not Parent];\n            B--&gt;BA[Hypergeometric &lt;br&gt; + Binomial];\n            B--&gt;BB[Negative Binomial &lt;br&gt; + Binomial];\n            B--&gt;BC[Poisson + &lt;br&gt; Binomial];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;hypergeometric-binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypergeometric + Binomial&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Hypergeometric (n,Y,N) \bigwedge_Y Binomial(N,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-binomial-binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Negative Binomial + Binomial&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Negative Binomial (kY,P) \bigwedge_Y Binomial(n,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson + Binomial&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Poisson (\theta) \bigwedge_{\theta/\phi} Binomial(n,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;alternate-binomial-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternate Binomial Distributions&lt;/h2&gt;
&lt;p&gt;This is a sub group of distributions which can be used as an alternative to Binomial Distribution.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:800px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n            graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Alternate Binomial &lt;br&gt; Distribution];\n            B--&gt;C[Additive Binomial &lt;br&gt; Distribution];\n            B--&gt;D[Beta-Correlated &lt;br&gt; Binomial Distribution];\n            B--&gt;E[COM-Poisson Binomial &lt;br&gt; Distribution];\n            B--&gt;F[Correlated Binomial &lt;br&gt; Distribution];\n            B--&gt;G[Multiplicative Binomial &lt;br&gt; Distribution];\n            &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;additive-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additive Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Johnson, N. L., Kemp, A. W., &amp;amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.&lt;/p&gt;
&lt;p&gt;L. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.&lt;/p&gt;
&lt;p&gt;Paul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-correlated-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beta-Correlated Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Paul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;com-poisson-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;COM-Poisson Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Borges, P., Rodrigues, J., Balakrishnan, N. and Bazan, J., 2014. A COM-Poisson type generalization of the binomial distribution and its properties and applications. Statistics &amp;amp; Probability Letters, 87, pp.158-166.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlated-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlated Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Johnson, N. L., Kemp, A. W., &amp;amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.&lt;/p&gt;
&lt;p&gt;L. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.&lt;/p&gt;
&lt;p&gt;Paul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.&lt;/p&gt;
&lt;p&gt;Jorge G. Morel and Nagaraj K. Neerchal. Overdispersion Models in SAS. SAS Institute, 2012.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplicative-binomial-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiplicative Binomial Distribution&lt;/h3&gt;
&lt;p&gt;Johnson, N. L., Kemp, A. W., &amp;amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.&lt;/p&gt;
&lt;p&gt;L. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.&lt;/p&gt;
&lt;p&gt;Paul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neyman-type-a-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Neyman Type A Distribution&lt;/h2&gt;
&lt;div id=&#34;htmlwidget-5&#34; style=&#34;width:800px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-5&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n            graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[ Neyman Type A &lt;br&gt; Distribution];\n            B--&gt;C[Poisson + Binomial +  Beta];\n            B--&gt;D[Poisson + Poisson + Binomial + Beta &lt;br&gt; or Poisson + Binomial + Poisson + Beta];\n            B--&gt;E[Poisson + Binomial + Poisson + Beta &lt;br&gt; or Binomial + Poisson + Poisson + Beta];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;poisson-binomial-beta&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson + Binomial + Beta&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Poisson(\phi) \bigvee Binomial(1,P) \bigwedge_P Beta(\alpha,\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-poisson-binomial-beta-or-poisson-binomial-poisson-beta&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Poisson(\lambda) \bigvee [\{Poisson(\phi) \vee Binomial(1,P) \} \bigwedge_P Beta(\alpha,\beta)]\]&lt;/span&gt; or &lt;span class=&#34;math display&#34;&gt;\[ Poisson(\lambda) \bigvee [\{Binomial(M,P) \bigvee_M Poisson(\phi) \} \bigwedge_P Beta(\alpha,\beta)]\]&lt;/span&gt; ### Poisson + Binomial + Poisson + Beta or Binomial + Poisson + Poisson + Beta&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[[\{ Poisson(\lambda) \bigvee Binomial(1,P)\} \bigvee Poisson(\phi)] \bigwedge_P Beta(a,b)\]&lt;/span&gt; or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[[\{ Binomial(N,P) \bigvee Poisson(\phi)\} \bigwedge_N Poisson(\lambda)] \bigwedge_P Beta(a,b)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Gurland, J. (1958). A generalized class of contagious distributions, Biometrics, 14, 229-249.&lt;/p&gt;
&lt;p&gt;Feller, W. (1943). On a general class of “contagious” distributions, Annals of Mathematical Statistics, 14, 389-400.&lt;/p&gt;
&lt;p&gt;Neyman, J. (1939). On a new class of “contagious” distributions applicable in entomology and bacteriology, Annals of Mathematical Statistics, 10, 35-57.&lt;/p&gt;
&lt;p&gt;Subrahmaniam, Kocherlakota (1966). On a general class of contagious distributions: The Pascal-Poisson distribution, Trabajos de Estadistica, 17, 109-127.&lt;/p&gt;
&lt;p&gt;Subrahmaniam, Kathleen (1978). The Pascal-Poisson distribution revisited: Estimation and efficiency, Communications in Statistics-Theory and Methods, A7, 673-683.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hermite-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hermite Distribution&lt;/h2&gt;
&lt;div id=&#34;htmlwidget-6&#34; style=&#34;width:800px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-6&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n           graph TB;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Hermite &lt;br&gt; Distribution];\n           B--&gt;C[Binomial + Poisson];\n           B--&gt;D[Poisson + Binomial];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;binomial-poisson&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial + Poisson&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(N,p) \bigwedge_{N/2} Poisson(\lambda)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-binomial-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson + Binomial&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Poisson(\lambda) \bigvee Binomial(2,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Skellam, J. G. (1952). Studies in statistical ecology I: Spatial pattern, Biometrika, 39, 346-362.&lt;/p&gt;
&lt;p&gt;McGuire, J. U., Brindley, T. A., and Bancroft, T. A. (1957). The distribution of European corn borer Pyrausta Nubilalis (Hbn.) in field corn, Biometrics, 13, 65-78 [errata and extensions (1958) 14, 432-434].&lt;/p&gt;
&lt;p&gt;Kemp, C. D., and Kemp, A. W. (1965). Some properties of the “Hermite” distribution, Biometrika, 52, 381-394.&lt;/p&gt;
&lt;p&gt;Fisher, R. A. (1951). Properties of the functions, (Part of introduction to) British Association Mathematical Tables (third edition), Vol. 1, London: British Association.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-parent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial Parent&lt;/h2&gt;
&lt;p&gt;This is the sub group where Binomial distribution is the parent distribution. By considering the distribution and its parameters we can use possible different mixing distributions and generate new Binomial Mixture Distributions.&lt;/p&gt;
&lt;div id=&#34;nn-k-and-y-mixtures&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;N/n , K and Y Mixtures&lt;/h3&gt;
&lt;p&gt;The first mixtures with few distributions are mentioned below.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-7&#34; style=&#34;width:800px;height:400px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-7&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n           graph TB;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[N/n &lt;br&gt; Binomial];\n           D--&gt;DA[Poisson];\n           D--&gt;DB[Binomial];\n           D--&gt;DC[Negative Binomial];\n           D--&gt;DD[Logarithmic];\n           C--&gt;E[K &lt;br&gt; Binomial];\n           E--&gt;EA[Poisson];\n           C--&gt;F[Y &lt;br&gt; Binomial];\n           F--&gt;FA[Hypergeometric];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;poisson-mix-for-nn-of-binomial-distributon&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Poisson Mix for N/n of Binomial distributon&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(N,p) \bigwedge_{N/n} Poisson(\lambda)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-binomial-mix-for-nn-of-binomial-distributon&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Negative Binomial Mix for N/n of Binomial distributon&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(N,p) \bigwedge_{N/n} Negative Binomial(k,P`)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-mix-for-nn-of-binomial-distributon&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Binomial Mix for N/n of Binomial distributon&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(N,p) \bigwedge_{N/n} Binomial(N`,p`)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-mix-for-nn-of-binomial-distribution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logarithmic Mix for N/n of Binomial distribution&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(N,p) \bigwedge_{N/n} Logarithmic(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-mix-for-k-of-poisson-distributon&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Poisson Mix for K of Poisson distributon&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(nK,p) \bigwedge_K Poisson(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypergeometric-mix-for-y-of-binomial-distributon&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hypergeometric Mix for Y of Binomial distributon&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Binomial(m,\frac{Y}{n}) \bigwedge_{Y} Hypergeometric(n,Np,N)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;p-transformed-binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;p Transformed Binomial&lt;/h3&gt;
&lt;div id=&#34;htmlwidget-8&#34; style=&#34;width:800px;height:1000px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-8&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n           graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[p transformed &lt;br&gt; Binomial];\n           D--&gt;E[p = 1 - exp_-t_ ];\n           E--&gt;EA[Binomial Exponential &lt;br&gt; Distribution];\n           E--&gt;EB[Binomial Gamma 1 &lt;br&gt; Distribution];\n           E--&gt;EC[Binomial Gamma 2 &lt;br&gt; Distribution];\n           E--&gt;ED[Binomial Generalized &lt;br&gt; Exponential 1 Distribution];\n           E--&gt;EE[Binomial Generalized &lt;br&gt; Exponential 2 Distribution];\n           D--&gt;F[p = exp_-t_ ];\n           F--&gt;FA[Binomial Exponential &lt;br&gt; Distribution];\n           F--&gt;FB[Binomial Gamma 1 &lt;br&gt; Distribution];\n           F--&gt;FC[Binomial Gamma 2 &lt;br&gt; Distribution];\n           F--&gt;FD[Binomial Generalized &lt;br&gt; Exponential 1 Distribution];\n           F--&gt;FE[Binomial Generalized &lt;br&gt; Exponential 2 Distribution];\n           F--&gt;FF[Binomial Variated &lt;br&gt; Exponential Distribution];\n           F--&gt;FG[Binomial Variated &lt;br&gt; Gamma 2,alpha Distribution];\n           F--&gt;FH[Binomial Inverse &lt;br&gt; Gaussian Distribution];\n           D--&gt;G[p = cy];\n           G--&gt;GA[Binomial Generalized Beta 4 Distribution];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Bowman, K. O., Shenton, L. R., Kastenbaum, M. A., &amp;amp; Broman, K. (1992). Overdispersion: Notes on Discrete distributions. Oak Ridge Tennessee : Oak Ridge National Laboratory.&lt;/p&gt;
&lt;p&gt;Alanko, T., &amp;amp; Duffy, J. C. (1996). Compound Binomial distributions for modeling consumption data. Journal of the Royal Statistical society, series D (The Statistician) Vol. 45, No. 3 ,269-286.&lt;/p&gt;
&lt;p&gt;Gerstenkorn, T. (2004). A compound of the Generalized Negative Binomial distribution with the Generalized Beta distribution. Central European Science journals, CEJM 2 (4), 527-537.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-inverse-distribution-01-domain&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log Inverse Distribution [0,1] Domain&lt;/h3&gt;
&lt;div id=&#34;htmlwidget-9&#34; style=&#34;width:800px;height:1000px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-9&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n           graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[Log Inverse &lt;br&gt; Distribution &lt;br&gt; 0,1 Domain];\n           D--&gt;DA[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Exponential Distribution];\n           D--&gt;DB[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Exponential Distribution];\n           D--&gt;DC[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 1 Distribution];\n           D--&gt;DD[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 1 Distribution];\n           D--&gt;DE[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 2 Distribution];\n           D--&gt;DF[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 2 Distribution];\n           D--&gt;DG[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 1 &lt;br&gt; Distribution];\n           D--&gt;DH[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 1 &lt;br&gt; Distribution];\n           D--&gt;DJ[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 2 &lt;br&gt; Distribution];\n           D--&gt;DK[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 2 &lt;br&gt; Distribution];\n           D--&gt;DL[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 3 &lt;br&gt; Distribution];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Grassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformations of the Gamma and derived compound distributions. Australian journal of Statistics, 19 (2) 108-114.&lt;/p&gt;
&lt;p&gt;McDonald, J. B., &amp;amp; Yexiao, J. X. (1995). A generalization of the Beta distribution with applications. Journal of Econometrics 66,133-152.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-distribution-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative Distribution Function&lt;/h3&gt;
&lt;div id=&#34;htmlwidget-10&#34; style=&#34;width:1000px;height:750px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-10&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n            graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial Parent];\n            B--&gt;C[Mixing Parameter];\n            C--&gt;D[Cumulative Distribution];\n            D--&gt;DA[Beta Generated &lt;br&gt; Distribution];\n            DA--&gt;DAA[Binomial &lt;br&gt; Beta Exponential &lt;br&gt; Distribution];\n            DA--&gt;DAB[Binomial &lt;br&gt; Beta Generalized &lt;br&gt; Exponential Distribution];\n            DA--&gt;DAC[Binomial &lt;br&gt; Beta Power &lt;br&gt; Distribution];\n            D--&gt;DB[Kumaraswamy &lt;br&gt; Generated &lt;br&gt; Distribution];\n            DB--&gt;DBA[Binomial &lt;br&gt; Kumaraswamy &lt;br&gt; Power Distribution];\n            DB--&gt;DBB[Binomial &lt;br&gt; Kumaraswamy &lt;br&gt; Exponential &lt;br&gt; Distribution];        \n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Eugene, N., Lee, C., &amp;amp; Famoye, F. (2002). Beta-normal distributions and its applications. Communications in Statistics-Theory and Methods 31, 4 ,497-512.&lt;/p&gt;
&lt;p&gt;Nadarajah, S., &amp;amp; Kotz, S. (2006). The Beta Exponential distribution. Reliability engineering and system safety, Vol. 91, Issue 6 ,689-697&lt;/p&gt;
&lt;p&gt;Barreto-Souza, W., Santos, A., &amp;amp; Cordeiro, G. M. (2009). The Beta Generalized Exponential distribution. Journal of Statistical Computation and Simulation , 1-14.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;p-binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;p Binomial&lt;/h3&gt;
&lt;div id=&#34;htmlwidget-11&#34; style=&#34;width:700px;height:1200px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-11&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n           graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[ Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[p &lt;br&gt; Binomial];\n           D--&gt;DA[Beyond Beta &lt;br&gt; Distribution];\n           DA--&gt;DAA[Binomial Triangular &lt;br&gt; Distribution];\n           DA--&gt;DAB[Binomial Kumaraswamy 2 &lt;br&gt; Distribution];\n           DA--&gt;DAC[Binomial Kumaraswamy 1 &lt;br&gt; Distribution];\n           DA--&gt;DAD[Binomial Truncated &lt;br&gt; Exponential Distribution];\n           DA--&gt;DAE[Binomial Truncated &lt;br&gt; Gamma Distribution];\n           DA--&gt;DAF[Binomial - MinusLog &lt;br&gt; Distribution];\n           DA--&gt;DAG[Binomial Standard &lt;br&gt; Two Sied Power &lt;br&gt; Distribution];\n           DA--&gt;DAH[Binomial Ogive &lt;br&gt; Distribution];\n           DA--&gt;DAI[Binomial - Two Sided &lt;br&gt; Ogive Distribution];\n           D--&gt;DB[Beta Distribution];\n           DB--&gt;DBA[Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBB[McDonald Generalized &lt;br&gt; Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBC[Libby and Novick &lt;br&gt; Generalized Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBD[Gauss Hypergeometric &lt;br&gt; Binomial Distribution];\n           DB--&gt;DBE[Confluent Hypergeometric &lt;br&gt; Binomial Distribution];\n           DB--&gt;DBF[Binomial Uniform &lt;br&gt; Distribution];\n           DB--&gt;DBG[Binomial Power &lt;br&gt; Function Distribution];\n           DB--&gt;DBH[Binomial Truncated &lt;br&gt; Beta Distribution];\n           DB--&gt;DBI[Binomial Arcsine &lt;br&gt; Distribution];\n           &#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Karlis, D., &amp;amp; Xekalaki, E. (2006). The Polygonal distributions, ntemational Conference on Mathematical and Statistical modeling in honnor of Enrique Castillo. University of Castilla-La Mancha.&lt;/p&gt;
&lt;p&gt;Jones, M. C. (2009). Kumaraswamy’s distribution: a beta-type distibution with some tractability advantages. Statistical Methodology Vol. 6, Issue 1 ,70-81.&lt;/p&gt;
&lt;p&gt;Jones, M. C. (2007). The Minimax distribution: A Beta type distribution with some tractability advantages. Retrieved from The Open University: &lt;a href=&#34;http://stats-www.open.ac.uk/TechnicalReports/minimax.pdf&#34; class=&#34;uri&#34;&gt;http://stats-www.open.ac.uk/TechnicalReports/minimax.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumaraswamy, P. (1980). Genralized probability density functions for double-bounded random processes. Journal of hydrology, 79-88.&lt;/p&gt;
&lt;p&gt;Dorp, J. R., &amp;amp; Kotz, S. (2003). Generalizations f Two-Sided Power distributions and their Convolution. Communications in Statistics-Theory and Methods, Vol. 32, Issue 9 ,1703-1723.&lt;/p&gt;
&lt;p&gt;Armero, S., &amp;amp; Bayarri, M. J. (1994). Prior assessments for prediction in queues. The Statistician 43,139- 153.&lt;/p&gt;
&lt;p&gt;Bhattacharya, S. K. (1968). Bayes approach to compound distributions arising from truncated mixing densities. Annals of the institute of Statistical Mathematics, Vol 20, No. 1 ,375-381.&lt;/p&gt;
&lt;p&gt;Johnson, N. L., Kotz, S., &amp;amp; Kemp, A. (1992). Univariate Discrete distributions, Second Edition. New York: John Wiley and Sons.&lt;/p&gt;
&lt;p&gt;Libby, D. I., 8t Novick, M. R. (1982). Multivariate generalized beta-distributions with applicatons to utility assessment. Journal of Educational Statistics 9 ,163-175.&lt;/p&gt;
&lt;p&gt;Nadarajah, S., 8i Kotz, S. (2007). Multitude of Beta distributions with applications. Statistics: a journal of theoretical and applied statistics, Vol. 41, No. 2 ,153-179.&lt;/p&gt;
&lt;p&gt;Sivaganesan, S., 8i Berger, J. (1993). Robust Bayesian analysis of the Binomial empirical Bayes problem. The Canadian journal of Statistics, 21,107-119.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Fisher, R. A., and Mather, K. (1936). A linkage test with mice, Annals of Eugenics, London, 7, 265-280.&lt;/p&gt;
&lt;p&gt;Boswell, M. T., Ord, J. K., and Patil, G. P. (1979). Chance mechanisms underlying univariate distributions, Statistical Ecology, Vol. 4:Statistical Distributions in Ecological Work, J. K. Ord, G. P. Patil, and C. Taillie (editors), 1-156. Fairland, MD: International Co-operative Publishing House.&lt;/p&gt;
&lt;p&gt;Kaplan, N., and Risko, K. (1982). A method for estimating rates of nucleotide substitution using DNA sequence data, Theoretical Population Biology, 21, 318-328.&lt;/p&gt;
&lt;p&gt;Seber, G. A. F. (1982b). The Estimation of Animal Abundance (second edition), London: Griffin.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 7: Spending On Science Stuff</title>
      <link>/post/tidytuesday2019/week7/week7/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week7/week7/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#climate-change-research&#34;&gt;Climate Change Research&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#energy&#34;&gt;Energy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#federal&#34;&gt;Federal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages 
library(readr)
library(tidyverse)
library(gganimate)
library(dplyr)
library(magrittr)

# load the data
climate &amp;lt;- read_csv(&amp;quot;climate_spending.csv&amp;quot;)

energy &amp;lt;- read_csv(&amp;quot;energy_spending.csv&amp;quot;, 
                    col_types = cols(year = col_integer()))

federal &amp;lt;- read_csv(&amp;quot;fed_r_d_spending.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though I can go further and do an investigative plotting from the rest data it is not done here. I was more focused on the scientific notation values in the plotting and scales, which were bothering me a lot.&lt;/p&gt;
&lt;p&gt;3 Data sets are given here, they are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Global Climate Change Research Program Spending. - climate&lt;/li&gt;
&lt;li&gt;Energy Departments Data. - energy&lt;/li&gt;
&lt;li&gt;Total Federal R &amp;amp; D Spending by Department. - federal&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Oddly though climate data-set did not have year values, I checked the downloaded csv file and the GitHub upload as well. Well, that did not stop me from doing some tidy plotting.&lt;/p&gt;
&lt;p&gt;You can obtain the data from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-12&#34;&gt;here.&lt;/a&gt; It should be noted that I am not going to rename the abbreviation of departments with their full names, so below is a screen shot which would come in handy.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;department.jpg&#34; alt=&#34;Department Full Names with Abbreviations&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Department Full Names with Abbreviations&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Scientific notation of numbers and plotting them was fun. Except for the R and D budget, other 3 have a steady increase over the years. Further, the R and D budget is very small than the others. Code: &lt;a href=&#34;https://t.co/jmzfTGMRaT&#34;&gt;https://t.co/jmzfTGMRaT&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/3WfBU72kWW&#34;&gt;pic.twitter.com/3WfBU72kWW&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1095152521507733505?ref_src=twsrc%5Etfw&#34;&gt;February 12, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week7&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;climate-change-research&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Climate Change Research&lt;/h1&gt;
&lt;p&gt;As I mentioned earlier for the climate data there are no values in the year column, but according to summary I was able to deduce that we have 18 years of information. When we do plot it is going to be the summation for each department in a bar.&lt;/p&gt;
&lt;p&gt;Clearly NASA has the most amount ( above than 2.5 x 10^10 USD) of spending because rockets are expensive, second place goes to NSF (5 x 10^9 USD) and third place to NOAA. Lowest amount of spending is to the Department of Interior (8.47 x 10^8 USD).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(climate,aes(x=fct_inorder(department),y=gcc_spending,fill=department))+
  geom_bar(stat=&amp;quot;identity&amp;quot;,show.legend = FALSE)+
  ggtitle(&amp;quot;Total GCC Spending for 18 Years&amp;quot;)+
  scale_y_continuous(labels = scales::scientific,breaks = seq(0,2.75e+10,0.25e+10))+
  xlab(&amp;quot;Sub Agency / Department&amp;quot;)+ylab(&amp;quot;GCC Spending (in USD)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Global%20Climate%20change%20Research-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;energy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Energy&lt;/h1&gt;
&lt;p&gt;Since 1997 to 2018 how Energy Department funding has changed with sub agency/ department is the purpose of the below bar plot. Office of Science R &amp;amp; D and Atomic Energy Defense are competitive over the years and for a short period of time the latter has less funding than the former, this was between 2006 to 2010.&lt;/p&gt;
&lt;p&gt;Other agencies oscillates over the years while reaching new highs and lows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(energy,aes(x=department,y=energy_spending,fill=year))+
          geom_bar(stat=&amp;quot;identity&amp;quot;,position =&amp;quot;identity&amp;quot;)+
          transition_time(year)+
          geom_text(aes(label=scales::scientific(energy_spending)),
                    vjust = &amp;quot;inward&amp;quot;, hjust = &amp;quot;inward&amp;quot;)+
          ease_aes(&amp;quot;linear&amp;quot;)+coord_flip()+
          ylab(&amp;quot;Energy Spending (in USD)&amp;quot;)+
          theme(legend.position = &amp;quot;right&amp;quot;)+
          xlab(&amp;quot;Sub Agency / Department&amp;quot;)+
          scale_fill_continuous(breaks = seq(1997,2018,3))+
          scale_y_continuous(labels = scales::scientific)+
          ggtitle(&amp;quot;Energy Spending Of Year : {frame_time}&amp;quot;)

animate(p,fps=1,nframes=22)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Energy%20Funding-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;federal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Federal&lt;/h1&gt;
&lt;p&gt;Data of Federal funding has four different types to be compared and they are mentioned below in the description image which would make explanation more easier.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;description.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Except rd_budget others have a very clear increase in amount between 1976 to 2018. Further, all four plots have different scales and the limits are widely different for each plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-federal %&amp;gt;%
    gather(funding,amount,c(rd_budget,total_outlays,discretionary_outlays,gdp)) %&amp;gt;%
    ggplot(.,aes(x=factor(department),y=amount,color=year))+
           geom_jitter()+transition_time(year)+
           ease_aes(&amp;quot;linear&amp;quot;)+coord_flip()+
           shadow_mark()+
           theme(legend.position = &amp;quot;right&amp;quot;)+
           ylab(&amp;quot;Spending in USD&amp;quot;)+xlab(&amp;quot;Department&amp;quot;)+
           ggtitle(&amp;quot;Total Federal R&amp;amp;D for Year : {frame_time}&amp;quot;)+
           scale_color_continuous(breaks = seq(1976,2018,6),labels=seq(1976,2018,6))+
           scale_y_continuous(labels = scales::scientific)+
           facet_wrap(~funding,scales = &amp;quot;free&amp;quot;)

animate(p,fps=1,nframes=42)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Federal%20Funding-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 6 : Mortgage, Recession and States</title>
      <link>/post/tidytuesday2019/week6/week6/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week6/week6/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mortgage&#34;&gt;Mortgage&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-rate-30-years-from-1971-to-2018&#34;&gt;Fixed Rate 30 Years from 1971 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-rate-15-years-from-1991-to-2018&#34;&gt;Fixed Rate 15 Years from 1991 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fees-and-points-of-30-years-from-1971-to-2018&#34;&gt;Fees and Points of 30 Years from 1971 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fees-and-points-of-15-years-from-1991-to-2018&#34;&gt;Fees and Points of 15 Years from 1991 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#states&#34;&gt;States&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#new-england-region&#34;&gt;New England Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mideast-region&#34;&gt;Mideast Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#great-lakes-region&#34;&gt;Great Lakes Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plains-region&#34;&gt;Plains Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#southeast-region&#34;&gt;Southeast Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#southwest-region&#34;&gt;Southwest Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rocky-mountain-region&#34;&gt;Rocky Mountain Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#far-west-region&#34;&gt;Far West Region&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(bbplot)
library(gganimate)
library(magrittr)
library(lubridate)

# load the data
mortgage &amp;lt;- read_csv(&amp;quot;mortgage.csv&amp;quot;, 
                     col_types = cols(adjustable_margin_5_1_hybrid = col_double(), 
                     adjustable_rate_5_1_hybrid = col_double(), 
                     fees_and_pts_15_yr = col_double(), fees_and_pts_30_yr = col_double(), 
                     fees_and_pts_5_1_hybrid = col_double(), 
                     fixed_rate_15_yr = col_double(), 
                     spread_30_yr_fixed_and_5_1_adjustable = col_double())
                     )
recessions &amp;lt;- read_csv(&amp;quot;recessions.csv&amp;quot;)
state_hpi &amp;lt;- read_csv(&amp;quot;state_hpi.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Week 6 has three data-sets, which are mortgage, recession and state_hpi. Number of variables in each data-set is less than 10. You can acquire the data-set from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-05&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week6&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  What happened in the year 2000 !, and the 2007 recession has made some drastic changes in US avg and the Price index for all regions.  What is happening to Hawaii? Code:  &lt;a href=&#34;https://t.co/WuZD9k8X3S&#34;&gt;https://t.co/WuZD9k8X3S&lt;/a&gt; &lt;a href=&#34;https://t.co/0ecmqnUsrJ&#34;&gt;pic.twitter.com/0ecmqnUsrJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1093047376879775744?ref_src=twsrc%5Etfw&#34;&gt;February 6, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;According to the description there is not much of variation in the recession data-set, but this is not the case in other two data-sets.&lt;/p&gt;
&lt;div id=&#34;mortgage&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mortgage&lt;/h1&gt;
&lt;p&gt;Mortgage data-set has 9 variables with 8 of them are related to the financial sector and one is refereed to date. So the below analysis or interpretation will be values changing over time. These values will be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fixed Rate 30 Years&lt;/li&gt;
&lt;li&gt;Fixed Rate 15 Years&lt;/li&gt;
&lt;li&gt;Fees and Percentage Points (30 Years) of the loan amount.&lt;/li&gt;
&lt;li&gt;Fees and Percentage Points (15 Years) of the loan amount.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;fixed-rate-30-years-from-1971-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Rate 30 Years from 1971 to 2018&lt;/h2&gt;
&lt;p&gt;Each week the Fixed Rate of 30 Years has been set and I am exploring how it changes in each year from 1971 to 2018. We can clearly see in the early Weeks of 1980 it has significantly increased higher than 17.5%, but in early 1970 it was only 7.5%.&lt;/p&gt;
&lt;p&gt;By 1990 it has dropped to 7.5% and this pattern continues further until year 2018 where in December the Fixed Rate of 30 Years is slightly less than 5%.&lt;/p&gt;
&lt;p&gt;Each year there can be one of the below patterns I mentioned if the year is divided into two half’s.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First and Second Half of the Year hold the same Percentage points.&lt;/li&gt;
&lt;li&gt;First Half of the Year has Higher percentage Points than the second half.&lt;/li&gt;
&lt;li&gt;Vice versa of 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(mortgage,aes(x=factor(year(date)),y=fixed_rate_30_yr,color=week(date)))+
          geom_jitter()+transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
          shadow_mark()+xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fixed Rate 30 Year Mortgage (%)&amp;quot;)+
          ggtitle(&amp;quot;Fixed Rate 30 Year Morgage Change by the Year: {round(frame_time)}&amp;quot;)+
          labs(color=&amp;quot;Week of the Year&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;,
                axis.text.x =element_text(angle = 90, hjust = 1))
    
animate(p,nframes=48, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fixed%20rate%2030%20years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-rate-15-years-from-1991-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Rate 15 Years from 1991 to 2018&lt;/h2&gt;
&lt;p&gt;From 1991 only we have Fixed Rate for 15 Years and in the beginning we can see the percentage slightly above 8. and over the years it is decreasing while some fluctuations occur. This fluctuations happen in the years of 2000, 2006, 2007 and 2018, where they brake pattern of decreasing.&lt;/p&gt;
&lt;p&gt;In the year 2018 it reaches slightly less than 4% in the first 20 or so weeks, but the last 20 weeks the percentage is above 4%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(mortgage,year(date)&amp;gt;=1991),
          aes(x=factor(year(date)),y=fixed_rate_15_yr,color=week(date)))+
          geom_jitter()+transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
          shadow_mark()+xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fixed Rate 15 Year Mortgage (%)&amp;quot;)+
          ggtitle(&amp;quot;Fixed Rate 15 Year Morgage Change by the Year: {round(frame_time)}&amp;quot;)+
          labs(color=&amp;quot;Week of the Year&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;,
                axis.text.x =element_text(angle = 90, hjust = 1))
    
animate(p,nframes=28, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fixed%20rate%2015%20years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fees-and-points-of-30-years-from-1971-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fees and Points of 30 Years from 1971 to 2018&lt;/h2&gt;
&lt;p&gt;Highest peek occurs in 1983 which is 2.7 and it decreases over the years gradually. While in the year 1971 the points were close to 1. The gradual decrease is not in effect between the years 1995 and 1996 and its clear in the plot. Yet, we can see no other anomaly in the next few years after 1996, while in 2007 it reaches its lowest point of slightly less than 0.3 (Could be related to the Great recession)&lt;/p&gt;
&lt;p&gt;Anyway by year 2018 after this 2007 recession the points have increased but has not reached 1 and is always oscillating between 0.4 and 0.6 in the years of 2015 to 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1&amp;lt;-ggplot(mortgage,aes(x=factor(year(date)),y=factor(fees_and_pts_30_yr),color=week(date)))+
       geom_jitter()+ theme(legend.position = &amp;quot;bottom&amp;quot;,
                            axis.text.x =element_text(angle = 90, hjust = 1))+
       xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fees and Percentage points of the Loan Amount&amp;quot;)+
       labs(color=&amp;quot;Week of the Year&amp;quot;)+
       ggtitle(&amp;quot;Fess and Percentage points (30 Years) of the Loan Amount \n 
                by the Year : {round(frame_time)}&amp;quot;)+
       transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
       shadow_mark()

animate(p1,nframes=48, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fees%20and%20pts%20of%2030%20year-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fees-and-points-of-15-years-from-1991-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fees and Points of 15 Years from 1991 to 2018&lt;/h2&gt;
&lt;p&gt;In 1991 the points are close to 1.9 and it wavers in between 1.6 and 1.8 until 1997. There is a significant drop from 1997 to 1998 where the points end up averaged around 1 and over the years it slowly decreases until year 2007. Where the lowest point of 0.3 occurs.&lt;/p&gt;
&lt;p&gt;After this new low it struggles to maintain any steady increase and rather holds below 0.8 over the next few years until 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(mortgage,year(date)&amp;gt;=1991),
          aes(x=factor(year(date)),y=factor(fees_and_pts_15_yr),color=week(date)))+
       geom_jitter()+ theme(legend.position = &amp;quot;bottom&amp;quot;,
                            axis.text.x =element_text(angle = 90, hjust = 1))+
       xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fees and Percentage points of the Loan Amount&amp;quot;)+
       labs(color=&amp;quot;Week of the Year&amp;quot;)+
       ggtitle(&amp;quot;Fees and Percentage points (15 Year)of the Loan Amount by the Year : {round(frame_time)}&amp;quot;)+
       transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
       shadow_mark()

animate(p,nframes=28, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fees%20and%20pts%20of%2015%20year-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;states&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;States&lt;/h1&gt;
&lt;p&gt;United States of America has 50 states and comparing all of them at the same time is a ludicrous idea. Therefore, I decided to combine few states and compare them as regions. In order to do this clustering I chose the Wikipedia page which was helpful for me.&lt;/p&gt;
&lt;p&gt;There are multiple reasons to make different regions out of the 50 states of USA. But according to the Wikipedia page I figured it would be best to focus on the financial side or to be precise cluster of states based on the “Bureau of Economic Analysis Regions”.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States&#34;&gt;Wikipedia for US Regions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So according to the above choice we have 8 regions clustering 50 states and they are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;New England&lt;/li&gt;
&lt;li&gt;Mideast&lt;/li&gt;
&lt;li&gt;Great Lakes&lt;/li&gt;
&lt;li&gt;Plains&lt;/li&gt;
&lt;li&gt;Southeast&lt;/li&gt;
&lt;li&gt;Southwest&lt;/li&gt;
&lt;li&gt;Rocky Mountain&lt;/li&gt;
&lt;li&gt;Far West&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;new-england-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New England Region&lt;/h2&gt;
&lt;p&gt;Clear visibility of 2007 recession where US Avg and Price Index declining until 2010 and then improving over the next few years. All states begin very closely but end up very differently in 2018 and in troubled times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;CT&amp;quot;|state==&amp;quot;ME&amp;quot;|state==&amp;quot;MA&amp;quot;|
                           state==&amp;quot;NH&amp;quot;| state==&amp;quot;RI&amp;quot;|state==&amp;quot;VT&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20New%20England-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mideast-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mideast Region&lt;/h2&gt;
&lt;p&gt;After the 2007 recession there is clear difference among DC and other states and the gap cannot be ignored at all.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;DE&amp;quot;|state==&amp;quot;DC&amp;quot;|state==&amp;quot;MD&amp;quot;|
                           state==&amp;quot;NJ&amp;quot;| state==&amp;quot;NY&amp;quot;|state==&amp;quot;PA&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Mideast%20Region-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;great-lakes-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Great Lakes Region&lt;/h2&gt;
&lt;p&gt;After year 2000 there is clear difference among the 5 states and it becomes more complex with the 2007 recession and recovery periods. But this is not the case in year 2018 because all five states are now closely intact with the increase with both variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;IL&amp;quot;|state==&amp;quot;OH&amp;quot;|state==&amp;quot;WI&amp;quot;|
                           state==&amp;quot;IN&amp;quot;| state==&amp;quot;MI&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Great%20Lakes-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plains-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plains Region&lt;/h2&gt;
&lt;p&gt;Before the 2007 recession all states behaved very similarly, but this is not the case after year 2011 where North Dakota has a higher Price index and US Average than other states which is clearly seen in the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;IO&amp;quot;|state==&amp;quot;MN&amp;quot;|state==&amp;quot;NE&amp;quot;|
                           state==&amp;quot;KS&amp;quot;| state==&amp;quot;MS&amp;quot;|state==&amp;quot;ND&amp;quot;|state==&amp;quot;SD&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Plains-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;southeast-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Southeast Region&lt;/h2&gt;
&lt;p&gt;Southeast region has alot of states therefore it would be time consuming to compare. Clearly the 2007 recession has a toll on both variables, but not as the effect from year 2000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AL&amp;quot;|state==&amp;quot;FL&amp;quot;|state==&amp;quot;KY&amp;quot;|
                           state==&amp;quot;AR&amp;quot;|state==&amp;quot;GA&amp;quot;|state==&amp;quot;MS&amp;quot;|
                           state==&amp;quot;LA&amp;quot;|state==&amp;quot;NC&amp;quot;|state==&amp;quot;SC&amp;quot;|
                           state==&amp;quot;TN&amp;quot;|state==&amp;quot;VA&amp;quot;|state==&amp;quot;WV&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Southeast-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;southwest-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Southwest Region&lt;/h2&gt;
&lt;p&gt;Before the 2007 recession and after also we can see the clear changes. Before that in year 2000 also we can see rapid changes which lead up-to the recession. The damage done by the recession have not been recovered in some states even by 2018 according to the gap in Price index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AZ&amp;quot;|state==&amp;quot;OK&amp;quot;|
                           state==&amp;quot;TX&amp;quot;|state==&amp;quot;NM&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Southwest-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rocky-mountain-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rocky Mountain Region&lt;/h2&gt;
&lt;p&gt;Changes after 2000 are very different for the 5 states in this region and after the 2007 recession also we can see the rapid set back in Us avg and price index. But this is not the case after 2013 even though it has already made significant amount of divide between the state of MO and other states, which is clearly seen at the end of year 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;CO&amp;quot;|state==&amp;quot;MO&amp;quot;|state==&amp;quot;WY&amp;quot;|
                           state==&amp;quot;ID&amp;quot;| state==&amp;quot;UT&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Rocky%20Mountain-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;far-west-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Far West Region&lt;/h2&gt;
&lt;p&gt;Early 1990 has a sudden raise and it quickly settles down close to year 1998. Where by 2000 all six states share the same price index value, but this changes over time with clear difference among two groups. Each group containing 3 states, but this progress entirely changes by the 2007 recession and its recovery. Because clearly after 2013 there is no more 2 groups, it is now 3 groups. Where state of Hawaii has the highest pricing index and lowest goes to Alaska, this is by the end of year 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AL&amp;quot;|state==&amp;quot;NV&amp;quot;|state==&amp;quot;OR&amp;quot;|
                           state==&amp;quot;CA&amp;quot;| state==&amp;quot;HI&amp;quot;|state==&amp;quot;WA&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Far%20West-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;It might look that I have not done enough justice for the changes which occurred before the year 2000, and I do agree with you. But if I do add them into my consideration this article would be very long. Hopefully, the animated plots clearly indicate the strong changes which occurred in the pre-y2k era.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 5: Dairy Products in USA</title>
      <link>/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fluid-milk-sales&#34;&gt;Fluid Milk Sales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-milk-production&#34;&gt;State Milk Production&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#us-states-and-milk-production-over-the-years&#34;&gt;US States and Milk Production Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summing-states-of-same-regions-over-the-years&#34;&gt;Summing States of Same Regions Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#averaging-regions-considering-all-the-states-over-the-years&#34;&gt;Averaging Regions Considering All the States Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese&#34;&gt;Cheese&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-other&#34;&gt;Cheese with Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-total&#34;&gt;Cheese with Total&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-known-type-names&#34;&gt;Cheese with known Type Names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(tidyverse)
library(magrittr)
library(ggthemr)
library(readr)
library(gganimate)
library(usmap)

# load the data
fluid_milk_sales &amp;lt;- read_csv(&amp;quot;fluid_milk_sales.csv&amp;quot;)
state_milk_production &amp;lt;- read_csv(&amp;quot;state_milk_production.csv&amp;quot;)
clean_cheese &amp;lt;- read_csv(&amp;quot;clean_cheese.csv&amp;quot;)

# load the theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;5 data-sets are given here, but I will be only discussing 3. They are Fluid Milk Sales, State Milk production and Clean Cheese. Clean Cheese has only few rows (48) and few columns (17). This is not the case in Fluid Milk Sales and State Milk production.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Code: &lt;a href=&#34;https://t.co/3bSXxttydA&#34;&gt;https://t.co/3bSXxttydA&lt;/a&gt; Wisconsin and California have a milk interest and it is very high. &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/Ide8jfalyd&#34;&gt;pic.twitter.com/Ide8jfalyd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1090266660785729536?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week5&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fluid-milk-sales&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fluid Milk Sales&lt;/h1&gt;
&lt;p&gt;Fluid Milk Sales has information for 9 different types of milk and how their Consumption in Pounds has changed over time from 1970 to 2017. We can see how Whole and Reduced Fat(2%) type milk are changing over the years with significant amount. Further, We can see how other types are changing from the initial order in 1970 of lower(Eggnog) amount to higher(Whole) amount in pounds over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(fluid_milk_sales)

fluid_milk_sales$pounds&amp;lt;-fluid_milk_sales$pounds/(10^7)

# how sales change over the years for 9 different types of milk
p&amp;lt;-ggplot(fluid_milk_sales,aes(x=fct_inorder(milk_type) ,y=pounds,fill=year))+
       geom_bar(stat=&amp;quot;identity&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       xlab(&amp;quot;Milk Type&amp;quot;)+ylab(&amp;quot;Pounds (in 10^7)&amp;quot;)+
       coord_flip()+
       ggtitle(&amp;quot;Milk Type vs Pounds in year: {round(frame_time)}&amp;quot;)

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Fluid%20milk%20Sales-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(fluid_milk_sales)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;state-milk-production&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;State Milk Production&lt;/h1&gt;
&lt;p&gt;In the 48 years we are separating them by 24 years each for a plot. Using &lt;a href=&#34;https://cran.r-project.org/web/packages/usmap/index.html&#34;&gt;usmap&lt;/a&gt; package I am going to plot it into their respective states in perspective of Milk Produced lbs in 10^6.&lt;/p&gt;
&lt;div id=&#34;us-states-and-milk-production-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;US States and Milk Production Over the Years&lt;/h2&gt;
&lt;p&gt;In the first half of 1970 to 1993 we can see how a few states are having steady increase over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(state_milk_production)

# dividing the milk produced by 10^6
#summary(state_milk_production$milk_produced)
state_milk_production$milk_produced&amp;lt;-state_milk_production$milk_produced/(10^6)

# plot in us map the milk produced by state for years 1970 to 1993
plot_usmap(data=subset(state_milk_production,year &amp;lt;=&amp;quot;1993&amp;quot;),values = &amp;quot;milk_produced&amp;quot;)+
          facet_wrap(~factor(year),ncol = 4)+
          ggtitle(&amp;quot;Over the years Milk production changing in USA&amp;quot;)+
          theme(legend.position = &amp;quot;left&amp;quot;)+
          labs(fill=&amp;quot;lbs in 10^6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/State%20Milk%20production%20Us%20Map-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is similar to the next half which is from 1994 to 2017 as well. Similar increase occurs for the above same states as I see in the below plot. Well it is not very accurately described in the two plots for us to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot in us map the milk produced by state for years 1994 to 2017
plot_usmap(data=subset(state_milk_production,year &amp;gt;&amp;quot;1993&amp;quot;),values = &amp;quot;milk_produced&amp;quot;)+
          facet_wrap(~factor(year),ncol = 4)+
          ggtitle(&amp;quot;Over the years Milk production changing in USA&amp;quot;)+
          theme(legend.position = &amp;quot;left&amp;quot;)+
          labs(fill=&amp;quot;lbs in 10^6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/State%20Milk%20production%20Us%20Map%201-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To understand the above same change over the years clearly I have created a bar plot how the increase occurs. This plot also indicates how much change has occurred over the 48 years for each state who produces milk. Clearly the states California and Wisconsin have higher increase over the years, which is very strong. There are some states which have not produced more amount each year than their previous years.&lt;/p&gt;
&lt;p&gt;The states Wyoming, Rhode island, Hawaii, Delaware and Alaska have very low amount of milk production over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# over the years how milk production has changed for each year in a bar plot
p&amp;lt;-ggplot(state_milk_production,aes(x=state,y=milk_produced,
                                    fill=year))+
      geom_bar(stat=&amp;quot;identity&amp;quot;)+coord_flip()+
      transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()+
      xlab(&amp;quot;State&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
      ggtitle(&amp;quot;States vs Milk Produced in year: {round(frame_time)}&amp;quot;)

animate(p,nframes = 48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/State%20Milk%20production%20over%20the%20years%20for%20states-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-states-of-same-regions-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summing States of Same Regions Over the Years&lt;/h2&gt;
&lt;p&gt;There are 50 states but only 10 regions and not all regions have equal amount of states. Therefore I am going to sum up the milk production for all regions over the years and try to understand if there is any pattern.&lt;/p&gt;
&lt;p&gt;In order to do this I have used the &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/index.html&#34;&gt;dplyr&lt;/a&gt; package and created a function which would sum up the production for each region of each year. Similarly, this function has the ability to get the average production for each region of each year as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# manipulating the data by sum and mean
by_region_sum&amp;lt;-function(i,ch_sum)
{
  if(ch_sum==TRUE)
  {
    # subsetting by summation over all years for each region
    temp&amp;lt;-subset(state_milk_production,year==i,select=c(&amp;quot;region&amp;quot;,&amp;quot;milk_produced&amp;quot;)) %&amp;gt;%
                  group_by(region) %&amp;gt;%
                  summarise_each(funs(sum))
  
  output&amp;lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)
  }
  else
  {
    # subsetting by Average over all years for each region    
    temp&amp;lt;-subset(state_milk_production,year==i,select=c(&amp;quot;region&amp;quot;,&amp;quot;milk_produced&amp;quot;)) %&amp;gt;%
                  group_by(region) %&amp;gt;%
                  summarise_each(funs(mean))
  
    output&amp;lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)
  }
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the by_region_sum function I am now finding the sum as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting by summation
milk_region_data&amp;lt;-rbind.data.frame(by_region_sum(1970,T),by_region_sum(1971,T),
                                   by_region_sum(1972,T),by_region_sum(1973,T),
                                   by_region_sum(1974,T),by_region_sum(1975,T),
                                   by_region_sum(1976,T),by_region_sum(1977,T),
                                   by_region_sum(1978,T),by_region_sum(1979,T),
                                   by_region_sum(1980,T),by_region_sum(1981,T),
                                   by_region_sum(1982,T),by_region_sum(1983,T),                   
                                   by_region_sum(1984,T),by_region_sum(1985,T),               
                                   by_region_sum(1986,T),by_region_sum(1987,T),                 
                                   by_region_sum(1988,T),by_region_sum(1989,T),
                                   by_region_sum(1990,T),by_region_sum(1991,T), 
                                   by_region_sum(1992,T),by_region_sum(1993,T), 
                                   by_region_sum(1994,T),by_region_sum(1995,T), 
                                   by_region_sum(1996,T),by_region_sum(1997,T), 
                                   by_region_sum(1998,T),by_region_sum(1999,T), 
                                   by_region_sum(2000,T),by_region_sum(2001,T),
                                   by_region_sum(2002,T),by_region_sum(2003,T),   
                                   by_region_sum(2004,T),by_region_sum(2005,T),                 
                                   by_region_sum(2006,T),by_region_sum(2007,T), 
                                   by_region_sum(2008,T),by_region_sum(2009,T),
                                   by_region_sum(2010,T),by_region_sum(2011,T),
                                   by_region_sum(2012,T),by_region_sum(2013,T),
                                   by_region_sum(2014,T),by_region_sum(2015,T),
                                   by_region_sum(2016,T),by_region_sum(2017,T)                  
                                   )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we consider the summation we can see clearly how centered and very limited variation is there for some regions such as Southeast, Northern Plains, Delta States, Corn Belt and Appalachian. There is some variation in the Northeast region. Clear and highest variation is in for Pacific, Mountain and Lake States regions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise total milk production changing  over the year 
ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Total Milk Produced by Year in All Regions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Summation%20by%20region%20for%20jitter-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below is the same graph with points animated by year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise total milk production changing  over the year animated
p&amp;lt;-ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Total Milk Produced for All Regions for Year: {frame_time}&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()
        
animate(p,nframes=48,fps=1)        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Summation%20by%20region%20for%20jitter%20by%20year%20animation-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;averaging-regions-considering-all-the-states-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Averaging Regions Considering All the States Over the Years&lt;/h2&gt;
&lt;p&gt;If we consider the same approach but for the average of each region we can develop the same two plots. Here also we can see the same variation and centering for points for the same regions over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting by average
milk_region_data_new&amp;lt;-rbind.data.frame(by_region_sum(1970,F),by_region_sum(1971,F),
                                   by_region_sum(1972,F),by_region_sum(1973,F),
                                   by_region_sum(1974,F),by_region_sum(1975,F),
                                   by_region_sum(1976,F),by_region_sum(1977,F),
                                   by_region_sum(1978,F),by_region_sum(1979,F),
                                   by_region_sum(1980,F),by_region_sum(1981,F),
                                   by_region_sum(1982,F),by_region_sum(1983,F),                   
                                   by_region_sum(1984,F),by_region_sum(1985,F),               
                                   by_region_sum(1986,F),by_region_sum(1987,F),                 
                                   by_region_sum(1988,F),by_region_sum(1989,F),
                                   by_region_sum(1990,F),by_region_sum(1991,F), 
                                   by_region_sum(1992,F),by_region_sum(1993,F), 
                                   by_region_sum(1994,F),by_region_sum(1995,F), 
                                   by_region_sum(1996,F),by_region_sum(1997,F), 
                                   by_region_sum(1998,F),by_region_sum(1999,F), 
                                   by_region_sum(2000,F),by_region_sum(2001,F),
                                   by_region_sum(2002,F),by_region_sum(2003,F),   
                                   by_region_sum(2004,F),by_region_sum(2005,F),                 
                                   by_region_sum(2006,F),by_region_sum(2007,F), 
                                   by_region_sum(2008,F),by_region_sum(2009,F),
                                   by_region_sum(2010,F),by_region_sum(2011,F),
                                   by_region_sum(2012,F),by_region_sum(2013,F),
                                   by_region_sum(2014,F),by_region_sum(2015,F),
                                   by_region_sum(2016,F),by_region_sum(2017,F)                  
                                   )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is the plot for the average of regions over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise average milk production changing  over the year 
ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+ 
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Average Milk Produced by Year in All Regions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/average%20by%20region%20for%20jitter-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The same plot is now animated for each year and all regions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise average milk production changing  over the year animated
p&amp;lt;-ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+ shadow_mark()+       
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Average Milk Produced for All Regions for Year: {frame_time}&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
        
animate(p,nframes=48,fps=1)        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/average%20by%20region%20for%20jitter%20by%20year%20animation-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(state_milk_production)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cheese&lt;/h1&gt;
&lt;p&gt;16 types of cheese are provided in this clean cheese data-set. I will divide these types into 3 types and will not consider few types of cheese. The unit of measurement for the consumption is lbs per person.&lt;/p&gt;
&lt;div id=&#34;cheese-with-other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with Other&lt;/h2&gt;
&lt;p&gt;I am going to consider the types American Other, Italian Other and Swiss for this plot. Red color indicates to American Other, yellow color refers to Italian other and blue for Swiss. Alot of fluctuation for American other type, but this is not the case for Swiss type cheese. There is steady increase for the Italian other type cheese over the years. All of these are less than 4 lbs per person and it is animated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=`American Other`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=`Italian other`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=Swiss),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          transition_time(Year)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Cheese%20with%20other-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese-with-total&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with Total&lt;/h2&gt;
&lt;p&gt;This is also an animated plot but for the cheese types which has the word Total. They are Total American Cheese, Total Italian Cheese, Total Natural Cheese and Total Processed Cheese Products with the colors represented respectively red, yellow, blue and green.&lt;/p&gt;
&lt;p&gt;All the Consumption units are in between 0 to 40 lbs per person. Clearly Total Natural Cheese has a steady amount of increase from 1970(slightly above 10) to 2017(approximately less than 40). Considering the other three types we can see it is not the same order that it is in 1970 over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=`Total American Chese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=`Total Italian Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=`Total Natural Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          geom_point(aes(y=`Total Processed Cheese Products`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;green&amp;quot;)+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+  
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          transition_time(Year)+ 
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Cheese%20with%20Total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese-with-known-type-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with known Type Names&lt;/h2&gt;
&lt;p&gt;Next group of cheese types include Cheddar, Mozzarella, Brick, Processed Cheese and Foods &amp;amp; spreads for the colors representing respectively red, yellow, blue, green and white. Clearly Cheddar and mozzarella type cheese are are mostly consumed by 2017 above 10 lbs per person, but this is not the case in 1970 where consumption is less than 6 lbs per person.&lt;/p&gt;
&lt;p&gt;Well Processed Cheese and Foods &amp;amp; Spreads have changed very small over the years. The consumption is always less than 6 lbs per person. This is not the case for Brick type cheese where the consumption is close to zero over the years from 1970 until 2017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=Cheddar),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=Mozzarella),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=`Brick`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          geom_point(aes(y=`Processed Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;green&amp;quot;)+
          geom_point(aes(y=`Foods and spreads`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;white&amp;quot;)+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          transition_time(Year)+ 
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/index_files/figure-html/Cheese%20with%20known-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build Your Own Website or Blog Using R, RStudio and R Packages.</title>
      <link>/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#materials&#34;&gt;Materials&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.&#34;&gt;“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#up-and-running-with-blogdown-by-alison-presmanes-hill.&#34;&gt;“Up and Running with Blogdown by Alison Presmanes Hill.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown-websites.&#34;&gt;“Rmarkdown Websites.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.&#34;&gt;“How to make an RMarkdown Website by Nick Strayer &amp;amp; Lucy D’Agostino McGowan.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-websites-in-r-by-emily-c-zabor.&#34;&gt;“Creating websites in R by Emily C Zabor.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-blogdown-by-david-selby.&#34;&gt;“Getting Started with Blogdown by David Selby.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-blogdown-by-danielle-navarro.&#34;&gt;“Getting Started with Blogdown by Danielle Navarro.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blogdown-by-peters-blog.&#34;&gt;“Blogdown by Peter’s Blog.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#academic-by-george-cushen&#34;&gt;“Academic by George Cushen”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#happy-git-and-github-for-the-user.&#34;&gt;“Happy Git and GitHub for the User.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.&#34;&gt;“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.&#34;&gt;“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#my-personal-website-blog&#34;&gt;My Personal Website / Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;R enthusiasts are focused on developing packages and websites to promote their profile and share their knowledge to the world. Recently released packages such as hugo, blogdown, Rmarkdown, bookdown has played a significant amount of role in this popularity for R statistical software among general users and academics in every field of statistics.&lt;/p&gt;
&lt;p&gt;Due to this reason, I also wanted to develop my own R package to solve problem in hand and share it with the #rstats community. Even though Social Media is a strong way of sharing this amount of information, it is not sturdy over time. To resolve this only I chose to develop my own website using R and supportive tools from R. You are reading this post on my website which I developed in a very short period of time and have being maintaining regularly by posting articles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;materials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Materials&lt;/h1&gt;
&lt;p&gt;I could have written an extensive and long article describing how I developed this website with screenshots and explanatory steps. As it should be a valuable experience I am only going to give you the materials which were used with important points with facts. Further, I shall give you certain specifics of my own website.&lt;/p&gt;
&lt;div id=&#34;making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://slides.yihui.name/2017-rstudio-webinar-blogdown-Yihui-Xie.html#1&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Presentation of 20 slides.&lt;/li&gt;
&lt;li&gt;Most of the basic information for packages which are necessary for website development.&lt;/li&gt;
&lt;li&gt;Brief introduction about the website structure and process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;up-and-running-with-blogdown-by-alison-presmanes-hill.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Up and Running with Blogdown by Alison Presmanes Hill.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://alison.rbind.io/post/up-and-running-with-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brief information for blogdown and other development materials.&lt;/li&gt;
&lt;li&gt;Described information Deployment and maintaining the website with other tools related to R and Rstudio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown-websites.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Rmarkdown Websites.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-13.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Briefest description about using Rmarkdown/Rmd files for website development.&lt;/li&gt;
&lt;li&gt;There are few other links which could be considered useful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“How to make an RMarkdown Website by Nick Strayer &amp;amp; Lucy D’Agostino McGowan.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nickstrayer.me/RMarkdown_Sites_tutorial/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One example sample website developed and explained briefly by the authors.&lt;/li&gt;
&lt;li&gt;Several links to spark curiosity about website development using Rmarkdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-websites-in-r-by-emily-c-zabor.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Creating websites in R by Emily C Zabor.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation on different type of websites which can be produced by R.&lt;/li&gt;
&lt;li&gt;Deployment and additional requirements for website development.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started-with-blogdown-by-david-selby.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Getting Started with Blogdown by David Selby.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://selbydavid.com/wrugdown/2017/05/10/getting-started-with-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Limited amount of information regarding website development.&lt;/li&gt;
&lt;li&gt;This was written for a talk for the “Warwick R User Group Talk” in 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started-with-blogdown-by-danielle-navarro.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Getting Started with Blogdown by Danielle Navarro.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://djnavarro.net/post/2018-04-27-starting-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extensive amount of information about website development using blogdown.&lt;/li&gt;
&lt;li&gt;More than enough information about the insides of the website.&lt;/li&gt;
&lt;li&gt;Detailed steps of writing posts and changing elements of the website.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;blogdown-by-peters-blog.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Blogdown by Peter’s Blog.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://notes.peter-baumgartner.net/tags/blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Tutorials explaining from scratch about how to develop your website using blogdown.&lt;/li&gt;
&lt;li&gt;Tutorial 1 explains about website themes and setting up R and Rstudio.&lt;/li&gt;
&lt;li&gt;Tutorial 2 is about hosting the website locally or sharing the work with others.&lt;/li&gt;
&lt;li&gt;Tutorial 3 will be information about getting the site live.&lt;/li&gt;
&lt;li&gt;Tutorial 4 describes how to bring the website online through Netlify or GitHub.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;academic-by-george-cushen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Academic by George Cushen”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the information about the academic theme this is very popular among people in rstats community.&lt;/li&gt;
&lt;li&gt;Descriptive amount of information about changing elements in the academic theme to make it more homely fo the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;happy-git-and-github-for-the-user.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Happy Git and GitHub for the User.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Git and GitHub with R with all the information that anyone needs to know.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Book with all the information related to Rmarkdown files.&lt;/li&gt;
&lt;li&gt;No need to look anywhere for clarification regarding Rmarkdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar to the previous two books this is also the most useful for blogdown.&lt;/li&gt;
&lt;li&gt;No need to look anywhere else for further understanding regarding blogdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-personal-website-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;My Personal Website / Blog&lt;/h1&gt;
&lt;p&gt;Information related to my website / blog will be discussed here. Mostly encouraging other people to make necessary changes in the original template to satisfy their curiosity and interest.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I also used the academic theme but made alot of changes.&lt;/li&gt;
&lt;li&gt;These changes include with a new css, color changing, font changing and others.&lt;/li&gt;
&lt;li&gt;In the Home page I explored adding icons, heading names and changing the header successfully.&lt;/li&gt;
&lt;li&gt;While writing blog posts I was able to learn inventive ways to make them interesting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now that is all I have done in related to developing my own website. It should be your own choice what you are going to develop, a website / blog. Mine is a website where articles are regularly posted. Further, it should be noted that the above materials were active when I was writing this post, therefore if it is not active do use google and try to find it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Olympic : Rshiny Approach</title>
      <link>/post/olympicrshiny/olympic-rshiny-approach/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/olympicrshiny/olympic-rshiny-approach/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#material-useful-for-rsiny-development&#34;&gt;Material Useful for Rsiny Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-use-the-olympic-rshiny-app&#34;&gt;How To Use The Olympic Rshiny App ?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1&#34;&gt;Step 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2&#34;&gt;Step 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3&#34;&gt;Step 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4&#34;&gt;Step 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5&#34;&gt;Step 5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6&#34;&gt;Step 6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7&#34;&gt;Step 7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-8&#34;&gt;Step 8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Rshiny is very popular in the rstats community. The glamourous interface and functionality has helped for this level of popularity. In perspective of using an Rshiny App anyone can use it with minimal amount of knowledge. Which is very useful in bringing statistical analysis to consumers or general public without any trouble.&lt;/p&gt;
&lt;p&gt;I initially wanted to develop an Rshiny App for my fitODBOD package, but I thought it would be best to test the waters. That is what I have done here. Using the Olympic data from kaggle I have found a very convenient way to understand specific results for a choosen country from the Rshiny App.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/screenshots/olympicrshiny.PNG&#34; /&gt; At the beginning I wanted to compare between diferent countries or sports or seasons and come to a conclusion. Well, what kind of a conclusion would make sense bothered me, therefore I turned towards an Rshiny Approach.&lt;/p&gt;
&lt;p&gt;This data-set includes information from 1896 to 2016. Analyzing the data-set would take tedious amount of time and in my opinion unnecessary amount of complications will arise when it comes to concluding. Information from the data-set includes about Medals, participants name, country, sports, events, season and year.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results/home&#34;&gt;Kaggle Olympic Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://amalan-con-stat.shinyapps.io/olympic/&#34;&gt;Olympic Rshiny App&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/Olympic-Data-Rshiny-&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;material-useful-for-rsiny-development&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Material Useful for Rsiny Development&lt;/h2&gt;
&lt;p&gt;Easiest way to build your own shiny app is to refer the official website. It provides an extensive amount of information regarding Rshiny development. Already developed Rshiny Apps and Templates are also available, which would come in handy. Further, when you do start an Rshiny App through Rstudio you will initially receive a sample App with its code. A few tweaks and changes would lead to necessary changes that you need.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Official Rshiny Website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-use-the-olympic-rshiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How To Use The Olympic Rshiny App ?&lt;/h2&gt;
&lt;p&gt;Instructions are also listed in the Rshiny App panel.&lt;/p&gt;
&lt;div id=&#34;step-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 1&lt;/h3&gt;
&lt;p&gt;First Choose a country that you want to study and find the three letter NOC CODE from the “NOC CODE” tab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 2&lt;/h3&gt;
&lt;p&gt;Choose the “GRAPH” tab to understand how medals were won for a chosen country over the years with respective to Gender.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 3&lt;/h3&gt;
&lt;p&gt;Choose the “DATA” tab to look at the data for the chosen. Further you can scroll through this data and find specific attendee’s Name, Sex, Age, Year, Season, City, Sport, Event and Medal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 4&lt;/h3&gt;
&lt;p&gt;Using “DESCRIBE” tab you can simply study the descriptive statistics for the data of the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 5&lt;/h3&gt;
&lt;p&gt;“G/Years” tab is there to explain the Gender representation over the years of the chosen country through a bar plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 6&lt;/h3&gt;
&lt;p&gt;“S/Years” tab shows a bar plot which has the representation of the Gender of the Sports event participants of the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 7&lt;/h3&gt;
&lt;p&gt;“H/W/Sport” tab explores how participants Height and Weight relationship for each Sporting event with respective to Gender for the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-8&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 8&lt;/h3&gt;
&lt;p&gt;Repeat the Steps 1 to 7 and be amused of the results from different countries.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PLEASE NOTE&lt;/em&gt; - You should remember that as a user of this Rshiny Application not all countries have won atleast one medal at the Olympics. At these occurences “MEDAL GRAPH” tab does not show any graph but only an error. This can be confirmed by the “DESCRIBE” tab which will produce the summary for that chosen country.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 4: Prison Data</title>
      <link>/post/tidytuesday2019/week4/week-4-prison-data/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week4/week-4-prison-data/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary&#34;&gt;Prison Summary&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-gender&#34;&gt;Prison Summary With Gender&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-ethnicity&#34;&gt;Prison Summary with Ethnicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-other-and-total&#34;&gt;Prison Summary with Other and Total&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pretrial-summary-with-gender-and-total&#34;&gt;Pretrial Summary with Gender and Total&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-data-of-incarceration-trends&#34;&gt;Complete Data of Incarceration Trends&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-rural-area&#34;&gt;Rape Crimes over the Years in States of Rural Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-small-or-mid-area&#34;&gt;Rape Crimes over the Years in States of Small or Mid Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-suburban-area&#34;&gt;Rape Crimes over the Years in States of Suburban Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-urban-area&#34;&gt;Rape Crimes over the Years in States of Urban Area&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(magrittr)
library(gganimate)
library(ggthemr)

# load the theme
ggthemr(&amp;quot;flat dark&amp;quot;)

# load the data
pretrial_summary &amp;lt;- read_csv(&amp;quot;pretrial_summary.csv&amp;quot;)

prison_summary &amp;lt;- read_csv(&amp;quot;prison_summary.csv&amp;quot;)

incarceration_trends&amp;lt;-read_csv(&amp;quot;incarceration_trends.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TidyTuesday Week 4 of 2019 is focused on prison data. You can find the data &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-22&#34;&gt;here&lt;/a&gt;. There are 5 csv files, clearly 2 files are a summary of the main data, which are Prison Summary and Pretrial Summary.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Code: &lt;a href=&#34;https://t.co/x1Wiq1JzYS&#34;&gt;https://t.co/x1Wiq1JzYS&lt;/a&gt;  . opinion: Decline of prisoners rate after certain periods for different regions. &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/VkOJDJJBoq&#34;&gt;pic.twitter.com/VkOJDJJBoq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1087702502240407552?ref_src=twsrc%5Etfw&#34;&gt;January 22, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I have mainly focused on these two data-sets here. Further in order of curiosity I did take a peak at a main data file, which is incarceration_trends.csv.&lt;/p&gt;
&lt;div id=&#34;prison-summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Prison Summary&lt;/h1&gt;
&lt;p&gt;Prison summary data-set has 4 variables. The year of records begin from 1983 and ends in 2015. The unit of incarceration in is Rate per 100,000. Considering the population categories there are clearly 4 sub groups. Each of these sub groups have been plotted here. Further the variable ‘urbanicity’ is simply grouping the observations according to the developed status. Such as rural, small/mid, suburban and urban.&lt;/p&gt;
&lt;div id=&#34;prison-summary-with-gender&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary With Gender&lt;/h2&gt;
&lt;p&gt;The population increase has an effect on it according to the below plot. Urban area has an increase in these prisoners over the years but after mid 1990 there is a decline. This is true for males. Next considering the suburban area this is quite similar as before, only difference is that the decline begins in year 2005.&lt;/p&gt;
&lt;p&gt;Considering rural area there is a clear increase of prisoners for both genders in the years. There is an anomaly in year 1986 with alot of prisoners for males. Both rural and small/mid areas behave similarly for both genders as the increase rate gets somewhat slower after 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category==&amp;quot;Male&amp;quot; | pop_category==&amp;quot;Female&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
  facet_wrap(~urbanicity)+geom_area()+
  transition_reveal(year)+ labs(fill=&amp;quot;Gender&amp;quot;)+
  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
  theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
  scale_y_continuous(breaks = seq(0,1750,250),labels=seq(0,1750,250))+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
  ggtitle(&amp;quot;Gender change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/prison%20summary%20Male%20and%20Female-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prison-summary-with-ethnicity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary with Ethnicity&lt;/h2&gt;
&lt;p&gt;5 ethnicity types are considered here which are Asian, Black, Latino, White and Native American. From 1980 only we can see the active prisoners of Latino and Native American ethnicity. Over the years we can the increase of prisoners for African American Community. The increase is very high considering the other ethnicity types.&lt;/p&gt;
&lt;p&gt;Asian ethnicity people have prisoners but it is only negligible considering the other ethnicity types. Except the suburban area others have an increase rate until 2005 and there is a decline followed in the next years. This is not the case for suburban area. Here less change after year 2000 and the decline begins only in year 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category == &amp;quot;Asian&amp;quot; | pop_category==&amp;quot;Black&amp;quot; |
                      pop_category == &amp;quot;Latino&amp;quot; |pop_category==&amp;quot;White&amp;quot; |
                      pop_category == &amp;quot;Native American&amp;quot; ) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
  facet_wrap(~urbanicity)+geom_area()+
  transition_reveal(year)+ labs(fill=&amp;quot;Ethnicity&amp;quot;)+
  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
  theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
  scale_y_continuous(breaks = seq(0,5250,250),labels=seq(0,5250,250))+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
  ggtitle(&amp;quot;Ethnicity change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/prison%20summary%20Ethnicity-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prison-summary-with-other-and-total&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary with Other and Total&lt;/h2&gt;
&lt;p&gt;The Other category is no longer active since 1989, but before that we can see different anomalies in all four areas. Clearly urban area has more prisoners and final place goes to suburban according to the below area plot.&lt;/p&gt;
&lt;p&gt;Rural area has an increase in prisoners over the years and there is no decline. This is not the case for small/mid and suburban areas. Urban areas has a sudden decline in between year 1995 to 2000 and again there is a steep decline after year 2005.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category == &amp;quot;Other&amp;quot; | pop_category==&amp;quot;Total&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
    facet_wrap(~urbanicity)+geom_area()+
    transition_reveal(year)+ labs(fill=&amp;quot;Category&amp;quot;)+
    scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
    theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
    scale_y_continuous(breaks = seq(0,1000,100),labels=seq(0,1000,100))+
    xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
    ggtitle(&amp;quot;Total and Other category change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/prison%20summary%20Other%20and%20Total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pretrial-summary-with-gender-and-total&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pretrial Summary with Gender and Total&lt;/h1&gt;
&lt;p&gt;Year 1970 to 2015 is the range of time considered here and the genders male and female are considered with the total. The data-set is for Pretrial prisoners. There is sudden increase after mid 1980s to all the areas. This sudden increase occurs to both genders and the total as well.&lt;/p&gt;
&lt;p&gt;Here, also we can see an odd behavior for urban area in the entire time range with sudden steeps and peaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(pretrial_summary,aes(x=year,y=rate_per_100000,fill=pop_category))+
      geom_area()+facet_wrap(~urbanicity)+
      transition_reveal(year)+ labs(fill=&amp;quot;Category&amp;quot;)+
      scale_x_continuous(breaks=c(1970:2015),labels=c(1970:2015))+
      theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
      scale_y_continuous(breaks = seq(0,800,50),labels=seq(0,800,50))+
      xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
      ggtitle(&amp;quot;Total and Gender category change over the years from 1970-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/pretrial%20summary%20gender%20and%20total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-data-of-incarceration-trends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Complete Data of Incarceration Trends&lt;/h1&gt;
&lt;p&gt;This data-set has all the necessary information related to Incarceration. Further, it includes data for 11 different crime types. Rather than exploring all the crimes I have explored only one crime here, which is Rape.&lt;/p&gt;
&lt;p&gt;The are four areas in concern are rural, suburban, mid/small and urban. 51 states and 4 regions are considered to see the diversity of these prisoners. We have dropped the years from 1970 to 1976, 2015 and 2016 because they had no data. Even the years 1979 and 1993 has missing data but still I am including this in the plot.&lt;/p&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-rural-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Rural Area&lt;/h2&gt;
&lt;p&gt;I developed these plots to understand a pattern in state wise or region wise, apparently its very difficult but still I am keeping these plots here. Well, frankly I think there could be some other better way to visualize the above selective data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;rural&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Rural Areas on the Year : {round(frame_time)}&amp;quot;)

animate(p1,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/All%20Data%20Rape%20Crime%20but%20rural-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-small-or-mid-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Small or Mid Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;small/mid&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Small or Mid Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p2,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/All%20Data%20Rape%20Crime%20but%20small%20or%20mid-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-suburban-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Suburban Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;suburban&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Suburban Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p3,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/All%20Data%20Rape%20Crime%20but%20suburban-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-urban-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Urban Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;urban&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Urban Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p4,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/index_files/figure-html/All%20Data%20Rape%20Crime%20but%20Urban-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 3: Space Agencies and Launches</title>
      <link>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agencies&#34;&gt;AGENCIES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agency-vs-count&#34;&gt;Agency vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#type-vs-count&#34;&gt;Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-vs-count&#34;&gt;Class vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agency-type-vs-count&#34;&gt;Agency Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-count&#34;&gt;State Code vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location-vs-count&#34;&gt;Location vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#start-year-and-end-year-vs-agency&#34;&gt;Start Year and End Year vs agency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#launches&#34;&gt;LAUNCHES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-of-these-missions-vs-category-variables&#34;&gt;Success or Failure of these missions vs Category Variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-launch-year&#34;&gt;Success or Failure vs Launch Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-agency-type&#34;&gt;Success or Failure vs Agency Type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-state-code&#34;&gt;Success or Failure vs State Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-category-over-time-for-success-and-failure&#34;&gt;State Code vs Category Over time for Success and Failure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(ggalt)
library(magrittr)
library(dplyr)
library(ggthemr)
library(gganimate)

# Load the Agency data
agencies&amp;lt;-read_csv(&amp;quot;agencies.csv&amp;quot;)

# Load the Launches data
launches&amp;lt;-read_csv(&amp;quot;launches.csv&amp;quot;)

attach(agencies)
attach(launches)

# load a theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;agencies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;AGENCIES&lt;/h1&gt;
&lt;p&gt;Space related agencies of 74 are in the world from this data set. Another, data set is for launches from the agencies in concern. In the agencies data set there are 19 variables and launches data set has 11 variables. You can find the data set and information regarded to it &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-15&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Early years of space launches had more mistakes and they were owned by the state. After the cold war, there is short of enthusiasm, but now we have an increase in launches. Code: &lt;a href=&#34;https://t.co/R0BLjFOH4U&#34;&gt;https://t.co/R0BLjFOH4U&lt;/a&gt;    &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/6dX338hpPD&#34;&gt;pic.twitter.com/6dX338hpPD&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1085387691393458179?ref_src=twsrc%5Etfw&#34;&gt;January 16, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week3&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;agency-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency vs Count&lt;/h2&gt;
&lt;p&gt;Most amount of launches are from Rakentiye Voiska Strategicheskogo Naznacheniye (RVSN) and it is 1528 and next is Upravleniye Nachalnika Kosmicheskikh Sredstv (UNKS) with 904. Out of the Top 10 places (considering the most launches) it is clear that class D agencies has the most amount (of 5),while 3 from class C agencies and the rest with class B. NASA is in third place with 469 launches and it is a class C agency. My favorite agency Space X (SPX) has launched 65 times and it is a class B agency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=class,fill=class))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(label=agencies$count, hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
       ggtitle(&amp;quot;Space Agency vs Frequency By Class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Agency%20vs%20Count%20and%20class-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, for the same bar plot if we change color according to agency type we have different insight. Top 10 agencies is 90% filled with state ownership and 10% is with private ownership. It should be noted that overall there are only two start-ups and close to 10 have private ownership, rest is state owned. The highest amount of launches for a private ownership is from Arian Space(AE) and for start-up its Space X (SPX). Respectively, their counts are 258 and 65.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=agency_type,fill=agency_type))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(aes(label=count), hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
      labs(color=&amp;quot;Agency Type&amp;quot;,fill=&amp;quot;Agency Type&amp;quot;)+
      ggtitle(&amp;quot;Space Agency vs Frequency By Agency Type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Agency%20vs%20Count%20and%20agency%20type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Type vs Count&lt;/h2&gt;
&lt;p&gt;Type of agencies is very complex because an agency can play multiple roles. Highest amount of count is for O/LA type with 3227 and second place is for LA type with 821 counts. There are 145 agencies with the highest combination of types and this category is O/LA/LV/PL/E/S with 145 counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(type) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),hjust=-0.15)+coord_flip()+
  xlab(&amp;quot;Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Type vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;class-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class vs Count&lt;/h2&gt;
&lt;p&gt;Class C and B has similar amounts of count which is close to 1100 and most launches are from D class agencies with the count of 3584.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;class&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(class) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(class),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Class&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Class vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Class%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;agency-type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency Type vs Count&lt;/h2&gt;
&lt;p&gt;In perspective of agency type there are 4765 state owned launches, but only 67 launches from start-ups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;agency_type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(agency_type) %&amp;gt;% 
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(agency_type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Agency%20Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Count&lt;/h2&gt;
&lt;p&gt;Close to 2500 missions were launched by Soviet Union and 1709 were done by Unite States.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;state_code&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(state_code) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(state_code),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/State%20Code%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Location vs Count&lt;/h2&gt;
&lt;p&gt;More than 1500 launches are from Mosvka? and exactly 1204 launches from Moskva. Further, 469 launches from Washington D.C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;location&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(location) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(location),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;Location&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Location vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Location%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;start-year-and-end-year-vs-agency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Start Year and End Year vs agency&lt;/h2&gt;
&lt;p&gt;Below is a Dumbbell plot to see at the agencies which are no longer active. Before 1960 there was very small activity and they are all owned by the state. With the American and Russian Space race we have private sector also being part of this adventure, but most of them are ending their service around the first half of 1990. There is more activity after this regularly but they are short lived for these agencies. Royal Aircraft Establishment (RAE) has long life for space adventure which was begun around late 1915, and ends its service in around 1990.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=as.numeric(substr(tstart,1,4)),xend=as.numeric(substr(tstop,1,4)),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 2.75,size=1.25)+ 
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+ 
  scale_x_continuous(breaks=seq(1910,2020,5),labels=seq(1910,2020,5))+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If We Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Start%20year%20and%20End%20Year%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It should be effectively noted that “-” means still active and “*&amp;quot; means unknown in my perspective. Here we cannot consider the years as numeric because of the characters used. Agencies like NASA and Space X are still active according to my knowledge therefore I considered the above assumption for characters. Most of these agencies are state owned and after Space X there is Rocket Lab USA (RLABU). Most of these agencies were launched after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=substr(tstart,1,4),
           xend=substr(tstop,1,4),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 3,size=1.25)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If Do Not Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Start%20year%20and%20End%20Year%20Unknown%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;launches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;LAUNCHES&lt;/h1&gt;
&lt;p&gt;Counts of above missions are mentioned here thoroughly.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-of-these-missions-vs-category-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Success or Failure of these missions vs Category Variables&lt;/h2&gt;
&lt;p&gt;There are few categorical variables which could be associated with the success or failure of these missions.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-vs-launch-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Launch Year&lt;/h3&gt;
&lt;p&gt;Less mistakes over the year with technologies improving and in between 1960 to 1990 we can see alot of launches always above 100 per year. This enthusiasm no longer exists until 2005. After 2005 there is positive increase in launches and failures also less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=factor(launch_year),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;Years&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Years vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,150,10),breaks=seq(0,150,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Success%20or%20Failure%20vs%20Launch%20Year-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-agency-type&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Agency Type&lt;/h3&gt;
&lt;p&gt;State owned agencies has more failures than private and start-ups because it would be costly. More than 4750 launches are from state owned agencies but in them more than 500 launches are failures. Even though private owned agencies has a history from 1990 they have less than 1000 launches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(agency_type)),fill=category))+
  geom_bar()+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,5000,250),breaks=seq(0,5000,250))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Success%20or%20Failure%20vs%20Agency%20Type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-state-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs State Code&lt;/h3&gt;
&lt;p&gt;Soviet Union (SU) and United States (US) has the most dominant appearance in this field. More than 2400 launches from SU and for US it is more than 1700 launches. Failures also considerably higher for SU and US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(state_code)),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,2500,100),breaks=seq(0,2500,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/Success%20or%20Failure%20vs%20State%20Code-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-category-over-time-for-success-and-failure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Category Over time for Success and Failure&lt;/h2&gt;
&lt;p&gt;Animated jitter plot here explains how over the years these launches occur based on States and Success(O) or Failure(F).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(launches,aes(y=category,x=state_code,color=agency_type))+
       geom_jitter()+
       labs(title = &amp;quot;States vs Success or Failure by : {round(frame_time,0)}&amp;quot;,
            x=&amp;quot;State Code&amp;quot;,y= &amp;quot;Success or Failure&amp;quot;)+
       transition_time(launch_year)+ease_aes(&amp;#39;linear&amp;#39;)+
       labs(color=&amp;quot;Agency Type&amp;quot;)

animate(p,fps=2,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/index_files/figure-html/State%20code%20vs%20Category%20by%20time-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Developing an R package</title>
      <link>/post/yourownpackage/developing-an-r-package/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/yourownpackage/developing-an-r-package/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#credit-to-people-of-r-community&#34;&gt;Credit to People of R Community&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coding-standards-coding-to-understand&#34;&gt;1) Coding Standards (Coding to Understand)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#package-structure&#34;&gt;2) Package Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#description-file&#34;&gt;3) DESCRIPTION file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#readme-file&#34;&gt;4) README file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-directory&#34;&gt;5) /R directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-directory&#34;&gt;6) /data directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tests-directory&#34;&gt;7) /tests directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#man-directory&#34;&gt;8) /man directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#namsespace-file&#34;&gt;9) NAMSESPACE file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbuildignore-file&#34;&gt;10) .Rbuildignore file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gitignore-file&#34;&gt;11) .gitignore file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-package&#34;&gt;Building the Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributing-the-package&#34;&gt;Distributing the Package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;R package development is no longer as it was before 2010 because now most of the work can be done by just a simple mouse-click or with the use of a function. My intention of writing this blog post is not to give a thorough demonstration of how to develop your own R package. But it will briefly explain the process with the most important steps, and will include valuable blog posts and websites which helped me to develop my own R package &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credit-to-people-of-r-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Credit to People of R Community&lt;/h1&gt;
&lt;p&gt;I would definitely recommend to read all of these books and start your package development. Or at least make step by step progress in your work while reading them. If you have a basic knowledge regarding R, R studio, CRAN and writing programs, they are more than enough for you to start.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R packages by Hadley Wickam&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This website contains everything that is in the book. The basic things related to an R package development process are structured properly here. It is very useful to read this book/website. The package structure, the type of files necessary, how should the writing be in these files and further, what kind of ways can we use to achieve the final outputs. The book is mainly focused on producing an R package which can be updated with the highest standards using reproducing ability. Such as CRAN standards and GitHub releases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;Writing R Extensions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything related to package development is included here with clear instructions. This document is provided by the CRAN project to make package development more friendlier. It includes the official standards for files and naming conventions related to R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf&#34;&gt;Creating R packages: A Tutorial by Friedrich Leish&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A brief article explaining about writing functions, classes and methods for R package development. This is very abstract and useful in package development specially as it is focusing on object oriented programming and S formulas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf&#34;&gt;R for Beginners by Emmanuel Paradis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A book explaining R and its ability in detail, for example regarding functions, data, abilities and limitations of R. There is also a section for R packages, which has valuable information. Writing functions is very crucial in R package development therefore going through this document is worth. Several packages also include data-sets in them. While you develop functions similarly we can develop data-sets as well. There are sections which includes information regarding data-sets as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf&#34;&gt;The Art of R Programming by Norman Matloff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another book that will be useful in understanding how R functions and data-sets can be used in R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.burns-stat.com/pages/Tutor/R_inferno.pdf&#34;&gt;The R Inferno by Patrick Burns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Useful book related to object oriented programming, functions and data objects related to R. Very thorough and scrutinized information with valuable explanation which makes things more clearer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;Cheat Sheets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Easy implementation of packages mentioned in these cheatsheets. Very essential for someone who is interested in doing R related stuff efficient and eloquent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes related to RMarkdown, very useful for vignette building.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Happy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using GitHub for package development is very useful, specially when it comes to sharing and version control. This book explains it all with simplicity.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:900px;height:480px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph TB;\n           A(Code with Standards and comment)--&gt;C(Create R package);\n           C--&gt;D(R package &lt;br&gt; Structure);\n           D--&gt;E(DESCRIPTION file);\n           D--&gt;F(README file);\n           D--&gt;G(/R directory);\n           D--&gt;H(/data directory);\n           D--&gt;I(/tests directory);\n           D--&gt;J(/man directory);\n           D--&gt;K(NAMESPACE file);\n           D--&gt;L(/vignettes &lt;br&gt; directory);\n           D--&gt;M(NEWS.md file);\n           E--&gt;O(Build &lt;br&gt; the package);\n           O--&gt;N(Source,Bundle,Binary,Installed,In Memory);\n           F--&gt;O; G--&gt;O; H--&gt;O; I--&gt;O; J--&gt;O; K--&gt;O;  L--&gt;O; M--&gt;O;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;coding-standards-coding-to-understand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1) Coding Standards (Coding to Understand)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Focus on naming conventions.&lt;/li&gt;
&lt;li&gt;Focus on input parameters and outputs.&lt;/li&gt;
&lt;li&gt;Focus on indentation.&lt;/li&gt;
&lt;li&gt;Comment regularly to make sense of the functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/YourOwnPackage/codingstandards/figure1.PNG&#34; alt=&#34;Sample Code&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sample Code&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;package-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2) Package Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very Important.&lt;/li&gt;
&lt;li&gt;Initially few files will be originated in the designated project folder.&lt;/li&gt;
&lt;li&gt;Over time we might add folders or create files manually.&lt;/li&gt;
&lt;li&gt;Example - tests directory, README.Rmd, …&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure1.PNG&#34; /&gt; Package structure inside your project folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;description-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3) DESCRIPTION file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;File explaining basic things related to your package.&lt;/li&gt;
&lt;li&gt;Example - package name, other packages needed, authors name, …&lt;/li&gt;
&lt;li&gt;Can edit manually or use specific R package.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure1.PNG&#34; /&gt; After changes the DESCRIPTION file&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readme-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4) README file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very much optional.&lt;/li&gt;
&lt;li&gt;Only used in related to GitHub submission.&lt;/li&gt;
&lt;li&gt;Using Rmarkdown to generate a GitHub output document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure1.PNG&#34; /&gt; Rmarkdown document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure2.PNG&#34; /&gt; GitHub document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure3.PNG&#34; /&gt; Preview of GitHub document&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5) /R directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most important directory.&lt;/li&gt;
&lt;li&gt;The place where all your R code is written by you.&lt;/li&gt;
&lt;li&gt;Best to have separate R script files for each function.&lt;/li&gt;
&lt;li&gt;Need to have a R script file for Data as well.&lt;/li&gt;
&lt;li&gt;R scripts can be modified further in order to create RDocumentation files(Rd files).&lt;/li&gt;
&lt;li&gt;These RDocumentation files will explain about the function.&lt;/li&gt;
&lt;li&gt;Processed R script files will automatically generate Rd files in the man directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure2.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure3.PNG&#34; /&gt; R script file with necessary roxygen tags to develop RDocumentation files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6) /data directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Not compulsory.&lt;/li&gt;
&lt;li&gt;Easy to use your own data therefore its worth it.&lt;/li&gt;
&lt;li&gt;This directory will include the data-sets.&lt;/li&gt;
&lt;li&gt;R directory can have an R script to generate Rd files for these data-sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure3.PNG&#34; /&gt; data directory which includes data-sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure1.PNG&#34; /&gt; Rscript file which includes necessary roxygen tags to generate Rd files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;7) /tests directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If your package is going to be in CRAN or going to be in a platform with large range of users this would be useful.&lt;/li&gt;
&lt;li&gt;Unit tests to check if functions are working properly.&lt;/li&gt;
&lt;li&gt;Testing if the data sets are in proper form.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure4.PNG&#34; /&gt; tests directory and files in side that directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure5.PNG&#34; /&gt; sub directory testthat which includes test R scripts for all functions and data sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure1.PNG&#34; /&gt; R script to test a function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure2.PNG&#34; /&gt; R script to test a data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;man-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;8) /man directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This directory will include the Rd files for all functions and data sets.&lt;/li&gt;
&lt;li&gt;If you use roxygen tags there is no need to manually type them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure6.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure7.PNG&#34; /&gt; With the help of R script files these RDocumentation files will be generated for each function and will be in the man directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure4.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure5.PNG&#34; /&gt; The RDocumentation files can be processed into html outputs or into a pdf manual.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;&#34; /&gt; Rd file of a data-set which is created with the help of data R script.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure2.PNG&#34; /&gt; Html file which is generated with the help of Rd file for the data-set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;namsespace-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;9) NAMSESPACE file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A file which will have all the functions that you created for your package.&lt;/li&gt;
&lt;li&gt;If a function is exported then it will be in this file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure2.PNG&#34; /&gt; NAMESPACE file and its components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rbuildignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;10) .Rbuildignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be used when building the package.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure3.PNG&#34; /&gt; .Rbuildignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gitignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;11) .gitignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be pushed to the GitHub repository.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure4.PNG&#34; /&gt; .gitignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building the Package&lt;/h1&gt;
&lt;p&gt;All the files should be in their respective directories and names should not be changed for files or folders manually if they are created automatically. After checking all of this we can proceed to building the package. This process has 9 steps and below is a diagram to show how it works.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:1000px;height:300px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph LR;\n           A(Document &lt;br&gt; Generation)--&gt;C(Clean and &lt;br&gt; Rebuild);\n           C--&gt;D(Spellcheck &lt;br&gt; Rd files);\n           D--&gt;B((Check for &lt;br&gt; issues));\n           B--&gt;Z((Make Necessary &lt;br&gt; Changes)); Z--&gt;A;\n           D--&gt;E(Test &lt;br&gt; the Package); E--&gt;B;\n           E--&gt;F(Check &lt;br&gt; for Errors); F--&gt;B;\n           F--&gt;G(Build &lt;br&gt; Source Package);\n           G--&gt;H(Build &lt;br&gt; Binary Package);\n           H--&gt;I(Generate &lt;br&gt; Manual pdf);\n           I--&gt;J(Check Errors &lt;br&gt; on Source Package); J--&gt;B&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;This process is explained through a small presentation &lt;a href=&#34;/YourOwnPackage/BuildYourOwnPackage.html&#34;&gt;here&lt;/a&gt;. This presentation also can be used when you need to update your package version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributing-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distributing the Package&lt;/h1&gt;
&lt;p&gt;There are several ways of distributing your package. They are mainly&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;tar.qz version.&lt;/li&gt;
&lt;li&gt;zip version.&lt;/li&gt;
&lt;li&gt;GitHub Repository.&lt;/li&gt;
&lt;li&gt;The project folder which includes the package.&lt;/li&gt;
&lt;li&gt;Submit to CRAN or Bioconductor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I have written two posts related to R packages as well. One is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/findrpackage/how-to-find-your-r-package/&#34;&gt;How to find your R package&lt;/a&gt; and Second is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/newpackage/build-a-new-package-with-existing-r-packages/&#34;&gt;Using R packages to develop your own package&lt;/a&gt;. These two posts will be very useful as well.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 2: IMDB TV Shows Data</title>
      <link>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre&#34;&gt;Genre&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-season&#34;&gt;Genre and Season&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-year&#34;&gt;Genre and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-month&#34;&gt;Genre and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season&#34;&gt;Season&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-year&#34;&gt;Season and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-month&#34;&gt;Season and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-3-genres&#34;&gt;Top 3 Genres&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#crime-drama-mystery&#34;&gt;Crime Drama Mystery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comedy-drama&#34;&gt;Comedy Drama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drama&#34;&gt;Drama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rating-over-the-years&#34;&gt;Rating over the years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tv-series-with-more-than-14-seasons&#34;&gt;Tv Series with more than 14 Seasons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the packages
library(tidyverse)
library(summarytools)
library(magrittr)
library(readr)
library(lubridate)
library(gganimate)
library(stringr)

# load the dataset
Ratings &amp;lt;- read_csv(&amp;quot;IMDb_Economist_tv_ratings.csv&amp;quot;, 
                    col_types = cols(date = col_date(format = &amp;quot;%Y-%m-%d&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08&#34;&gt;Ratings&lt;/a&gt; data-set is from the IMDB site. I just found out that IMDb is active from 1990, that is a very long time and new information to me.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The code in GitHub: &lt;a href=&#34;https://t.co/ITQnSRH7Ot&#34;&gt;https://t.co/ITQnSRH7Ot&lt;/a&gt;  &lt;br&gt;.Week 2 of 2019.  Rating over the year and changes in sharing.  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/xP5F7PWDFV&#34;&gt;pic.twitter.com/xP5F7PWDFV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1082693109304168448?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
 &lt;a href=&#34;&amp;#39;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/week2&#34;&gt;GitHub code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TV shows from 1990 to 2018 with their ratings, genres, sharing and aired dates is in this data-set. I wanted a cool function to summarize the data-set at once, therefore browse through the internet and found the package &lt;a href=&#34;https://github.com/dcomtois/summarytools&#34;&gt;summarytools&lt;/a&gt;. There are quite a few functions in the mix, yet I choose dfSummary.&lt;/p&gt;
&lt;p&gt;Below is the code of using that function on the Ratings data-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Basic summary of all variables
dfSummary(Ratings,style = &amp;#39;grid&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Frame Summary   
## Ratings     
## **Dimensions:** 2266 x 7     
## **Duplicates:** 1   
## 
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | No | Variable      | Stats / Values                 | Freqs (% of Valid)   | Text Graph                             | Valid  | Missing |
## +====+===============+================================+======================+========================================+========+=========+
## | 1  | titleId       | 1. tt0098844                   | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. tt0203259                   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. tt0118401                   | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. tt0108757                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. tt0247082                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. tt0413573                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. tt0452046                   | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. tt0118375                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. tt0460681                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. tt0460627                  | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 866 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 2  | seasonNumber  | mean (sd) : 3.26 (3.44)        | 27 distinct values   | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 1 &amp;lt; 2 &amp;lt; 44                     |                      | :                                      |        |         |
## |    |               | IQR (CV) : 3 (1.05)            |                      | :                                      |        |         |
## |    |               |                                |                      | : .                                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 3  | title         | 1. Law &amp;amp; Order                 | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. Law &amp;amp; Order: Special Vict   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. Midsomer Murders            | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. CSI: Crime Scene Investig   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. ER                          | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. Grey&amp;#39;s Anatomy              | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. Criminal Minds              | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. King of the Hill            | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. Supernatural                | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. Bones                      | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 858 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 4  | date          | min : 1990-01-03               | 1808 distinct val.   |                   :                    | 2266   | 0       |
## |    | [Date]        | med : 2012-12-07               |                      |                 . :                    | (100%) | (0%)    |
## |    |               | max : 2018-10-10               |                      |               . : :                    |        |         |
## |    |               | range : 28y 9m 7d              |                      |           . : : : :                    |        |         |
## |    |               |                                |                      | . . . . : : : : : :                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 5  | av_rating     | mean (sd) : 8.06 (0.67)        | 1997 distinct values |               :                        | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      |               : :                      | (100%) | (0%)    |
## |    |               | 2.7 &amp;lt; 8.11 &amp;lt; 9.68              |                      |               : :                      |        |         |
## |    |               | IQR (CV) : 0.76 (0.08)         |                      |             . : :                      |        |         |
## |    |               |                                |                      |             : : : .                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 6  | share         | mean (sd) : 1.28 (3.38)        | 454 distinct values  | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 0 &amp;lt; 0.32 &amp;lt; 55.65               |                      | :                                      |        |         |
## |    |               | IQR (CV) : 0.99 (2.64)         |                      | :                                      |        |         |
## |    |               |                                |                      | :                                      |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 7  | genres        | 1. Crime,Drama,Mystery         | 369 (16.3%)          | III                                    | 2266   | 0       |
## |    | [character]   | 2. Comedy,Drama                | 174 ( 7.7%)          | I                                      | (100%) | (0%)    |
## |    |               | 3. Drama                       | 168 ( 7.4%)          | I                                      |        |         |
## |    |               | 4. Action,Crime,Drama          | 146 ( 6.4%)          | I                                      |        |         |
## |    |               | 5. Action,Adventure,Drama      | 112 ( 4.9%)          |                                        |        |         |
## |    |               | 6. Crime,Drama                 | 107 ( 4.7%)          |                                        |        |         |
## |    |               | 7. Drama,Romance               | 86 ( 3.8%)           |                                        |        |         |
## |    |               | 8. Comedy,Crime,Drama          | 80 ( 3.5%)           |                                        |        |         |
## |    |               | 9. Comedy,Drama,Romance        | 76 ( 3.4%)           |                                        |        |         |
## |    |               | 10. Crime,Drama,Thriller       | 63 ( 2.8%)           |                                        |        |         |
## |    |               | [ 87 others ]                  | 885 (39.1%)          | IIIIIII                                |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basic summary of seasons indicate 27 distinct values and obviously 1 is the minimum value, but the maximum value is 44. Another odd thing is median for being 2, further the average is 3.26. Which means most of the TV shows have only up-to few seasons. I would guess not more than 5.&lt;/p&gt;
&lt;p&gt;Summary of date indicates the earliest TV show from 1990 and latest from 2018. So the year difference is 28 years, but there are only 1808 distinct values. Even though the data-set contains 2266 observations. Some TV shows might have to start on the same day, that is the only plausible conclusion.&lt;/p&gt;
&lt;p&gt;“av_rating” (I presume Audio/Video Rating) is in the scale from 1 to 10, where people had influence. The least rating value is 2.7 and the most rating value is 9.68, but the median is 8.11. Also the mean is 8.06. We can say most of these TV shows are excellent to watch. There is another numeric variable called share and the value ranges from 0 to 55.65 where the average is 1.28.&lt;/p&gt;
&lt;div id=&#34;genre&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Genre&lt;/h1&gt;
&lt;p&gt;Genres in this data-set has close to 100 distinct factors. The category “Crime,Drama,Mystery” holds the highest percentage of 16.3, while second place is to “Comedy,Drama” with 7.7% and third place is to “Drama” type with 7.4%. All the other types of genre is represented by less than 7%.&lt;/p&gt;
&lt;div id=&#34;genre-and-season&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Season&lt;/h2&gt;
&lt;p&gt;Genre and Season are two categorical variables which should be compared to find out if over time do people like the same genre type. If we look at the bar plot it is clear “Crime,Drama,Mystery” type has seasons from 1 to 20. Mostly in all genres there is clear sign of TV shows with seasons up-to three or four. Some of them make it to season ten or eleven, for example genres like “Drama”, “Drama,Thriller”, “Animation,Comedy,Drama” and “Adventure,Drama,Family”.&lt;/p&gt;
&lt;p&gt;Oddly in “Drama,Romance” and “Crime,Drama” there are TV shows which has seasons above 35 but very few. It becomes more weird where for the same genre types the seasons in-between 25 and 34 are missing. Clearly in the legend also until season 20 there is continuity, but this does not carry on for higher seasons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre and Seasons&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Genre%20vs%20Season-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Year&lt;/h2&gt;
&lt;p&gt;Bar plot indicates that after 2014 around 90% of these genre type TV shows have been done. In the top ten category according to the counts of TV shows clearly all of these genres have been active since 1990 to now. Some of them were started in mid 1990s which include the genre types “Action,Crime,Drama”, “Crime,Drama” and “Comedy,Crime,Drama”.&lt;/p&gt;
&lt;p&gt;Types such as “Drama,Romance,Sport” and “Adventure,Drama,Romance” were in active in the mid 2000s but no longer. Genres such as “Drama,History”, “Drama,Horror,Thriller” and “Action,Drama” are a few of them which were popular after 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(year(date))))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Year&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Year&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Genre%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Month&lt;/h2&gt;
&lt;p&gt;Month of airing might have an influence on the TV shows. Bar plot indicates that most of the TV shows are aired in the first quarter or last quarter of the year. Which means shows aired in the fall (September or October) or aired after winter break (January).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(month(date))))+
  geom_bar()+ coord_flip()+ labs(fill=&amp;quot;Month&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Genre%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;season&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Season&lt;/h1&gt;
&lt;p&gt;TV shows may run for few seasons or more most of the time. Some are limited seasons close to four or above, but definitely less than 10. It is very rare to see TV shows going beyond the 15 seasons mark.&lt;/p&gt;
&lt;div id=&#34;season-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Year&lt;/h2&gt;
&lt;p&gt;Clearly there is an increase in TV shows aired over the years. All time low occurs in 1992 but all time high occurs in 2017. From 1990 to 2010 the TV shows aired have increased from 25 to 100. By the end of year 2017 the number of shows aired has reached more than 250, but its drops to slightly above 175 the next year. The all time low of less than 12 seasons occurs in 1990.&lt;/p&gt;
&lt;p&gt;In the years 1990,1996,2005,2007,2010,2011 and 2015 there are TV shows which has season above 30, but it should be reminded that according to the legend after season 20 there is no continuity.&lt;/p&gt;
&lt;p&gt;If we focus closely until 2005 most of the TV shows have seasons up-to 10 , but after 2005 there are TV shows which aired season until 20. This shows the popularity of certain shows over three decades.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(year(date)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the Years&amp;quot;)+
  scale_y_continuous(breaks=seq(0,230,10),labels=seq(0,230,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Season%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;season-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Month&lt;/h2&gt;
&lt;p&gt;Highest amount of more than 500 shows were aired in January and lowest amount of slightly less than 100 was aired in June. Second place goes to February with shows close to 200 being aired this is not even the half of what aired in the previous month. In the months of January, February and September most of the seasons were aired, while in the other months the seasons aired are from the range of 1 to 10. Where very few of them ever reached the double digits or above season 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(month(date)),fill=factor(seasonNumber)))+
  geom_bar()+coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Month&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,550,25),labels=seq(0,550,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Season%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;top-3-genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 3 Genres&lt;/h1&gt;
&lt;p&gt;Let focus on the most mentioned genre types which are “Crime,Drama,Mystery”, “Comedy,Drama” and “Drama”. The below section is to graphically represent the TV shows of a certain genre with its seasons and when they were aired.&lt;/p&gt;
&lt;p&gt;Therefore I will not be factual, mostly biased towards the shows I watched and special characteristics.&lt;/p&gt;
&lt;div id=&#34;crime-drama-mystery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crime Drama Mystery&lt;/h2&gt;
&lt;p&gt;Law and Order of 20 seasons has been over for more than 5 years and it began airing in 1990. Law and Order Special Victims Unit started airing in 1999 even now its still being aired. There are also odd shows like which has not aired continuously and skipped an year or two. Among them Columbo, Agatha Christie’s Marple and II commissario Montalbano are specially noted.&lt;/p&gt;
&lt;p&gt;There are lot of shows which only aired one or two seasons only and then stopped. They also can be noted from the bar plot. In the legend there are colors to indicate all the years from 1990 to 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Crime,Drama,Mystery&amp;quot;) %&amp;gt;%
  ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Crime,Drama,Mystery&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:20,labels=0:20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Crime%20Drama%20Mystery-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comedy-drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comedy Drama&lt;/h2&gt;
&lt;p&gt;Similarly as above we can interpret the bar plot as well. But here the highest amount of seasons any TV show has reached is 9. Only the TV shows “Scrubs” and “Shameless” have reached that milestone and both of them begin in different decades. “Scrubs” began in early 2000s, but “Shameless” was aired after 2010.&lt;/p&gt;
&lt;p&gt;In the years 1998 and 1999 there were no TV shows aired under the genre “Comedy,Drama”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Comedy,Drama&amp;quot;) %&amp;gt;%
    ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Comedy,Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:9,labels=0:9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Comedy%20and%20Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drama&lt;/h2&gt;
&lt;p&gt;Most of the TV shows in this genre type are limited to one season an a few more with 2 or 3 seasons. Even though most of them were aired after 2015. TV shows “Mad Men”, “Skins” and “The West Wing” has aired for 7 seasons.&lt;/p&gt;
&lt;p&gt;Some of these shows were limited series like “The News Room”. The only very early TV show is “Rebel Highway” which was aired in 1994 and in year 2004 “Summerland” was aired, where both of them were limited to one season.&lt;/p&gt;
&lt;p&gt;According to the legend the years 1992,1993,1995 to 1998 were years free of “Drama” genre TV shows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Drama&amp;quot;) %&amp;gt;%
 ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:7,labels=0:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rating-over-the-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rating over the years&lt;/h1&gt;
&lt;p&gt;Lets discuss regarding TV shows and their rating over the years with related to sharing. Clearly we can see the number of shows increasing from 1990 to 2018. Oddly there was more sharing related to TV shows before 2000, but this is not the case after the birth of Millenium. To be honest sharing becomes more extinct even with the power of Internet and smart phones.&lt;/p&gt;
&lt;p&gt;In perspective of rating the range is very much centered and small(between 7.5 - 9), but over the years this changes and expands to a wider range.(6.5 - 9.5). Only a handful of TV shows are rated below 5 in the scale over the year span of 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(Ratings,aes(x=factor(year(date)),y=av_rating,color=genres,size=share))+
      geom_point(show.legend = FALSE)+ 
      labs(title = &amp;quot;Ratings and Sharing : {frame_time}&amp;quot;
           ,x=&amp;quot;Year&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_y_continuous(breaks=2:10,labels=2:10)+
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      theme(axis.text.x = element_text(angle = 90))+
      shadow_mark()

animate(p,fps= 5,duration =60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/Rating%20vs%20Years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tv-series-with-more-than-14-seasons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tv Series with more than 14 Seasons&lt;/h1&gt;
&lt;p&gt;This is simply me trying to focus on the TV shows which has seasons more than 14 and their rating, sharing changes over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(Ratings,title==&amp;quot;CSI: Crime Scene Investigation&amp;quot;|
          title==&amp;quot;ER&amp;quot;| title==&amp;quot;Grey&amp;#39;s Anatomy&amp;quot;| title==&amp;quot;Midsomer Murders&amp;quot;| 
          title==&amp;quot;Law &amp;amp; Order&amp;quot;|
          title==&amp;quot;Law &amp;amp; Order: Special Victims Unit&amp;quot;) %&amp;gt;%
ggplot(aes(x=seasonNumber,y=av_rating,color=title,size=share))+ 
      geom_point()+
      labs(title = &amp;#39;Season and Rating Year: {frame_time}&amp;#39;,
           x=&amp;quot;Season&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_x_continuous(breaks=1:20,labels=1:20)+
      #scale_y_continuous(breaks=)
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      shadow_mark()+theme(legend.position = &amp;quot;bottom&amp;quot;)

animate(p,fps=5,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/index_files/figure-html/More%20than%2014%20Seasons-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2019 Week 1 : #TidyTuesday Tweets </title>
      <link>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidytuesday-tweets&#34;&gt;#Tidytuesday Tweets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earliest-tweet&#34;&gt;Earliest Tweet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#any-verified-profiles&#34;&gt;Any Verified Profiles ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#source-of-tweets&#34;&gt;Source of Tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tweets-per-month&#34;&gt;Tweets Per Month&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name&#34;&gt;Most Tweets By Screen Name&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-and-their-source&#34;&gt;Most Tweets By Screen Name and their Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-retweet-counts&#34;&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-favorite-counts&#34;&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-favorite-counts-vs-retweet-counts&#34;&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-followers-count-vs-friends-count&#34;&gt;Relationship between Followers Count vs Friends Count ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the necessary packages
library(tidyverse)
library(lubridate)
library(kableExtra)
library(ggthemr)

#load the ggthemr
ggthemr(&amp;quot;flat dark&amp;quot;)

# load the data set
tidytuesday_tweets&amp;lt;-readRDS(&amp;quot;tidytuesday_tweets.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;tidytuesday-tweets&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;#Tidytuesday Tweets&lt;/h1&gt;
&lt;p&gt;Using plots and Tables to express the #TidyTuesday data-set. You can obtain the dataset from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Favorite count vs Retweet count. Code: &lt;a href=&#34;https://t.co/QJwXxzrkFG&#34;&gt;https://t.co/QJwXxzrkFG&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://t.co/l14pHDS3kq&#34;&gt;pic.twitter.com/l14pHDS3kq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1080163707601195010?ref_src=twsrc%5Etfw&#34;&gt;January 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week1&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;earliest-tweet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Earliest Tweet&lt;/h2&gt;
&lt;p&gt;The first tweet is on April 2nd and it has 156 favorites and 64 retweets, where the tweet is from Thomas Mock and the next 3 tweets are also from him.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[order(tidytuesday_tweets$created_at),c(3,4,13,14,71)] %&amp;gt;%
  head(5) %&amp;gt;%
  kable()  %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
created_at
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 23:31:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-03 00:25:51
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
umairdurrani87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Umair Durrani
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;any-verified-profiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Any Verified Profiles ?&lt;/h2&gt;
&lt;p&gt;There are only 3 verified profiles where Hadley Wickham has the highest amount of followers with 76469, where that tweet has 61 favorites but no retweets. Other profiles are Civis Analytics and grspur, but both of them have friends above 600 counts, but Hadley Wickham friends count is close to 290.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(tidytuesday_tweets[,c(4,13,14,76,77,82)],verified==TRUE) %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
followers_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
friends_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
verified
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CivisAnalytics
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
658
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
hadleywickham
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76469
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
288
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
grspur
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
857
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
623
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;source-of-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Source of Tweets&lt;/h2&gt;
&lt;p&gt;Close to 1050 tweets are done by the web client and other clients such as Android and Iphone have tweet counts of respectively 106 and 233. Other sources include oddly Instagram, Facebook, WordPress and LinkedIn, which I am naming because of their popularity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,aes(fct_infreq(source)))+
  geom_bar()+coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.25)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Types of Sources&amp;quot;)+
  ggtitle(&amp;quot;Source of Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Source%20of%20Tweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets-per-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tweets Per Month&lt;/h2&gt;
&lt;p&gt;Beginning of #TidyTuesday we have 293 tweets on the month of April. Even though over the next months the number of tweets are decreasing this is not the case in October. Lowest number of tweets are recorded in September with 115 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,
       aes(x=month(tidytuesday_tweets$created_at)))+
  geom_bar()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),vjust=-0.15)+
  scale_x_continuous(breaks = seq(1,12),labels = seq(1,12))+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Months&amp;quot;)+
  ggtitle(&amp;quot;Tweet Counts By Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Tweets%20Per%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Tweets By Screen Name&lt;/h2&gt;
&lt;p&gt;There are 30 twitter users if we consider the accounts that have tweeted more than or equal to 10 tweets under the hashtag “TidyTuesday”. Thomas Mock has tweeted most which is 172 including retweets, and the second place goes to R4DScommunity with 92 tweets. All the other users have individually less than 40 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name)))+
  geom_bar()+ coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.15)+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Most%20Tweets%20By%20screen%20name-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;most-tweets-by-screen-name-and-their-source&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name and their Source&lt;/h3&gt;
&lt;p&gt;For the same plot if we consider the source for the tweets, it is clear that only seven sources were used. Mostly all of these users are using the web client, but some are using the iPhone as well. R4DS community does more tweeting through iPhone than TweetDeck. TweetDeck is a simple way of handling multiple twitter accounts at the same time. Tidyyourworld account only uses Android and WeAreRLadies uses only TweetDeck.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name),fill=source))+
  geom_bar(position = &amp;quot;stack&amp;quot;,stat=&amp;quot;count&amp;quot;)+ 
  coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=1,
            position = position_stack())+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Source&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Most%20Tweets%20By%20Screen%20name%20and%20their%20Source-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-retweet-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/h3&gt;
&lt;p&gt;Of the Top 30 users with most amount of tweets the highest amount of retweets is to a tweet from WeAreRLadies and it is 95. There are more outliers from Thomas Mock. and the highest range is to the user drob. Most from this top 30 users have the range between 0 and 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;retweet_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=retweet_count))+
  geom_boxplot()+ coord_flip()+
  scale_y_continuous(breaks = seq(0,100,5),labels = seq(0,100,5))+
  ylab(&amp;quot;Retweets&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Retweets Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Retweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-favorite-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/h3&gt;
&lt;p&gt;Similarly Thomas Mock has more outliers, and the highest range is to the user drob. Second place for outliers goes to R4DScommunity user. Close to 500 favorites are counted to a tweet by drob and second place is to a tweet by WeAreRladies with favorite counts slightly above 450.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;favorite_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=favorite_count))+
  geom_boxplot()+ coord_flip()+
   scale_y_continuous(breaks = seq(0,500,25),labels = seq(0,500,25))+
  ylab(&amp;quot;Favourites Count&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Favourties Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Favorite%20counts-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-favorite-counts-vs-retweet-counts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/h2&gt;
&lt;p&gt;Very clear positive correlation. Y scale ranges from 0 to 500, where x scale range is from 0 to 100 and most of the data points are centered around the range of 0 to 12 retweets and 0 to 60 Favorites. A Few data data points are out of the above mentioned range.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets, 
       aes(x=retweet_count,y=favorite_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,100,2) ,labels =seq(0,100,2))+
  scale_y_continuous(breaks =seq(0,500,10),labels =seq(0,500,10))+
  xlab(&amp;quot;Retweets&amp;quot;)+ylab(&amp;quot;Likes&amp;quot;)+
  ggtitle(&amp;quot;Retweets Versus Likes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Scatter%20plot%20between%20favourite%20count%20vs%20retweet%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-followers-count-vs-friends-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Followers Count vs Friends Count ?&lt;/h2&gt;
&lt;p&gt;Here I have considered only twitter profiles which has followers count less than 5000 with friends count also less than 5000. The reason is to explain the relationship more clearly. Clearly most of the twitter profiles are having followers less than 2000 with friends also less than 2000. Clearly there are some profiles with Followers count above 1000 but friends count less than 1000. Even though there are few profiles with less than 1000 followers but more than 1000 Friends.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(subset(tidytuesday_tweets,
       followers_count &amp;lt; 5000 &amp;amp; friends_count &amp;lt; 5000), 
       aes(x=followers_count,y=friends_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  scale_y_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  xlab(&amp;quot;Followers Count&amp;quot;)+ylab(&amp;quot;Friends Count&amp;quot;)+
  ggtitle(&amp;quot;Followers Count Versus Friends Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/index_files/figure-html/Scatter%20plot%20between%20Followers%20count%20vs%20Friends%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and tables in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using tidyverse as usual is fun.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Box plots for several variables in the same plot is easy for the use of comparison.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Scatter plots are nice to understand the relationship among two continuous variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The geom_smooth function is also very useful in modelling the data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can focus on the text variable which could be used for a word cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further we can try to understand the hashtags with favorites and retweets.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the mle and mle2 function </title>
      <link>/post/mleand2/benchmarking-the-mle-and-mle2-function/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/mleand2/benchmarking-the-mle-and-mle2-function/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mle&#34;&gt;mle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mle2&#34;&gt;mle2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle&#34;&gt;mle&lt;/a&gt; and &lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2&#34;&gt;mle2&lt;/a&gt; are my favorite functions, because they provide extensive amount of outputs for the optimization process. Even though there is no difference in analytical methods used in both of these functions. Further, these analytical methods are the same ones used by optim function. To be honest mle and mle2 functions are wrapper functions of optim. It means both mle and mle2 are using the optim function inside but with some additional inputs, which would generate extended outputs.&lt;/p&gt;
&lt;p&gt;Even if I do Benchmark the analytical methods for the mle function it would be very similar to optim function tables but with additional time taken, because of the extra outputs. This would similarly occur when we benchmark analytical methods from the mle2 function as well.&lt;/p&gt;
&lt;p&gt;Therefore, I figure why do we need to benchmark them at all. So this blog post is to simply reiterate the initial things which I said in my earlier post on the blog post &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/&#34;&gt;Benchmarking optimization functions in R&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;mle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle&lt;/h2&gt;
&lt;p&gt;mle function is from the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats4/html/00Index.html&#34;&gt;stats4&lt;/a&gt; package. If we intend to use this function for the estimation of shape parameters a and b of the Beta-Binomial distribution wtih Binomial Outcome Data, then we need to use the EstMLEBetaBin function from the fitODBOD package. This is not enough because for limitations in the mle we need to make changes in our EstMLEBetaBin function as mentioned below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)
library(fitODBOD)

#new function to facilitate mle criteria 
formle&amp;lt;-function(a,b)
{
  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)
}

# optimizing values for a,b using default analytial method
mle_answer&amp;lt;-mle(minuslogl = formle,start = list(a=0.1,b=0.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use the Alcohol Consumption data of week 1. In the above code chunk we are using the mle function for our task of finding the optimum shape parameter values for a and b while using the given Binomial Outcome data. Also If you wish you study about the mle function by referring &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle-function&#34;&gt;this link&lt;/a&gt; from my previous post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mle2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle2&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/bbmle/index.html&#34;&gt;bbmle&lt;/a&gt; package holds the mle2 function. It is simply an updated version for the mle function. Although there need to be no changes in the EstMLEBetaBin function to satisfy the mle2 function’s criteria. Now it will be possible to use it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bbmle)

# optimizing values for a,b using default analytical method
mle2_answer&amp;lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),
                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still if someone needs a brief introduction to mle2 function they can refer my previous brief through &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle2-function&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My personal opinion is to use the mle2 function, but moving towards what should be the analytical method. It would be wise to choose it based on your needs as these methods completely depend on the data, function that needs to be estimated, complexity of the function and finally the number estimators that needs to be estimated.&lt;/p&gt;
&lt;p&gt;This is the link to the article which is for &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/&#34;&gt;Benchmarking optim function&lt;/a&gt;. It might be useful while understanding the analytical methods.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
