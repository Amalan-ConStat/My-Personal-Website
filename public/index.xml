<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Consulting Statistician on Consulting Statistician</title>
    <link>/</link>
    <description>Recent content in Consulting Statistician on Consulting Statistician</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Week 3: Space Agencies and Launches</title>
      <link>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agencies&#34;&gt;AGENCIES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agency-vs-count&#34;&gt;Agency vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#type-vs-count&#34;&gt;Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-vs-count&#34;&gt;Class vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agency-type-vs-count&#34;&gt;Agency Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-count&#34;&gt;State Code vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location-vs-count&#34;&gt;Location vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#start-year-and-end-year-vs-agency&#34;&gt;Start Year and End Year vs agency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#launches&#34;&gt;LAUNCHES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-of-these-missions-vs-category-variables&#34;&gt;Success or Failure of these missions vs Category Variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-launch-year&#34;&gt;Success or Failure vs Launch Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-agency-type&#34;&gt;Success or Failure vs Agency Type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-state-code&#34;&gt;Success or Failure vs State Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-category-over-time-for-success-and-failure&#34;&gt;State Code vs Category Over time for Success and Failure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(tidyverse)
library(ggalt)
library(magrittr)
library(dplyr)
library(ggthemr)
library(gganimate)

# Load the Agency data
agencies&amp;lt;-read_csv(&amp;quot;agencies.csv&amp;quot;)

# Load the Launches data
launches&amp;lt;-read_csv(&amp;quot;launches.csv&amp;quot;)

attach(agencies)
attach(launches)

# load a theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;agencies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;AGENCIES&lt;/h1&gt;
&lt;p&gt;Space related agencies of 74 are in the world from this data set. Another, data set is for launches from the agencies in concern. In the agencies data set there are 19 variables and launches data set has 11 variables. You can find the data set and information regarded to it &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-15&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Early years of space launches had more mistakes and they were owned by the state. After the cold war, there is short of enthusiasm, but now we have an increase in launches. Code: &lt;a href=&#34;https://t.co/R0BLjFOH4U&#34;&gt;https://t.co/R0BLjFOH4U&lt;/a&gt;    &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/6dX338hpPD&#34;&gt;pic.twitter.com/6dX338hpPD&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1085387691393458179?ref_src=twsrc%5Etfw&#34;&gt;January 16, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week3&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;agency-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency vs Count&lt;/h2&gt;
&lt;p&gt;Most amount of launches are from Rakentiye Voiska Strategicheskogo Naznacheniye (RVSN) and it is 1528 and next is Upravleniye Nachalnika Kosmicheskikh Sredstv (UNKS) with 904. Top 10 places considering the most launches it clear that class D agencies has the most amount of 5, 3 class C agencies and the rest with class B. NASA is in third place with 469 launches and it is a class C agency. My favorite agency Space X (SPX) has launched 65 times and it is a class B agency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=class,fill=class))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(label=agencies$count, hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
       ggtitle(&amp;quot;Space Agency vs Frequency By Class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20vs%20Count%20and%20class-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, for the same bar plot if we change color according to agency type we have different insight. Top 10 agencies is 90% filled with state ownership and 10% is with private ownership. It should be noted that there are only two start-ups and close to 10 have private ownership, rest is state owned. The highest amount of launches for a private ownership is from Arian Space(AE) and for start-up its Space X (SPX). Respectively, their counts are 258 and 65.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=agency_type,fill=agency_type))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(aes(label=count), hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
      labs(color=&amp;quot;Agency Type&amp;quot;,fill=&amp;quot;Agency Type&amp;quot;)+
      ggtitle(&amp;quot;Space Agency vs Frequency By Agency Type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20vs%20Count%20and%20agency%20type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Type vs Count&lt;/h2&gt;
&lt;p&gt;Type of agencies is very complex because an agency can play multiple roles. Highest amount of count is for O/LA type with 3227 and second count is for LA type with 821 counts. There are 145 agencies with the highest combination of types this category is O/LA/LV/PL/E/S.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(type) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),hjust=-0.15)+coord_flip()+
  xlab(&amp;quot;Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Type vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;class-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class vs Count&lt;/h2&gt;
&lt;p&gt;Class C and B has similar amounts of count which is close to 1100 and most launches are from D class agencies with the count of 3584.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;class&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(class) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(class),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Class&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Class vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Class%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;agency-type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency Type vs Count&lt;/h2&gt;
&lt;p&gt;In perspective of agency type there are 4765 state owned launches, but only 67 launches from start-ups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;agency_type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(agency_type) %&amp;gt;% 
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(agency_type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Count&lt;/h2&gt;
&lt;p&gt;Close to 2500 missions were launched by Soviet Union and 1709 were done by Unite States.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;state_code&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(state_code) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(state_code),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/State%20Code%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Location vs Count&lt;/h2&gt;
&lt;p&gt;More than 1500 launches are from Mosvka? and exactly 1204 launches from Moskva. Further, 469 launches from Washington D.C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;location&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(location) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(location),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;Location&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Location vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Location%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;start-year-and-end-year-vs-agency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Start Year and End Year vs agency&lt;/h2&gt;
&lt;p&gt;Below is a Dumbbell plot to see at the agencies which are no longer active. Before 1960 there was very small activity and they are all owned by the state. With the American and Russian Space race we have private sector also being part of this adventure, but most of them are ending their service around the first half of 1990. There is more activity after this regularly but they are short lived for these agencies. Royal Aircraft Establishment (RAE) has long life for space adventure which was begun around late 1915, and ends its service in around 1990.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=as.numeric(substr(tstart,1,4)),xend=as.numeric(substr(tstop,1,4)),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 2.75,size=1.25)+ 
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+ 
  scale_x_continuous(breaks=seq(1910,2020,5),labels=seq(1910,2020,5))+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If We Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Start%20year%20and%20End%20Year%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It should be effectively noted that “-” mean still active and “*&amp;quot; means unknown in my perspective. Here we cannot consider the years as numeric because of the characters used. Agencies like NASA and Space X are still active according to my knowledge therefore I considered the above assumption for characters. Most of these agencies are state owned and after Space X there is Rocket Lab USA (RLABU). Most of these agencies were launched after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=substr(tstart,1,4),
           xend=substr(tstop,1,4),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 3,size=1.25)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If Do Not Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Start%20year%20and%20End%20Year%20Unknown%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;launches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;LAUNCHES&lt;/h1&gt;
&lt;p&gt;Counts of above missions are mentioned here thoroughly.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-of-these-missions-vs-category-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Success or Failure of these missions vs Category Variables&lt;/h2&gt;
&lt;p&gt;There are few categorical variables which could be associated with the success or failure of these missions.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-vs-launch-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Launch Year&lt;/h3&gt;
&lt;p&gt;Less mistakes over the year with technologies improving and in between 1960 to 1990 we can see alot of launches always above 100 per year. This enthusiasm no longer exists until 2005. After 2005 there is positive increase in launches and failures also less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=factor(launch_year),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;Years&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Years vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,150,10),breaks=seq(0,150,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20Launch%20Year-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-agency-type&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Agency Type&lt;/h3&gt;
&lt;p&gt;State owned agencies has more failures than private and start-ups because it would be costly. More than 4750 launches are from state owned agencies but in them more than 500 launches are failures. Even though private owned agencies has a history from 1990 yet they have an amount of less than 1000 counts for launches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(agency_type)),fill=category))+
  geom_bar()+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,5000,250),breaks=seq(0,5000,250))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20Agency%20Type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-state-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs State Code&lt;/h3&gt;
&lt;p&gt;Soviet Union (SU) and United States (US) has the most dominant appearance in this field. More than 2400 launches from SU and for US it is more than 1700 launches. Failures also considerably higher for SU and US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(state_code)),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,2500,100),breaks=seq(0,2500,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20State%20Code-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-category-over-time-for-success-and-failure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Category Over time for Success and Failure&lt;/h2&gt;
&lt;p&gt;Animated jitter plot here explains how over the years these launches occur based on States and Success(O) or Failure(F).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(launches,aes(y=category,x=state_code,color=agency_type))+
       geom_jitter()+
       labs(title = &amp;quot;States vs Success or Failure by : {round(frame_time,0)}&amp;quot;,
            x=&amp;quot;State Code&amp;quot;,y= &amp;quot;Success or Failure&amp;quot;)+
       transition_time(launch_year)+ease_aes(&amp;#39;linear&amp;#39;)+
       labs(color=&amp;quot;Agency Type&amp;quot;)

animate(p,fps=2,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/State%20code%20vs%20Category%20by%20time-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Developing an R package</title>
      <link>/post/yourownpackage/developing-an-r-package/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/yourownpackage/developing-an-r-package/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#credit-to-people-of-r-community&#34;&gt;Credit to People of R Community&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coding-standards-coding-to-understand&#34;&gt;1) Coding Standards (Coding to Understand)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#package-structure&#34;&gt;2) Package Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#description-file&#34;&gt;3) DESCRIPTION file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#readme-file&#34;&gt;4) README file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-directory&#34;&gt;5) /R directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-directory&#34;&gt;6) /data directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tests-directory&#34;&gt;7) /tests directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#man-directory&#34;&gt;8) /man directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#namsespace-file&#34;&gt;9) NAMSESPACE file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbuildignore-file&#34;&gt;10) .Rbuildignore file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gitignore-file&#34;&gt;11) .gitignore file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-package&#34;&gt;Building the Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributing-the-package&#34;&gt;Distributing the Package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;R package development is no longer as it was before 2010 because now most of the work can be done by just a simple mouse-click or with the use of a function. My intention of writing this blog post is not to give a thorough demonstration of how to develop your own R package. But it will briefly explain the process with the most important steps, and will include valuable blog posts and websites which helped me to develop my own R package &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credit-to-people-of-r-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Credit to People of R Community&lt;/h1&gt;
&lt;p&gt;I would definitely recommend to read all of these books and start your package development. Or at least make step by step progress in your work while reading them. If you have a basic knowledge regarding R, R studio, CRAN and writing programs, they are more than enough for you to start.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R packages by Hadley Wickam&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This website contains everything that is in the book. The basic things related to an R package development process are structured properly here. It is very useful to read this book/website. The package structure, the type of files necessary, how should the writing be in these files and further, what kind of ways can we use to achieve the final outputs. The book is mainly focused on producing an R package which can be updated with the highest standards using reproducing ability. Such as CRAN standards and GitHub releases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;Writing R Extensions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything related to package development is included here with clear instructions. This document is provided by the CRAN project to make package development more friendlier. It includes the official standards for files and naming conventions related to R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf&#34;&gt;Creating R packages: A Tutorial by Friedrich Leish&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A brief article explaining about writing functions, classes and methods for R package development. This is very abstract and useful in package development specially as it is focusing on object oriented programming and S formulas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf&#34;&gt;R for Beginners by Emmanuel Paradis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A book explaining R and its ability in detail, for example regarding functions, data, abilities and limitations of R. There is also a section for R packages, which has valuable information. Writing functions is very crucial in R package development therefore going through this document is worth. Several packages also include data-sets in them. While you develop functions similarly we can develop data-sets as well. There are sections which includes information regarding data-sets as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf&#34;&gt;The Art of R Programming by Norman Matloff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another book that will be useful in understanding how R functions and data-sets can be used in R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.burns-stat.com/pages/Tutor/R_inferno.pdf&#34;&gt;The R Inferno by Patrick Burns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Useful book related to object oriented programming, functions and data objects related to R. Very thorough and scrutinized information with valuable explanation which makes things more clearer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;Cheat Sheets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Easy implementation of packages mentioned in these cheatsheets. Very essential for someone who is interested in doing R related stuff efficient and eloquent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes related to RMarkdown, very useful for vignette building.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Happy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using GitHub for package development is very useful, specially when it comes to sharing and version control. This book explains it all with simplicity.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:900px;height:480px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph TB;\n           A(Code with Standards and comment)--&gt;C(Create R package);\n           C--&gt;D(R package &lt;br&gt; Structure);\n           D--&gt;E(DESCRIPTION file);\n           D--&gt;F(README file);\n           D--&gt;G(/R directory);\n           D--&gt;H(/data directory);\n           D--&gt;I(/tests directory);\n           D--&gt;J(/man directory);\n           D--&gt;K(NAMESPACE file);\n           D--&gt;L(/vignettes &lt;br&gt; directory);\n           D--&gt;M(NEWS.md file);\n           E--&gt;O(Build &lt;br&gt; the package);\n           O--&gt;N(Source,Bundle,Binary,Installed,In Memory);\n           F--&gt;O; G--&gt;O; H--&gt;O; I--&gt;O; J--&gt;O; K--&gt;O;  L--&gt;O; M--&gt;O;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;coding-standards-coding-to-understand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1) Coding Standards (Coding to Understand)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Focus on naming conventions.&lt;/li&gt;
&lt;li&gt;Focus on input parameters and outputs.&lt;/li&gt;
&lt;li&gt;Focus on indentation.&lt;/li&gt;
&lt;li&gt;Comment regularly to make sense of the functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure1.PNG&#34; /&gt; Sample Code&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;package-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2) Package Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very Important.&lt;/li&gt;
&lt;li&gt;Initially few files be originated in the designated project folder.&lt;/li&gt;
&lt;li&gt;Over time we might add folders or create files manually.&lt;/li&gt;
&lt;li&gt;Example - tests directory, README.Rmd, …&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure1.PNG&#34; /&gt; Package structure inside your project folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;description-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3) DESCRIPTION file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;File explaining basic things related to your package.&lt;/li&gt;
&lt;li&gt;Example - package name, other packages needed, authors name, …&lt;/li&gt;
&lt;li&gt;Can edit manually or use specific R package.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure1.PNG&#34; /&gt; After changes the DESCRIPTION file&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readme-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4) README file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very much optional.&lt;/li&gt;
&lt;li&gt;Only used in related to GitHub submission.&lt;/li&gt;
&lt;li&gt;Using Rmarkdown to generate a GitHub output document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure1.PNG&#34; /&gt; Rmarkdown document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure2.PNG&#34; /&gt; GitHub document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure3.PNG&#34; /&gt; Preview of GitHub document&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5) /R directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most important directory.&lt;/li&gt;
&lt;li&gt;The place where all your R code is written by you.&lt;/li&gt;
&lt;li&gt;Best to have separate R script files for each function.&lt;/li&gt;
&lt;li&gt;Need to have a R script file for Data as well.&lt;/li&gt;
&lt;li&gt;R scripts can be modified further in order to create RDocumentation files(Rd files).&lt;/li&gt;
&lt;li&gt;These RDocumentation files will explain about the function.&lt;/li&gt;
&lt;li&gt;Processed R script files will automatically generate Rd files in the man directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure2.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure3.PNG&#34; /&gt; R script file with necessary roxygen tags to develop RDocumentation files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6) /data directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Not compulsory.&lt;/li&gt;
&lt;li&gt;Easy to use your own data therefore its worth it.&lt;/li&gt;
&lt;li&gt;This directory will include the data-sets.&lt;/li&gt;
&lt;li&gt;R directory can have an R script to generate Rd files for these data-sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure3.PNG&#34; /&gt; data directory which includes data-sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure1.PNG&#34; /&gt; Rscript file which includes necessary roxygen tags to generate Rd files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;7) /tests directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If your package is going to be in CRAN or going to be in a platform with large range of users this would be useful.&lt;/li&gt;
&lt;li&gt;Unit tests to check if functions are working properly.&lt;/li&gt;
&lt;li&gt;Testing if the data sets are in proper form.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure4.PNG&#34; /&gt; tests directory and files in side that directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure5.PNG&#34; /&gt; sub directory testthat which includes test R scripts for all functions and data sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure1.PNG&#34; /&gt; R script to test a function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure2.PNG&#34; /&gt; R script to test a data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;man-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;8) /man directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This directory will include the Rd files for all functions and data sets.&lt;/li&gt;
&lt;li&gt;If you use roxygen tags there is no need to manually type them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure6.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure7.PNG&#34; /&gt; With the help of R script files these RDocumentation files will be generated for each function and will be in the man directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure4.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure5.PNG&#34; /&gt; The RDocumentation files can be processed into html outputs or into a pdf manual.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;&#34; /&gt; Rd file of a data-set which is created with the help of data R script.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure2.PNG&#34; /&gt; Html file which is generated with the help of Rd file for the data-set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;namsespace-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;9) NAMSESPACE file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A file which will have all the functions that you created for your package.&lt;/li&gt;
&lt;li&gt;If a function is exported then it will be in this file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure2.PNG&#34; /&gt; NAMESPACE file and its components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rbuildignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;10) .Rbuildignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be used when building the package.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure3.PNG&#34; /&gt; .Rbuildignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gitignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;11) .gitignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be pushed to the GitHub repository.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure4.PNG&#34; /&gt; .gitignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building the Package&lt;/h1&gt;
&lt;p&gt;All the files should be in their respective directories and names should not changed for files or folders manually if they are created automatically. After checking all of this we can proceed to building the package. This process has 9 steps and below is a diagram to show how it works.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:1000px;height:300px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph LR;\n           A(Document &lt;br&gt; Generation)--&gt;C(Clean and &lt;br&gt; Rebuild);\n           C--&gt;D(Spellcheck &lt;br&gt; Rd files);\n           D--&gt;B((Check for &lt;br&gt; issues));\n           B--&gt;Z((Make Necessary &lt;br&gt; Changes)); Z--&gt;A;\n           D--&gt;E(Test &lt;br&gt; the Package); E--&gt;B;\n           E--&gt;F(Check &lt;br&gt; for Errors); F--&gt;B;\n           F--&gt;G(Build &lt;br&gt; Source Package);\n           G--&gt;H(Build &lt;br&gt; Binary Package);\n           H--&gt;I(Generate &lt;br&gt; Manual pdf);\n           I--&gt;J(Check Errors &lt;br&gt; on Source Package); J--&gt;B&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;This process is explained through a small presentation &lt;a href=&#34;/YourOwnPackage/BuildYourOwnPackage.html&#34;&gt;here&lt;/a&gt;. This presentation also can be used to when you need to update your package version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributing-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distributing the Package&lt;/h1&gt;
&lt;p&gt;There are several ways of distributing your package. They are mainly&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;tar.qz version.&lt;/li&gt;
&lt;li&gt;zip version.&lt;/li&gt;
&lt;li&gt;GitHub Repository.&lt;/li&gt;
&lt;li&gt;The project folder which includes the package.&lt;/li&gt;
&lt;li&gt;Submit to CRAN or Bioconductor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I have written two posts related to R packages as well. One is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/findrpackage/how-to-find-your-r-package/&#34;&gt;How to find your R package&lt;/a&gt; and Second is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/newpackage/build-a-new-package-with-existing-r-packages/&#34;&gt;Using R packages to develop your own package&lt;/a&gt;. These two posts will be very useful.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 2: IMDB TV Shows Data</title>
      <link>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre&#34;&gt;Genre&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-season&#34;&gt;Genre and Season&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-year&#34;&gt;Genre and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-month&#34;&gt;Genre and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season&#34;&gt;Season&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-year&#34;&gt;Season and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-month&#34;&gt;Season and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-3-genres&#34;&gt;Top 3 Genres&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#crime-drama-mystery&#34;&gt;Crime Drama Mystery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comedy-drama&#34;&gt;Comedy Drama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drama&#34;&gt;Drama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rating-over-the-years&#34;&gt;Rating over the years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tv-series-with-more-than-14-seasons&#34;&gt;Tv Series with more than 14 Seasons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the packages
library(tidyverse)
library(summarytools)
library(magrittr)
library(readr)
library(lubridate)
library(gganimate)
library(stringr)

# load the dataset
Ratings &amp;lt;- read_csv(&amp;quot;IMDb_Economist_tv_ratings.csv&amp;quot;, 
                    col_types = cols(date = col_date(format = &amp;quot;%Y-%m-%d&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08&#34;&gt;Ratings&lt;/a&gt; data-set is from the IMDB site. I just found out that IMDb is active from 1990, that is a very long time and new information to me.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The code in GitHub: &lt;a href=&#34;https://t.co/ITQnSRH7Ot&#34;&gt;https://t.co/ITQnSRH7Ot&lt;/a&gt;  &lt;br&gt;.Week 2 of 2019.  Rating over the year and changes in sharing.  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/xP5F7PWDFV&#34;&gt;pic.twitter.com/xP5F7PWDFV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1082693109304168448?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
 &lt;a href=&#34;&amp;#39;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/week2&#34;&gt;GitHub code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TV shows from 1990 to 2018 with their ratings, genres, sharing and aired dates is in this data-set. I wanted a cool function to summarize the data-set at once, therefore scroll through the internet and found the package &lt;a href=&#34;https://github.com/dcomtois/summarytools&#34;&gt;summarytools&lt;/a&gt;. There are quite a few functions in the mix, yet I choose dfSummary.&lt;/p&gt;
&lt;p&gt;Below is the code of using that function on the Ratings data-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Basic summary of all variables
dfSummary(Ratings,style = &amp;#39;grid&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Frame Summary   
## Ratings     
## **Dimensions:** 2266 x 7     
## **Duplicates:** 1   
## 
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | No | Variable      | Stats / Values                 | Freqs (% of Valid)   | Text Graph                             | Valid  | Missing |
## +====+===============+================================+======================+========================================+========+=========+
## | 1  | titleId       | 1. tt0098844                   | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. tt0203259                   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. tt0118401                   | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. tt0108757                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. tt0247082                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. tt0413573                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. tt0452046                   | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. tt0118375                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. tt0460681                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. tt0460627                  | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 866 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 2  | seasonNumber  | mean (sd) : 3.26 (3.44)        | 27 distinct values   | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 1 &amp;lt; 2 &amp;lt; 44                     |                      | :                                      |        |         |
## |    |               | IQR (CV) : 3 (1.05)            |                      | :                                      |        |         |
## |    |               |                                |                      | : .                                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 3  | title         | 1. Law &amp;amp; Order                 | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. Law &amp;amp; Order: Special Vict   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. Midsomer Murders            | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. CSI: Crime Scene Investig   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. ER                          | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. Grey&amp;#39;s Anatomy              | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. Criminal Minds              | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. King of the Hill            | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. Supernatural                | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. Bones                      | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 858 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 4  | date          | min : 1990-01-03               | 1808 distinct val.   |                   :                    | 2266   | 0       |
## |    | [Date]        | med : 2012-12-07               |                      |                 . :                    | (100%) | (0%)    |
## |    |               | max : 2018-10-10               |                      |               . : :                    |        |         |
## |    |               | range : 28y 9m 7d              |                      |           . : : : :                    |        |         |
## |    |               |                                |                      | . . . . : : : : : :                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 5  | av_rating     | mean (sd) : 8.06 (0.67)        | 1997 distinct values |               :                        | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      |               : :                      | (100%) | (0%)    |
## |    |               | 2.7 &amp;lt; 8.11 &amp;lt; 9.68              |                      |               : :                      |        |         |
## |    |               | IQR (CV) : 0.76 (0.08)         |                      |             . : :                      |        |         |
## |    |               |                                |                      |             : : : .                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 6  | share         | mean (sd) : 1.28 (3.38)        | 454 distinct values  | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 0 &amp;lt; 0.32 &amp;lt; 55.65               |                      | :                                      |        |         |
## |    |               | IQR (CV) : 0.99 (2.64)         |                      | :                                      |        |         |
## |    |               |                                |                      | :                                      |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 7  | genres        | 1. Crime,Drama,Mystery         | 369 (16.3%)          | III                                    | 2266   | 0       |
## |    | [character]   | 2. Comedy,Drama                | 174 ( 7.7%)          | I                                      | (100%) | (0%)    |
## |    |               | 3. Drama                       | 168 ( 7.4%)          | I                                      |        |         |
## |    |               | 4. Action,Crime,Drama          | 146 ( 6.4%)          | I                                      |        |         |
## |    |               | 5. Action,Adventure,Drama      | 112 ( 4.9%)          |                                        |        |         |
## |    |               | 6. Crime,Drama                 | 107 ( 4.7%)          |                                        |        |         |
## |    |               | 7. Drama,Romance               | 86 ( 3.8%)           |                                        |        |         |
## |    |               | 8. Comedy,Crime,Drama          | 80 ( 3.5%)           |                                        |        |         |
## |    |               | 9. Comedy,Drama,Romance        | 76 ( 3.4%)           |                                        |        |         |
## |    |               | 10. Crime,Drama,Thriller       | 63 ( 2.8%)           |                                        |        |         |
## |    |               | [ 87 others ]                  | 885 (39.1%)          | IIIIIII                                |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basic summary of seasons indicate 27 distinct values and obviously 1 is the minimum value, but the maximum value is 44. Another odd thing is median for being 2, further the average is 3.26. Which means most of the TV shows have only up-to few seasons.&lt;/p&gt;
&lt;p&gt;Summary of date indicates the earliest TV show from 1990 and latest from 2018. So the year difference is 28 years, but there are only 1808 distinct values. Even though the data-set contains 2266 observations. Some TV shows might have to start on the same day, that is only plausible conclusion.&lt;/p&gt;
&lt;p&gt;“av_rating” (I presume Audio/Video Rating) is in the scale from 1 to 10, where people had influence. The least rating value is 2.7 and the most rating value is 9.68, but the median is 8.11. Also the mean is 8.06. We can say most of these TV shows are excellent to watch. There is another numeric variable called share and the value ranges from 0 to 55.65 where the average is 1.28.&lt;/p&gt;
&lt;div id=&#34;genre&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Genre&lt;/h1&gt;
&lt;p&gt;Genres in this data-set has close to 100 distinct factors. The category “Crime,Drama,Mystery” holds the highest percentage of 16.3, while second place is to “Comedy,Drama” with 7.7% and third place is to “Drama” type with 7.4%. All the other types of genre is represented by less than 7%.&lt;/p&gt;
&lt;div id=&#34;genre-and-season&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Season&lt;/h2&gt;
&lt;p&gt;Genre and Season are two categorical variables which should be compared to find out if over time do people like the same genre type. If we look at the bar plot it is clear “Crime,Drama,Mystery” type has seasons from 1 to 20. Mostly in all genres there is clear sign of TV shows with seasons up-to three or four. Some of them make it to season ten or eleven, for example genres like “Drama”, “Drama,Thriller”, “Animation,Comedy,Drama” and “Adventure,Drama,Family”.&lt;/p&gt;
&lt;p&gt;Oddly in “Drama,Romance” and “Crime,Drama” there are TV shows which has seasons above 35 but very few. It becomes more weird where for the same genre types the seasons in-between 25 and 34 are missing. Clearly in the legend also until season 20 there is continuity, but this does not carry on for higher seasons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre and Seasons&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Season-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Year&lt;/h2&gt;
&lt;p&gt;Bar plot indicates that after 2014 around 90% of these genre type TV shows have been done. In the top ten category according to the counts of TV shows clearly all of these genres have been active since 1990 to now. Some of them were started in mid 1990s which include the genre types “Action,Crime,Drama”, “Crime,Drama” and “Comedy,Crime,Drama”.&lt;/p&gt;
&lt;p&gt;Types such as “Drama,Romance,Sport” and “Adventure,Drama,Romance” were in active in the mid 2000s but no longer. Genres such as “Drama,History”, “Drama,Horror,Thriller” and “Action,Drama” are a few of them which were popular after 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(year(date))))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Year&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Year&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Month&lt;/h2&gt;
&lt;p&gt;Month of airing might have an influence on the TV shows. Bar plot indicates that most of the TV shows are aired in the first quarter or last quarter of the year. Which means shows aired in the fall (September or October) or aired after winter break (January).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(month(date))))+
  geom_bar()+ coord_flip()+ labs(fill=&amp;quot;Month&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;season&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Season&lt;/h1&gt;
&lt;p&gt;TV shows may run for few seasons or more most of the time. Some are limited seasons close to four or above, but definitely less than 10. It is very rare to see TV shows going beyond the 15 seasons mark.&lt;/p&gt;
&lt;div id=&#34;season-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Year&lt;/h2&gt;
&lt;p&gt;Clearly there is an increase in TV shows aired over the years. All time low occurs in 1992 but all time high occurs in 2017. From 1990 to 2010 the TV shows aired have increased from 25 to 100. By the end of year 2017 the number of shows aired has reached more than 250, but its drops to slightly above 175 the next year. The all time low of less than 12 seasons occurs in 1990.&lt;/p&gt;
&lt;p&gt;In the years 1990,1996,2005,2007,2010,2011 and 2015 there are TV shows which has season above 30, but it should be reminded that according to the legend after season 20 there is no continuity.&lt;/p&gt;
&lt;p&gt;If we focus closely until 2005 most of the TV shows have seasons up-to 10 , but after 2005 there are TV shows which aired season until 20. This shows the popularity of certain shows over three decades.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(year(date)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the Years&amp;quot;)+
  scale_y_continuous(breaks=seq(0,230,10),labels=seq(0,230,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Season%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;season-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Month&lt;/h2&gt;
&lt;p&gt;Highest amount of more than 500 shows were aired in January and lowest amount of slightly less than 100 was aired in June. Second place goes to February with shows close to 200 being aired this is not even the half of what aired in the previous month. In the months of January, February and September most of the seasons were aired, while in the other months the seasons aired are from the range of 1 to 10. Where very few of them ever reached the double digits or above season 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(month(date)),fill=factor(seasonNumber)))+
  geom_bar()+coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Month&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,550,25),labels=seq(0,550,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Season%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;top-3-genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 3 Genres&lt;/h1&gt;
&lt;p&gt;Let focus on the most mentioned genre types which are “Crime,Drama,Mystery”, “Comedy,Drama” and “Drama”. The below section is to graphically represent the TV shows of a certain genre with its seasons and when they were aired.&lt;/p&gt;
&lt;p&gt;Therefore I will not be factual, mostly biased towards the shows I watched and special characteristics.&lt;/p&gt;
&lt;div id=&#34;crime-drama-mystery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crime Drama Mystery&lt;/h2&gt;
&lt;p&gt;Law and Order of 20 seasons has been over for more than 5 years and it began airing in 1990. Law and Order Special Victims Unit started airing in 1999 even now its still being aired. There are also odd shows like which has not aired continuously and skipped an year or two. Among them Columbo, Agatha Christie’s Marple and II commissario Montalbano are specially noted.&lt;/p&gt;
&lt;p&gt;There are lot of shows which only aired one or two seasons only and then stopped. They also can be noted from the bar plot. In the legend there are colors to indicate all the years from 1990 to 2018 through color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Crime,Drama,Mystery&amp;quot;) %&amp;gt;%
  ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Crime,Drama,Mystery&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:20,labels=0:20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Crime%20Drama%20Mystery-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comedy-drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comedy Drama&lt;/h2&gt;
&lt;p&gt;Similarly as above we can interpret the bar plot as well. But here the highest amount of seasons any TV show has reached is 9. Only the TV shows “Scrubs” and “Shameless” have reached that milestone and both of them begin in different decades. “Scrubs” began in early 2000s, but “Shameless” was aired after 2010.&lt;/p&gt;
&lt;p&gt;In the years 1998 and 1999 there were no TV shows aired under the genre “Comedy,Drama”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Comedy,Drama&amp;quot;) %&amp;gt;%
    ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Comedy,Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:9,labels=0:9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Comedy%20and%20Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drama&lt;/h2&gt;
&lt;p&gt;Most of the TV shows in this genre type are limited to one season an a few more with 2 seasons. Even though most of them were aired after 2015. TV shows “Mad Men”, “Skins” and “The West Wing” has aired for 7 seasons.&lt;/p&gt;
&lt;p&gt;Some of these shows were limited series like “The News Room”. The only very early TV show is “Rebel Highway” which was aired in 1994 and in year 2004 “Summerland” was aired, where both of them were limited to one season.&lt;/p&gt;
&lt;p&gt;According to the legend the years 1992,1993,1995 to 1998 were years free of “Drama” genre TV shows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Drama&amp;quot;) %&amp;gt;%
 ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:7,labels=0:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rating-over-the-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rating over the years&lt;/h1&gt;
&lt;p&gt;Lets discuss regarding TV shows and their rating over the years with related to sharing. Clearly we can see the number of shows of increasing from 1990 to 2018. Oddly there was more sharing related to movies before 2000, this is into the same over the next years. To be honest sharing becomes more extinct.&lt;/p&gt;
&lt;p&gt;In perspective of rating the range is very much centered and small(between 7.5 - 9), but over the years this changes and expands to a wider range.(6.5 - 9.5). Only a handful of TV shows are rated below 5 in the scale over the year span of 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(Ratings,aes(x=factor(year(date)),y=av_rating,color=genres,size=share))+
      geom_point(show.legend = FALSE)+ 
      labs(title = &amp;quot;Ratings and Sharing : {frame_time}&amp;quot;
           ,x=&amp;quot;Year&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_y_continuous(breaks=2:10,labels=2:10)+
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      theme(axis.text.x = element_text(angle = 90))+
      shadow_mark()

animate(p,fps= 5,duration =60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Rating%20vs%20Years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tv-series-with-more-than-14-seasons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tv Series with more than 14 Seasons&lt;/h1&gt;
&lt;p&gt;This is simply me trying to focus on the TV shows which has seasons more than 14 and their rating, sharing changes over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(Ratings,title==&amp;quot;CSI: Crime Scene Investigation&amp;quot;|
          title==&amp;quot;ER&amp;quot;| title==&amp;quot;Grey&amp;#39;s Anatomy&amp;quot;| title==&amp;quot;Midsomer Murders&amp;quot;| 
          title==&amp;quot;Law &amp;amp; Order&amp;quot;|
          title==&amp;quot;Law &amp;amp; Order: Special Victims Unit&amp;quot;) %&amp;gt;%
ggplot(aes(x=seasonNumber,y=av_rating,color=title,size=share))+ 
      geom_point()+
      labs(title = &amp;#39;Season and Rating Year: {frame_time}&amp;#39;,
           x=&amp;quot;Season&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_x_continuous(breaks=1:20,labels=1:20)+
      #scale_y_continuous(breaks=)
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      shadow_mark()+theme(legend.position = &amp;quot;bottom&amp;quot;)

animate(p,fps=5,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/More%20than%2014%20Seasons-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2019 Week 1 : #TidyTuesday Tweets </title>
      <link>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidytuesday-tweets&#34;&gt;#Tidytuesday Tweets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earliest-tweet&#34;&gt;Earliest Tweet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#any-verified-profiles&#34;&gt;Any Verified Profiles ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#source-of-tweets&#34;&gt;Source of Tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tweets-per-month&#34;&gt;Tweets Per Month&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name&#34;&gt;Most Tweets By Screen Name&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-and-their-source&#34;&gt;Most Tweets By Screen Name and their Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-retweet-counts&#34;&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-favorite-counts&#34;&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-favorite-counts-vs-retweet-counts&#34;&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-followers-count-vs-friends-count&#34;&gt;Relationship between Followers Count vs Friends Count ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the necessary packages
library(tidyverse)
library(lubridate)
library(kableExtra)
library(ggthemr)

#load the ggthemr
ggthemr(&amp;quot;flat dark&amp;quot;)

# load the data set
tidytuesday_tweets&amp;lt;-readRDS(&amp;quot;tidytuesday_tweets.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;tidytuesday-tweets&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;#Tidytuesday Tweets&lt;/h1&gt;
&lt;p&gt;Using plots and Tables to express the #TidyTuesday data-set. You can obtain the dataset from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Favorite count vs Retweet count. Code: &lt;a href=&#34;https://t.co/QJwXxzrkFG&#34;&gt;https://t.co/QJwXxzrkFG&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://t.co/l14pHDS3kq&#34;&gt;pic.twitter.com/l14pHDS3kq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1080163707601195010?ref_src=twsrc%5Etfw&#34;&gt;January 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week1&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;earliest-tweet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Earliest Tweet&lt;/h2&gt;
&lt;p&gt;The first tweet is on April 2nd and it has 156 favorites and 64 retweets, where the tweet is from Thomas Mock and the next 3 tweets are also from him.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[order(tidytuesday_tweets$created_at),c(3,4,13,14,71)] %&amp;gt;%
  head(5) %&amp;gt;%
  kable()  %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
created_at
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 23:31:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-03 00:25:51
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
umairdurrani87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Umair Durrani
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;any-verified-profiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Any Verified Profiles ?&lt;/h2&gt;
&lt;p&gt;There are only 3 verified profiles where Hadley Wickham has the highest amount of followers of 76469, where that tweet has 61 favorites but no retweets. Other profiles are Civis Analytics and grspur, but both of them have friends above 600 counts, but Hadley Wickham friends close to 290.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(tidytuesday_tweets[,c(4,13,14,76,77,82)],verified==TRUE) %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
followers_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
friends_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
verified
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CivisAnalytics
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
658
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
hadleywickham
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76469
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
288
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
grspur
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
857
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
623
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;source-of-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Source of Tweets&lt;/h2&gt;
&lt;p&gt;Close to 1050 tweets are done by the web client and other clients such as Android and Iphone have tweet counts of respectively 106 and 233. Other sources include oddly Instagram, Facebook, WordPress and LinkedIn, which I am naming because of their popularity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,aes(fct_infreq(source)))+
  geom_bar()+coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.25)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Types of Sources&amp;quot;)+
  ggtitle(&amp;quot;Source of Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Source%20of%20Tweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets-per-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tweets Per Month&lt;/h2&gt;
&lt;p&gt;Beginning of #TidyTuesday we have 293 tweets on the month of April. Even though over the next months the number of tweets are decreasing this is not the case in October. Lowest number of tweets are recorded in September with 115 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,
       aes(x=month(tidytuesday_tweets$created_at)))+
  geom_bar()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),vjust=-0.15)+
  scale_x_continuous(breaks = seq(1,12),labels = seq(1,12))+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Months&amp;quot;)+
  ggtitle(&amp;quot;Tweet Counts By Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Tweets%20Per%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Tweets By Screen Name&lt;/h2&gt;
&lt;p&gt;There are 30 twitter users if we consider the accounts that have tweeted more than or equal to 10 tweets under the hashtag “TidyTuesday”. Thomas Mock has tweeted most which is 172 including retweets, and the second place goes to R4DScommunity with 92 tweets. All the other users have individually less than 40 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name)))+
  geom_bar()+ coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.15)+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20screen%20name-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;most-tweets-by-screen-name-and-their-source&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name and their Source&lt;/h3&gt;
&lt;p&gt;For the same plot if we consider the source for the tweets, it is clear that only seven sources were used. Mostly all of these users are using the web client, but some are using the iPhone as well. R4DS community does more tweeting through iPhone than TweetDeck. TweetDeck is a simple way of handling multiple twitter accounts at the same time. Tidyyourworld account only uses Android and WeAreRLadies uses only TweetDeck.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name),fill=source))+
  geom_bar(position = &amp;quot;stack&amp;quot;,stat=&amp;quot;count&amp;quot;)+ 
  coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=1,
            position = position_stack())+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Source&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20and%20their%20Source-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-retweet-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/h3&gt;
&lt;p&gt;Of the Top 30 users with most amount of tweets the highest amount of retweets is to a tweet from WeAreRLadies and it is 95. There are more outliers from Thomas Mock. and the highest range is to the user drob. Most from this top 30 users have the range between 0 and 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;retweet_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=retweet_count))+
  geom_boxplot()+ coord_flip()+
  scale_y_continuous(breaks = seq(0,100,5),labels = seq(0,100,5))+
  ylab(&amp;quot;Retweets&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Retweets Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Retweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-favorite-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/h3&gt;
&lt;p&gt;Similarly Thomas Mock has more outliers, and the highest range is to the user drob. Second place for outliers goes to R4DScommunity user. Close to 500 favorites are counted to a tweet by drob and second place is to a tweet by WeAreRladies with favorite counts slightly above 450.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;favorite_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=favorite_count))+
  geom_boxplot()+ coord_flip()+
   scale_y_continuous(breaks = seq(0,500,25),labels = seq(0,500,25))+
  ylab(&amp;quot;Favourites Count&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Favourties Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Favorite%20counts-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-favorite-counts-vs-retweet-counts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/h2&gt;
&lt;p&gt;Very clear positive correlation. Y scale ranges from 0 to 500, where x scale range is from 0 to 100 and most of the data points are centered around the range of 0 to 12 retweets and 0 to 60 Favorite. a Few data data points are out of the above mentioned range.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets, 
       aes(x=retweet_count,y=favorite_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,100,2) ,labels =seq(0,100,2))+
  scale_y_continuous(breaks =seq(0,500,10),labels =seq(0,500,10))+
  xlab(&amp;quot;Retweets&amp;quot;)+ylab(&amp;quot;Likes&amp;quot;)+
  ggtitle(&amp;quot;Retweets Versus Likes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Scatter%20plot%20between%20favourite%20count%20vs%20retweet%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-followers-count-vs-friends-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Followers Count vs Friends Count ?&lt;/h2&gt;
&lt;p&gt;Here I have considers only twitter profiles which has followers count less than 5000 with friends count also less than 5000. The reason is to explain the relationship more clearly. Clearly most of the twitter profiles are has followers less than 2000 with Followers also less than 2000. Clearly there are some profiles with Followers count above 1000 but friends count less than 1000. Even though there are few profiles with less than 1000 followers but more than 1000 Friends&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(subset(tidytuesday_tweets,
       followers_count &amp;lt; 5000 &amp;amp; friends_count &amp;lt; 5000), 
       aes(x=followers_count,y=friends_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  scale_y_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  xlab(&amp;quot;Followers Count&amp;quot;)+ylab(&amp;quot;Friends Count&amp;quot;)+
  ggtitle(&amp;quot;Followers Count Versus Friends Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Scatter%20plot%20between%20Followers%20count%20vs%20Friends%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and tables in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using tidyverse as usual is fun.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Box plots for several variables in the same plot is easy for the use of comparison.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Scatter plots are nice to understand the relationship among two continuous variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The geom_smooth function is also very useful in modelling the data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can focus on the text variable which could be used for a word cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further we can try to understand the hashtags with favorites and retweets.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the mle and mle2 function </title>
      <link>/post/mleand2/benchmarking-the-mle-and-mle2-function/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/mleand2/benchmarking-the-mle-and-mle2-function/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mle&#34;&gt;mle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mle2&#34;&gt;mle2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle&#34;&gt;mle&lt;/a&gt; and &lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2&#34;&gt;mle2&lt;/a&gt; are my favorite functions, because they provide extensive amount of outputs for the optimization process. Even though there is no difference in analytical methods used in both of these functions. Further, these analytical methods are the same ones used by optim function. To be honest mle and mle2 functions are wrapper functions of optim. It means both mle and mle2 are using the optim function inside but with some additional inputs, which would generate extended outputs.&lt;/p&gt;
&lt;p&gt;Even if I do Benchmark the analytical methods for the mle function it would be very similar to optim function tables but with additional time taken, because of the extra outputs. This would similarly occur when we benchmark analytical methods from the mle2 function as well.&lt;/p&gt;
&lt;p&gt;Therefore, I figure why do we need to benchmark them at all. So this blog post is to simply reiterate the initial things which I said in my earlier post on the blog post &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/&#34;&gt;Benchmarking optimization functions in R&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;mle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle&lt;/h2&gt;
&lt;p&gt;mle function is from the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats4/html/00Index.html&#34;&gt;stats4&lt;/a&gt; package. If we intend to use this function for the estimation of shape parameters a and b of the Beta-Binomial distribution when Binomial Outcome Data, then we need to use the EstMLEBetaBin function from the fitODBOD package. This is not enough because for limitations in the mle we need to make changes in our EstMLEBetaBin function as mentioned below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)
library(fitODBOD)

#new function to facilitate mle criteria 
formle&amp;lt;-function(a,b)
{
  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)
}

# optimizing values for a,b using default analytial method
mle_answer&amp;lt;-mle(minuslogl = formle,start = list(a=0.1,b=0.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use the Alcohol Consumption data of week 1. In the above code chunk we are using the mle function for our task of finding the optimum shape parameter values for a and b while using the give Binomial Outcome data. Also If you wish you study about the mle function refer &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle-function&#34;&gt;this link&lt;/a&gt; from my previous post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mle2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle2&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/bbmle/index.html&#34;&gt;bbmle&lt;/a&gt; package holds the mle2 function. It is simply an updated version for the mle function. Although there need to be no changes in the EstMLEBetaBin function to satisfy the mle2 function’s criteria. Now it will be possible to use it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bbmle)

# optimizing values for a,b using default analytical method
mle2_answer&amp;lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),
                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still if someone needs a brief introduction to mle2 function they can refer my previous brief through &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle2-function&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My personal opinion is to use the mle2 function, but moving towards what should be the analytical method. It would be wise to choose it based on your needs as these methods completely depend on the data, function that needs to be estimated, complexity of the function and finally the number estimators that needs to be estimated.&lt;/p&gt;
&lt;p&gt;This is the link to the article which is for &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/&#34;&gt;Benchmarking optim function&lt;/a&gt;. It might be useful while understanding the analytical methods.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build a New Package with Existing R packages</title>
      <link>/post/newpackage/build-a-new-package-with-existing-r-packages/</link>
      <pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/newpackage/build-a-new-package-with-existing-r-packages/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-essential-packages&#34;&gt;Most Essential Packages&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#devtools&#34;&gt;devtools&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pkgbuild&#34;&gt;pkgbuild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgload&#34;&gt;pkgload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rcmdcheck&#34;&gt;rcmdcheck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usethis&#34;&gt;usethis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roxygen2&#34;&gt;roxygen2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#knitr&#34;&gt;knitr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#markdown-rmarkdown-rmdformats&#34;&gt;markdown, rmarkdown, rmdformats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spelling&#34;&gt;spelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trackmd&#34;&gt;trackmd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testthat&#34;&gt;testthat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#essential-packages&#34;&gt;Essential Packages&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#git2r-and-gh&#34;&gt;git2r and gh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#desc&#34;&gt;desc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#covr&#34;&gt;covr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#badgecreatr-and-badger&#34;&gt;badgecreatr and badger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hexsticker&#34;&gt;hexSticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgdown&#34;&gt;pkgdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#still-i-have-not-used&#34;&gt;Still I have not Used&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packrat&#34;&gt;packrat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgconfig&#34;&gt;pkgconfig&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkginspector&#34;&gt;pkginspector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rvcheck&#34;&gt;rvcheck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rversions&#34;&gt;rversions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formatr&#34;&gt;formatR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#whoami&#34;&gt;whoami&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Package development is a sense of accomplishment for any statistical programmer who needs self satisfaction. I developed the R package &lt;a href=&#34;&#34;&gt;fitODBOD&lt;/a&gt; for the purpose of fitting Over dispersed Binomial Outcome Data using Binomial Mixture Distributions and Alternate Binomial Distributions. It was a an amazing journey learning how to develop an R package, which took me around 6 months while understanding the theoretical aspects of my research project and doing my 4th year courses.&lt;/p&gt;
&lt;p&gt;I am still learning new things related to R, which is helpful for this R package development. Making package version updates regularly is for the benefit of the user. I have learned new ways to express the theoretical concepts in the simplest form of functions, classes and methods. Currently, I am exploring the possibility of using Rshiny dashboard and GUI.&lt;/p&gt;
&lt;p&gt;In the beginning, R package developers have used manual techniques (which mean difficult techniques)&lt;br /&gt;
to develop R functions, documentation and examples for their packages. Over time it has changed rapidly, where currently we are using R packages to develop our own R package. In this post I shall briefly mention these packages which you can use. Using these packages it is possible to make package development stress free, time efficient and objective effective. Simultaneously we can make our R packages more attractive for the users, which would lead to lot of attention in the R community.&lt;/p&gt;
&lt;p&gt;There are three types of packages in my perspective, first “Most Essential Packages” which cannot be ignored, second “Essential packages” it is your choice to ignore and finally, “Still I have not Used” packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-essential-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Most Essential Packages&lt;/h1&gt;
&lt;p&gt;There are three packages in my main interest list and they are &lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;devtools&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=roxygen2&#34;&gt;roxygen2&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/package=testthat&#34;&gt;testhat&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;devtools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;devtools&lt;/h2&gt;
&lt;p&gt;Collection of packages which would significantly help the package development process. Functions such as dev_mode, check_failures, check_win and check_man.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;pkgbuild&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pkgbuild&lt;/h3&gt;
&lt;p&gt;Locates compilers needed to build R packages on various platforms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgbuild&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgload&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pkgload&lt;/h3&gt;
&lt;p&gt;Simulate the process of installing a package and then attacking it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgload&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rcmdcheck&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;rcmdcheck&lt;/h3&gt;
&lt;p&gt;Run “R CMD check” from R programmaticallly, and capture the results of the individual checks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rcmdcheck&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;usethis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;usethis&lt;/h3&gt;
&lt;p&gt;Automating few tasks related to package building.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=usethis&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;roxygen2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;roxygen2&lt;/h2&gt;
&lt;p&gt;Generate Rd documentation, and Namespace file with simplicity which would save time, when package update occurs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=roxygen2&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;knitr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;knitr&lt;/h3&gt;
&lt;p&gt;Useful to develop vignettes related to R package development.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=knitr&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markdown-rmarkdown-rmdformats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;markdown, rmarkdown, rmdformats&lt;/h3&gt;
&lt;p&gt;Html formats to vignettes in R package development and special template styles for the vignettes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=markdown&#34;&gt;markdown&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=rmarkdown&#34;&gt;rmarkdown&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=rmdformats&#34;&gt;rmdformats&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spelling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;spelling&lt;/h3&gt;
&lt;p&gt;Checking for spelling issues in Rd documentation files.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=spelling&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trackmd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;trackmd&lt;/h3&gt;
&lt;p&gt;Tracking changes in markdown files for vignette.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/trackmd&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testthat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;testthat&lt;/h2&gt;
&lt;p&gt;Checking if functions work properly by testing them in multiple ways for errors, outputs and inputs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=testthat&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;essential-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Essential Packages&lt;/h1&gt;
&lt;p&gt;We would not necessarily need these packages to develop our package, but it would make things more official if we choose to use them. Creating official badges, using GitHub for version control, checking for code coverage, distinct logo and having a website to explore the functions and vignettes of the package.&lt;/p&gt;
&lt;div id=&#34;git2r-and-gh&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;git2r and gh&lt;/h2&gt;
&lt;p&gt;Access to GitHub so that version control would occur smoothly with integration in Rstudio.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=git2r&#34;&gt;git2r&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=gh&#34;&gt;gh&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;desc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;desc&lt;/h2&gt;
&lt;p&gt;Editing the Description file using package rather than manually editing the file.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=desc&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;covr&lt;/h2&gt;
&lt;p&gt;Checking code coverage, which means does all functions have examples and are there tests for error messages, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=covr&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;badgecreatr-and-badger&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;badgecreatr and badger&lt;/h2&gt;
&lt;p&gt;Adding badges to GitHub repository, for example Download, CRAN status, code coverage, Release date, version and much more.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=badgecreatr&#34;&gt;badgecreatr&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=badger&#34;&gt;badger&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hexsticker&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;hexSticker&lt;/h2&gt;
&lt;p&gt;Creating a hexagon sticker for your package. Mostly just for the fun, but in a while its like promoting a brand.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=hexSticker&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkgdown&lt;/h2&gt;
&lt;p&gt;Using man files, vignette of your package to develop a static website. Further, it is possible to promote this site to get more people interested in the package.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgdown&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;still-i-have-not-used&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Still I have not Used&lt;/h1&gt;
&lt;p&gt;In this list I will explore packages which can be used to make package development more simple and elegant. Mostly using functions for the tasks of proper code spaces, indent and necessary R versions of dependency packages.&lt;/p&gt;
&lt;div id=&#34;packrat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;packrat&lt;/h2&gt;
&lt;p&gt;Manage the R packages in an isolated, portable and reproducible way.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=packrat&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgconfig&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkgconfig&lt;/h2&gt;
&lt;p&gt;Set configuration options on a per-package basis.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgconfig&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkginspector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkginspector&lt;/h2&gt;
&lt;p&gt;Understand internal structure of an R package.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/pkginspector&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rvcheck&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;rvcheck&lt;/h2&gt;
&lt;p&gt;Check latest release version of R and R packages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rvcheck&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rversions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;rversions&lt;/h2&gt;
&lt;p&gt;Focusing on R version ‘r-release’ and ‘r-oldrel’. Further all previous R versions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rversions&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;formatr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;formatR&lt;/h2&gt;
&lt;p&gt;Spaces and Indent for the code automatically added&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=formatR&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whoami&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;whoami&lt;/h2&gt;
&lt;p&gt;Username and full-name of current user, also email address and GitHub username.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=whoami&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Hopefully we would have more packages and awareness towards R package development with more simplicity in the coming years. Even though we have more than 15,000 packages in CRAN it would still rapidly increase in the coming years, not only in CRAN but also in GitHub as well.&lt;/p&gt;
&lt;p&gt;So far this post has only word content and links, therefore I am adding a screenshot of my fitODBOD package GitHub package ReadME.md file. This screen shot includes badges for downloads, R version, published date, package version and a hexagon logo sticker.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/screenshots/GitHub1.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/screenshots/GitHub2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;I hope this blog post is useful for anyone who has intentions to develop their R package or in the process of development or version controlling. I developed fitODBOD as a research project for my final year and it included a thesis report as well. Over time I wrote a Journal article as well and it is still under review. Also I have seen R package development as a PhD submission as well. Therefore it would be worthwhile developing an R package as a way to keep an active status regarding your field of interest in Statistics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How To Find Your R package ?</title>
      <link>/post/findrpackage/how-to-find-your-r-package/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/findrpackage/how-to-find-your-r-package/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#google&#34;&gt;1. Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cran&#34;&gt;2. CRAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bio---conductor&#34;&gt;3. Bio - Conductor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#github-pages&#34;&gt;4. GitHub pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rdocumentation&#34;&gt;5. Rdocumentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#crantastic&#34;&gt;6. Crantastic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rpackages&#34;&gt;7. rpackages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r---opensci&#34;&gt;8. R - Opensci&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rseek&#34;&gt;9. Rseek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-site-search&#34;&gt;10. R Site Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-forge&#34;&gt;11. R-forge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#awesomer&#34;&gt;12. AwesomeR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cran-task-view&#34;&gt;13. CRAN Task View&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rstudio---rpackages&#34;&gt;14. Rstudio - Rpackages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stack-overflow---r&#34;&gt;15. stack overflow - r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cranalerts&#34;&gt;16. CRANalerts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;How to find your R package is simply a blog post helping people to provide a list of websites where they can find R packages. These websites were useful for me while developing my own R package &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt;. So that I would be sure that fitODBOD is a unique package and what its functions should be able to do.&lt;/p&gt;
&lt;p&gt;This is a list with 16 items&lt;/p&gt;
&lt;div id=&#34;google&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Google&lt;/h1&gt;
&lt;p&gt;When you have no idea to find a package first thing is to “Google”.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Google.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. CRAN&lt;/h1&gt;
&lt;p&gt;Official website to find standard packages. The packages downloaded here will have documentation manuals, vignettes and sometimes journal articles which would simplify work for people who use them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/CRAN.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bio---conductor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Bio - Conductor&lt;/h1&gt;
&lt;p&gt;Another standard location to publish your R package, but only related to the field of Biology.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Bioconductor.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github-pages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. GitHub pages&lt;/h1&gt;
&lt;p&gt;If CRAN or Bio - Conductor is with high standards or too much work for your package you can still publish it and the ideal place for this is GitHub.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/GitHub.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rdocumentation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Rdocumentation&lt;/h1&gt;
&lt;p&gt;A place to find interactive documentation for the packages in CRAN, Bio - Conductor and GitHub. They simply include everything in the manual of a package but in html format.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rdocumentation.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;crantastic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Crantastic&lt;/h1&gt;
&lt;p&gt;All packages which are a part of CRAN is in this website. We can search packages based on Authors, package name, reviews and tags.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.crantastic.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/crantastic.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rpackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7. rpackages&lt;/h1&gt;
&lt;p&gt;Similar to crantastic this website also provides information to CRAN packages, but it is better because package related statistics is also shown here.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpackages.io/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rpackages.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r---opensci&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. R - Opensci&lt;/h1&gt;
&lt;p&gt;Search range for R packages in this website has more categories which is informative. I would say better than above mentioned ones.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/ropensci.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rseek&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;9. Rseek&lt;/h1&gt;
&lt;p&gt;This is like a google search engine for R packages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rseek.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rseek.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-site-search&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;10. R Site Search&lt;/h1&gt;
&lt;p&gt;Website dedicated to search R functions, package vignettes and task views.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://finzi.psych.upenn.edu/search.html&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rsitesearch.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-forge&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;11. R-forge&lt;/h1&gt;
&lt;p&gt;Projects related to R are mentioned in this website and how progress has been made on them is also here. Most of these projects will be published as packages later with significant importance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r-forge.r-project.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rforge.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;awesomer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;12. AwesomeR&lt;/h1&gt;
&lt;p&gt;This is a website which has R packages based on topics related to statistics. Some of these topics are Machine Learning, Bayesian, Optimization, Bio statistics and much more.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://awesome-r.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/AwesomeR.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cran-task-view&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;13. CRAN Task View&lt;/h1&gt;
&lt;p&gt;Topic related R packages are bundled together in this website. Further, the topics give a brief explanation, but the webpages give an extensive amount of information about what is unique is these packages. Also you do not need internet to use this because it is part of Rstudio help.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/views/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Crantaskview.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio---rpackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;14. Rstudio - Rpackages&lt;/h1&gt;
&lt;p&gt;Several crucial packages which would be very useful are considered here. All of these packages are projects. Further they are very popular in the R community.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/products/rpackages/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rstudio-rpackages.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stack-overflow---r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;15. stack overflow - r&lt;/h1&gt;
&lt;p&gt;If you cannot achieve something very specific related to R coding it is possible to use the website. It provides answers from other R users, sometimes even blogs related to the issues with solutions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/tagged/r&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/stackoverflow.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cranalerts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;16. CRANalerts&lt;/h1&gt;
&lt;p&gt;This is an email service which would alert us regarding specific packages accordance to our request. Whenever there is an update for a chosen package we would receive an email alert.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cranalerts.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Cranalerts.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is my list of places for reaching out to help with related to R packages and R programming. I use them constantly and they are very much helpful to me. Finally, I hope this post would be useful to anyone who wants to find or use R packages.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the maxLik function</title>
      <link>/post/maxlik/benchmarking-maxlik-function/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/maxlik/benchmarking-maxlik-function/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-the-shape-parameters-of-beta-binomial-distribution&#34;&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#brief-of-maxlik-function&#34;&gt;Brief of maxLik Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nr-method&#34;&gt;NR method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bfgs-method&#34;&gt;BFGS method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bfgsr-method&#34;&gt;BFGSR method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bhhh-method&#34;&gt;BHHH method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sann-method&#34;&gt;SANN method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cg-method&#34;&gt;CG method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nm-method&#34;&gt;NM method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sumary-of-time-evalutation-for-different-analytical-methods-of-maxlik-function&#34;&gt;Sumary of Time evalutation for different Analytical methods of maxLik function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-results-after-using-the-maxlik-function-for-different-analytical-methods&#34;&gt;Summary of results after using the maxLik function for different analytical methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-conclusion&#34;&gt;Final Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;estimating-the-shape-parameters-of-beta-binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Beginning of this month I wrote a &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#maxlik-function&#34;&gt;small section&lt;/a&gt; regarding maxLik function by comparing it to other optimization functions. Here, we will further study the analytical methods which can be used in this function and compare them to find suitability. maxLik function is from the package &lt;a href=&#34;https://cran.r-project.org/web/packages/maxLik/index.html&#34;&gt;maxLik&lt;/a&gt;. Further, &lt;a href=&#34;https://www.rdocumentation.org/packages/maxLik/versions/1.3-4/topics/maxLik&#34;&gt;Documentation&lt;/a&gt; clearly indicates all things related to the function in detail.&lt;/p&gt;
&lt;p&gt;Focusing on the seven analytical methods is my intention from this blog post. So, we have the Beta-Binomial distribution and Binomial Outcome data, and need to estimate proper shape parameters which would Maximize the Log Likelihood value of the Beta-Binomial distribution for the above Binomial Outcome data. In this case Alcohol Consumption data from the &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt; package will be used.&lt;/p&gt;
&lt;p&gt;Further we will focus on the process time to optimization, estimated shape parameters, maximized Log Likelihood value, expected frequencies, p-value and Over-dispersion with tables.&lt;/p&gt;
&lt;p&gt;Below are the seven analytical methods in concern&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;NR&lt;/li&gt;
&lt;li&gt;BFGS&lt;/li&gt;
&lt;li&gt;BFGSR&lt;/li&gt;
&lt;li&gt;BHHH&lt;/li&gt;
&lt;li&gt;SANN&lt;/li&gt;
&lt;li&gt;CG&lt;/li&gt;
&lt;li&gt;NM&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fitODBOD)
kable(Alcohol_data,&amp;quot;html&amp;quot;,align=c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 14,full_width = F) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Days
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
49
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;brief-of-maxlik-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brief of maxLik Function&lt;/h2&gt;
&lt;p&gt;Small &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#maxlik-function&#34;&gt;section&lt;/a&gt; about the maxLik function will be very useful to understand this blog post.&lt;/p&gt;
&lt;p&gt;Reference : Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood estimation in R Computational Statistics 26, 443–458 Marquardt, D.W., (1963)&lt;/p&gt;
&lt;p&gt;An Algorithm for Least-Squares Estimation of Nonlinear Parameters, Journal of the Society for Industrial &amp;amp; Applied Mathematics 11, 2, 431–441&lt;/p&gt;
&lt;p&gt;So for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would maximize the Log Likelihood value of the Beta-Binomial distribution.&lt;/p&gt;
&lt;p&gt;First we are transforming the given EstMLEBetaBin function to satisfy the maxLik function conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new function to facilitate maxLik criteria
# only one input but has two elements
formaxLik&amp;lt;-function(a)
  {
  -EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the formaxLik function can be used as above and parameters are estimated for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (or a, b) for the Alcohol Consumption data week 1. Further the maxLik function can be scrutinized as below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : maxLik&lt;/li&gt;
&lt;li&gt;No of Inputs: 6&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list or class of maxim or class of maxLik&lt;/li&gt;
&lt;li&gt;No of outputs: 11&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 7&lt;/li&gt;
&lt;li&gt;Default Method : Automatically chosen&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;nr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NR method&lt;/h2&gt;
&lt;p&gt;NR is an abbreviation for Unconstrained and equality-constrained maximization based on the quadratic approximation (Newton) method. The idea of the Newton method is to approximate the function at a given location by a multidimensional quadratic function, and use the estimated maximum as the start value for the next iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(maxLik)

# optimizing values for a,b using NR analytical method
NR_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;NR&amp;quot;)

# obtaining class of output
class(NR_answer)

# length of output
length(NR_answer)

# the outputs
NR_answer$estimate # estimated values for a, b
NR_answer$maximum # minimized function value 
NR_answer$iterations  # no of iterations to succeed
NR_answer$gradient # last gradient value which was calculated
NR_answer$message # additional information
NR_answer$hessian # hessian matrix
NR_answer$code # indicates successful completion
NR_answer$fixed # logical vector indicating which parameters are constants
NR_answer$type # type of maximization
NR_answer$last.step # list describing the last unsuccessful step
NR_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           NR_answer$estimate[1],NR_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgs-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BFGS method&lt;/h2&gt;
&lt;p&gt;BFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.&lt;/p&gt;
&lt;p&gt;Reference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using BFGS analytical method
BFGS_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;BFGS&amp;quot;)

# obtaining class of output
class(BFGS_answer)

# length of output
length(BFGS_answer)

# the outputs
BFGS_answer$estimate # estimated values for a, b
BFGS_answer$maximum # minimized function value 
BFGS_answer$iterations  # no of iterations to succeed
BFGS_answer$gradient # last gradient value which was calculated
BFGS_answer$message # additional information
BFGS_answer$hessian # hessian matrix
BFGS_answer$code # indicates successful completion
BFGS_answer$fixed # logical vector indicating which parameters are constants
BFGS_answer$type # type of maximization
BFGS_answer$last.step # list describing the last unsuccessful step
BFGS_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           BFGS_answer$estimate[1],BFGS_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgsr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BFGSR method&lt;/h2&gt;
&lt;p&gt;Combination of two methods which are Newton-Raphson, BFGS (Broyden 1970, Fletcher 1970, Goldfarb 1970, Shanno 1970).&lt;/p&gt;
&lt;p&gt;Reference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using BFGSR analytical method
BFGSR_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;BFGSR&amp;quot;)

# obtaining class of output
class(BFGSR_answer)

# length of output
length(BFGSR_answer)

# the outputs
BFGSR_answer$estimate # estimated values for a, b
BFGSR_answer$maximum # minimized function value 
BFGSR_answer$iterations  # no of iterations to succeed
BFGSR_answer$gradient # last gradient value which was calculated
BFGSR_answer$message # additional information
BFGSR_answer$hessian # hessian matrix
BFGSR_answer$code # indicates successful completion
BFGSR_answer$fixed # logical vector indicating which parameters are constants
BFGSR_answer$type # type of maximization
BFGSR_answer$last.step # list describing the last unsuccessful step
BFGSR_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           BFGSR_answer$estimate[1],BFGSR_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bhhh-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BHHH method&lt;/h2&gt;
&lt;p&gt;BHHH method (Berndt, Hall, Hall, Hausman 1974). The BHHH (information equality) approximation is only valid for log-likelihood functions. It requires the score (gradient) values by individual observations and hence those must be returned by individual observations by grad or fn. With the complexity of BHHH method I choose not to discuss it here, but a reference is mentioned to anyone who has interest in this analytical method.&lt;/p&gt;
&lt;p&gt;Reference : Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974): Estimation and Inference in Nonlinear Structural Models, Annals of Social Measurement 3, 653–665.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sann-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SANN method&lt;/h2&gt;
&lt;p&gt;Method SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.&lt;/p&gt;
&lt;p&gt;By default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to &lt;span class=&#34;math inline&#34;&gt;\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the current iteration step and temp and tmax are specifiable via control.&lt;/p&gt;
&lt;p&gt;Note that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.&lt;/p&gt;
&lt;p&gt;Reference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using SANN analytical method
SANN_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;SANN&amp;quot;)

# obtaining class of output
class(SANN_answer)

# length of output
length(SANN_answer)

# the outputs
SANN_answer$estimate # estimated values for a, b
SANN_answer$maximum # minimized function value 
SANN_answer$iterations  # no of iterations to succeed
SANN_answer$gradient # last gradient value which was calculated
SANN_answer$message # additional information
SANN_answer$hessian # hessian matrix
SANN_answer$code # indicates successful completion
SANN_answer$fixed # logical vector indicating which parameters are constants
SANN_answer$type # type of maximization
SANN_answer$last.step # list describing the last unsuccessful step
SANN_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           SANN_answer$estimate[1],SANN_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cg-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CG method&lt;/h2&gt;
&lt;p&gt;Method CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak-Ribiere or Beale-Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.&lt;/p&gt;
&lt;p&gt;Reference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using CG analytical method
CG_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;CG&amp;quot;)

# obtaining class of output
class(CG_answer)

# length of output
length(CG_answer)

# the outputs
CG_answer$estimate # estimated values for a, b
CG_answer$maximum # minimized function value 
CG_answer$iterations  # no of iterations to succeed
CG_answer$gradient # last gradient value which was calculated
CG_answer$message # additional information
CG_answer$hessian # hessian matrix
CG_answer$code # indicates successful completion
CG_answer$fixed # logical vector indicating which parameters are constants
CG_answer$type # type of maximization
CG_answer$last.step # list describing the last unsuccessful step
CG_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           CG_answer$estimate[1],CG_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nm-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NM method&lt;/h2&gt;
&lt;p&gt;NM is the abbreviation to Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.&lt;/p&gt;
&lt;p&gt;Reference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using NM analytical method
NM_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;NM&amp;quot;)

# obtaining class of output
class(NM_answer)

# length of output
length(NM_answer)

# the outputs
NM_answer$estimate # estimated values for a, b
NM_answer$maximum # minimized function value 
NM_answer$iterations  # no of iterations to succeed
NM_answer$gradient # last gradient value which was calculated
NM_answer$message # additional information
NM_answer$hessian # hessian matrix
NM_answer$code # indicates successful completion
NM_answer$fixed # logical vector indicating which parameters are constants
NM_answer$type # type of maximization
NM_answer$last.step # list describing the last unsuccessful step
NM_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           NM_answer$estimate[1],NM_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sumary-of-time-evalutation-for-different-analytical-methods-of-maxlik-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sumary of Time evalutation for different Analytical methods of maxLik function&lt;/h1&gt;
&lt;p&gt;Below table will compare the system process time for different analytical methods. In order to do this time comparison it is possible to use the &lt;a href=&#34;https://www.rdocumentation.org/packages/rbenchmark/versions/1.0.0/topics/benchmark&#34;&gt;benchmark&lt;/a&gt; function of &lt;a href=&#34;https://cran.r-project.org/package=rbenchmark&#34;&gt;rbenchmark&lt;/a&gt; package. Below mentioned code chunk provides the output in a table form which includes the analytical methods and their respective time values. The estimation process of the parameters where each method has been replicated 1000 times to receive a more accurate table for time values.&lt;/p&gt;
&lt;p&gt;Table is in the ascending order for elapsed time column. It is evidently clear that SANN analytical method has taken the most time. Before that the analytical method BFGSR is in 5th place. While NM or Nelder Mead method has taken the least time. This time is calculated for 1000 replications of the function being repeated under same conditions. These times does not only reflect based on analytical method, rather on the Log Likelihood function that needs to be maximized, the data provided, the number of estimated that needs to be estimated, the complexity of the function and finally the computer’s processing power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rbenchmark)

Results1&amp;lt;-benchmark(
          &amp;quot;NR&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;NR&amp;quot;)},
          &amp;quot;BFGS&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;BFGS&amp;quot;)},
          &amp;quot;BFGSR&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;BFGSR&amp;quot;)},
          &amp;quot;SANN&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;SANN&amp;quot;)},
          &amp;quot;CG&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;CG&amp;quot;)},
          &amp;quot;NM&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;NM&amp;quot;)},
          replications = 1000,
          columns = c(&amp;quot;test&amp;quot;,&amp;quot;replications&amp;quot;,&amp;quot;elapsed&amp;quot;,
                      &amp;quot;relative&amp;quot;,&amp;quot;user.self&amp;quot;,&amp;quot;sys.self&amp;quot;),
          order = &amp;#39;elapsed&amp;#39;
          )

kable(Results1,&amp;quot;html&amp;quot;,align = c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(full_width=T,bootstrap_options=c(&amp;quot;striped&amp;quot;),font_size = 14)%&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: blue;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
test
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
replications
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
elapsed
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
relative
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
user.self
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
sys.self
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
16.86
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
16.81
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BFGS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
32.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.933
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
32.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NR
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.348
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
CG
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
65.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.862
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
65.02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BFGSR
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
700.44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41.544
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
681.86
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.82
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
SANN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1741.50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
103.292
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1736.73
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.17
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-results-after-using-the-maxlik-function-for-different-analytical-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of results after using the maxLik function for different analytical methods&lt;/h1&gt;
&lt;p&gt;Estimated the shape parameters a,b pair wise from the analytical methods NR, BFGS, BFGSR, SANN, CG and NM. These estimated parameters will be now used in the fitBetaBin function to find the expected frequencies, p-values and over-dispersion. The above measurements can be used to compared for each analytical method for any significance difference.&lt;/p&gt;
&lt;p&gt;Comparing p-values it is clear that all analytical methods generate the same value up-to third decimal point. This is not the case in Maximum Log Likelihood value where analytical methods NR, BFGS, CG and NM have obtained the value -813.4571, while BFGSR and SANN have shown -813.4576. Further, Over-dispersion values are similar until third decimal point, but after that there is a clear difference among all six methods.&lt;/p&gt;
&lt;p&gt;All of the analytical methods have produced distinct values for estimated shape parameters of a and b. For the shape parameter a, similarity is only until second decimal point, and for shape parameter b, similarity of value is only on first decimal point.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 12px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BinomialRandomVariable
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
NR
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BFGS
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BFGSR
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
SANN
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
CG
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
NM
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41.98
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.85
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.91
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.97
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.98
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.76
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.77
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Total No of Observations
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
p-value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Estimated a and b
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229428 b=0.5808488
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7228919 b=0.5807283
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7209896 b=0.5790360
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7219066 b=0.5813484
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229403 b=0.5808469
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7230707 b=0.5809894
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Maximum Log Likelihood
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4576
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4576
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Over Dispersion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.434067
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340993
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4347778
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4341682
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340679
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340165
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;final-conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Conclusion&lt;/h1&gt;
&lt;p&gt;Now we can conclude the above findings using the tables provided, and it is clear that there is no strong change in expected frequencies, maximum Log Likelihood value or p-value if we use any one of the methods mentioned above. If time is crucial it is best to avoid BFGSR and SANN methods as they take considerable amount of time. I would recommend choose the analytical method from maxLik function based on your needs of output and research objective.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank you&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 38: Sea Creatures</title>
      <link>/post/week_38/week-38-sea-creatures/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/week_38/week-38-sea-creatures/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ios-slide-presentation&#34;&gt;IOS slide Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#packages-used&#34;&gt;Packages Used&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-vs-sex-vs-birthyear-code&#34;&gt;Species vs Sex vs BirthYear (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-vs-sex-vs-birthyear-plot&#34;&gt;Species vs Sex vs BirthYear (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#status-vs-sex-vs-birthyear-code&#34;&gt;Status vs Sex vs BirthYear (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#status-vs-sex-vs-birthyear-plot&#34;&gt;Status vs Sex vs BirthYear (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-vs-sex-vs-status-code&#34;&gt;Species vs Sex vs Status (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-vs-sex-vs-status-plot&#34;&gt;Species vs Sex vs Status (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#birth-year-and-sex-of-the-acquisitioned-code&#34;&gt;Birth Year and Sex of the Acquisitioned (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#birth-year-and-sex-of-the-acquisitioned-plot&#34;&gt;Birth Year and Sex of the Acquisitioned (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-and-their-sex-over-current-location-code&#34;&gt;Species and their sex over current location (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#species-and-their-sex-over-current-location-plot&#34;&gt;Species and their sex over current location (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acquisitioned-ones-and-thier-sex-with-status-code&#34;&gt;Acquisitioned ones and thier Sex with Status (code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acquisitioned-ones-and-thier-sex-with-status-plot&#34;&gt;Acquisitioned ones and thier Sex with Status (plot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thank-you&#34;&gt;THANK YOU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Over the weeks I have only done blog posts for TidyTuesday. Today for week 38, I am going to present my blog post in a presentation. This presentation does not include plots, but the code only. Therefore, I am putting the plots here.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Blog post : &lt;a href=&#34;https://t.co/QhXIzSRuit&#34;&gt;https://t.co/QhXIzSRuit&lt;/a&gt;&lt;br&gt; GitHub code for presentation: &lt;a href=&#34;https://t.co/GERv0GziFL&#34;&gt;https://t.co/GERv0GziFL&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/week38?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#week38&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/x2aDqjhbnl&#34;&gt;pic.twitter.com/x2aDqjhbnl&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1074965237231697921?ref_src=twsrc%5Etfw&#34;&gt;December 18, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;ios-slide-presentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IOS slide Presentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/html/Seaslides.html&#34;&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/blob/master/Week%2038/Seaslides.Rmd&#34;&gt;Rmarkdown for the same presentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below is the content from the presentation, but I have included the plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the packages
library(readr)
library(lubridate)
library(tidyverse)
library(magrittr)
library(ggthemr)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2194 observations.&lt;/li&gt;
&lt;li&gt;21 variables.&lt;/li&gt;
&lt;li&gt;Data : &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2018-12-18&#34;&gt;Cetacean Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read : &lt;a href=&#34;https://pudding.cool/2017/07/cetaceans/&#34;&gt;The Pudding Article&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;About : Big Fish in the Sea.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;packages-used&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages Used&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;readr&lt;/li&gt;
&lt;li&gt;lubridate&lt;/li&gt;
&lt;li&gt;tidyverse&lt;/li&gt;
&lt;li&gt;magrittr&lt;/li&gt;
&lt;li&gt;ggthemr&lt;/li&gt;
&lt;li&gt;stringr&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#load the data
SeaCreature &amp;lt;- read_csv(&amp;quot;allCetaceanData.csv&amp;quot;, 
                         col_types = cols(X1 = col_skip()))
attach(SeaCreature)

# loading theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;species-vs-sex-vs-birthyear-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species vs Sex vs BirthYear (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot1&amp;lt;-ggplot(SeaCreature,aes(x=species,y=birthYear,color=sex))+
       geom_jitter()+
       coord_flip()+
       theme(axis.text.x =element_text(angle = 90, hjust = 1))+
       ggtitle(&amp;quot;Species and Sex over their BirthYear&amp;quot;)+
       ylab(&amp;quot;Birth Year&amp;quot;)+
       xlab(&amp;quot;Species&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_1.png&amp;quot;,width = 12,height = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;species-vs-sex-vs-birthyear-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species vs Sex vs BirthYear (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 1&lt;/li&gt;
&lt;li&gt;Alot of Bottle-nose type species from early years.&lt;/li&gt;
&lt;li&gt;More missing values for Birth Year.&lt;/li&gt;
&lt;li&gt;Second most goes to Killer Whale Orca.&lt;/li&gt;
&lt;li&gt;Third place is in with Beluga type Species.&lt;/li&gt;
&lt;li&gt;Here and there few of them without knowledge of Gender.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_1.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;status-vs-sex-vs-birthyear-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Status vs Sex vs BirthYear (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot2&amp;lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),
                              y=birthYear,color=sex))+
       geom_jitter()+
       coord_flip()+
       theme(axis.text.x =element_text(angle = 90, hjust = 1))+
       ggtitle(&amp;quot;Status and Sex over their BirthYear&amp;quot;)+
       ylab(&amp;quot;Birth Year&amp;quot;)+
       xlab(&amp;quot;Status&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_2.png&amp;quot;,width = 12,height = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;status-vs-sex-vs-birthyear-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Status vs Sex vs BirthYear (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 2&lt;/li&gt;
&lt;li&gt;Dead Sea Creatures from the beginning of time itself.&lt;/li&gt;
&lt;li&gt;Mostly dead, but from 1960 alot of them are alive.&lt;/li&gt;
&lt;li&gt;Birth Year unknown for most of the Dead and few of the Released.&lt;/li&gt;
&lt;li&gt;Quite a few with status unknown.&lt;/li&gt;
&lt;li&gt;Only one escaped and it is a male in 1981.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_2.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;species-vs-sex-vs-status-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species vs Sex vs Status (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot3&amp;lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),
                              y=str_wrap(species,12),color=sex))+
       geom_jitter()+
       coord_flip()+
       theme(axis.text.x =element_text(angle = 90, hjust = 1))+
       ggtitle(&amp;quot;Species and Sex over their status&amp;quot;)+
       ylab(&amp;quot;Species&amp;quot;)+
       xlab(&amp;quot;Status&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_3.png&amp;quot;,width = 14,height = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;species-vs-sex-vs-status-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species vs Sex vs Status (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 3&lt;/li&gt;
&lt;li&gt;One male Bottle-nose species escaped.&lt;/li&gt;
&lt;li&gt;More Killer whale orca’s and White-sided Pacific Species are dead than alive&lt;/li&gt;
&lt;li&gt;Around 15 Species have dead creatures and non alive.&lt;/li&gt;
&lt;li&gt;One male Bottle-nose species Escaped but found dead.&lt;/li&gt;
&lt;li&gt;There are 4 miscarriaged Bottle-nose species and three are female.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_3.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;birth-year-and-sex-of-the-acquisitioned-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Birth Year and Sex of the Acquisitioned (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot4&amp;lt;-ggplot(SeaCreature,aes(x=acquisition,
                              y=birthYear,color=sex))+
       geom_jitter()+
       theme(axis.text.x =element_text(angle = 90, hjust = 1))+
       ggtitle(&amp;quot;Acquisitioned ones with their and BirthYear&amp;quot;)+
       ylab(&amp;quot;Birth Year&amp;quot;)+
       xlab(&amp;quot;Acquisition&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_4.png&amp;quot;,width = 12,height = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;birth-year-and-sex-of-the-acquisitioned-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Birth Year and Sex of the Acquisitioned (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 4&lt;/li&gt;
&lt;li&gt;With early Birth Year to until 1990 the creatures were captured.&lt;/li&gt;
&lt;li&gt;From Birth Year 1971 to 2017 only the creatures are born.&lt;/li&gt;
&lt;li&gt;After 1965 around 30 creatures have been rescued.&lt;/li&gt;
&lt;li&gt;Close to 40 creatures with unknown status with Birth Year known.&lt;/li&gt;
&lt;li&gt;Most of the rescued ones are of Male gender.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_4.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;species-and-their-sex-over-current-location-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species and their sex over current location (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot5&amp;lt;-ggplot(SeaCreature,aes(x=str_wrap(species,12),
                              y=currently,color=sex))+
       geom_jitter()+
       theme(axis.text.x =element_text(angle = 90, hjust = 1))+
       ggtitle(&amp;quot;Species and Sex over their Current Location&amp;quot;)+
       ylab(&amp;quot;Current Location&amp;quot;)+
       xlab(&amp;quot;Species&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_5.png&amp;quot;,width = 14,height = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;species-and-their-sex-over-current-location-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species and their sex over current location (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 5&lt;/li&gt;
&lt;li&gt;Close to 50 current locations.&lt;/li&gt;
&lt;li&gt;There are few locations with only one type of species.&lt;/li&gt;
&lt;li&gt;Bottle-nose creatures in most of these locations.&lt;/li&gt;
&lt;li&gt;Sea Life park in Hawaii has a diverse amount of Species.&lt;/li&gt;
&lt;li&gt;Sea world in San Diego is second when it comes to diversity.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_5.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acquisitioned-ones-and-thier-sex-with-status-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acquisitioned ones and thier Sex with Status (code)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Plot6&amp;lt;-ggplot(SeaCreature,aes(x=status,y=acquisition,color=sex))+
       geom_jitter()+
       ggtitle(&amp;quot;Acquisitioned with Sex and Status&amp;quot;)+
       xlab(&amp;quot;Status&amp;quot;)+
       ylab(&amp;quot;Acquisition&amp;quot;)+ 
       legend_bottom()  

#ggsave(&amp;quot;Plot_6.png&amp;quot;,width = 12,height = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;acquisitioned-ones-and-thier-sex-with-status-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acquisitioned ones and thier Sex with Status (plot)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Plot 6&lt;/li&gt;
&lt;li&gt;Most of the Captured creatures are Dead, but few of them Released.&lt;/li&gt;
&lt;li&gt;Most of the Rescued creatures are Dead, few alive and some Released.&lt;/li&gt;
&lt;li&gt;In Unknown acquisition-ed type alot of them are Dead.&lt;/li&gt;
&lt;li&gt;One rescued creature with unknown status.&lt;/li&gt;
&lt;li&gt;6 creatures which were born have been released and 50% are male.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/html/Plot_6.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ios slides are NICE.&lt;/li&gt;
&lt;li&gt;Jitter plots useful for categorical.&lt;/li&gt;
&lt;li&gt;Plots are too complex when using Location, Currently and Birth Year, but manageable.&lt;/li&gt;
&lt;li&gt;Bottle-nose species is holding a special place in this data-set.&lt;/li&gt;
&lt;li&gt;Alot of unknown data points when it comes to Birth Year.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;thank-you&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;THANK YOU&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the optim function</title>
      <link>/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/</link>
      <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-the-shape-parameters-of-beta-binomial-distribution&#34;&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#brief-of-optim-function&#34;&gt;Brief of optim Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nelder-and-mead-method&#34;&gt;Nelder and Mead method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bfgs-method&#34;&gt;BFGS method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cg-method&#34;&gt;CG method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l-bfgs-b-method&#34;&gt;L-BFGS-B method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sann-method&#34;&gt;SANN method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#brent-method&#34;&gt;Brent method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-time-evaluation-for-different-analytical-methods-of-optim-function&#34;&gt;Summary of Time evaluation for different Analytical methods of optim function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-results-after-using-the-optim-function-for-different-analytical-methods&#34;&gt;Summary of results after using the optim function for different analytical methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-conclusion&#34;&gt;Final Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;estimating-the-shape-parameters-of-beta-binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I wrote a &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/&#34;&gt;blog post&lt;/a&gt; earlier this month to understand the optimization functions in R and compare them. Here I am taking my time to go through one function at a time, only when there are more than one analytical method to use.&lt;/p&gt;
&lt;p&gt;Today I will scrutinize the optim function which has six analytical methods. Setting the stage, we have the Beta-Binomial distribution and Binomial Outcome data, and we need to estimate proper shape parameters which would minimize the Negative Log Likelihood value or Maximize the Log Likelihood value of the Beta-Binomial distribution for the above Binomial Outcome data. In this case Alcohol Consumption data from the &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt; package will be used.&lt;/p&gt;
&lt;p&gt;In this blog post we focus on the process time to optimization, estimated shape parameters, minimized Negative Log Likelihood value, expected frequencies, p-value and Over-dispersion in tables.&lt;/p&gt;
&lt;p&gt;Below are the six analytical methods in concern&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Nelder Mead&lt;/li&gt;
&lt;li&gt;BFGS&lt;/li&gt;
&lt;li&gt;CG&lt;/li&gt;
&lt;li&gt;L-BFGS-B&lt;/li&gt;
&lt;li&gt;SANN&lt;/li&gt;
&lt;li&gt;Brent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fitODBOD)
kable(Alcohol_data,&amp;quot;html&amp;quot;,align=c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 14,full_width = F) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Days
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
49
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;brief-of-optim-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brief of optim Function&lt;/h2&gt;
&lt;p&gt;Reading through the optim function &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#optim-function&#34;&gt;brief&lt;/a&gt; from the previous post it will help the reader regarding operational questions of the function.&lt;/p&gt;
&lt;p&gt;So for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would minimize the Negative Log Likelihood value of the Beta-Binomial distribution.&lt;/p&gt;
&lt;p&gt;First we are transforming the given EstMLEBetaBin function to satisfy the optim function conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new function to facilitate optim criteria
# only one input but has two elements
foroptim&amp;lt;-function(a)
  {
  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the foroptim function can be used as above and parameters are estimated for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (or a, b) for the Alcohol Consumption data week 1. Further the optim function can be scrutinized as below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : stats&lt;/li&gt;
&lt;li&gt;No of Inputs: 7&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list&lt;/li&gt;
&lt;li&gt;No of outputs: 5&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 6&lt;/li&gt;
&lt;li&gt;Default Method : Nelder-Mead&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;nelder-and-mead-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nelder and Mead method&lt;/h2&gt;
&lt;p&gt;Default analytical method is Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.&lt;/p&gt;
&lt;p&gt;Reference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.&lt;/p&gt;
&lt;p&gt;Below is the code of using optim function with Nelder and Mead analytical method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using default analytical method or Nelder and Mead
NM_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim)

# the outputs
NM_answer$par # estimated values for a, b
NM_answer$value # minimized function value 
NM_answer$counts  # see the documentation to understand
NM_answer$convergence # indicates successful completion
NM_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,NM_answer$par[1],NM_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgs-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BFGS method&lt;/h2&gt;
&lt;p&gt;The documentation indicates that BFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.&lt;/p&gt;
&lt;p&gt;Reference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.&lt;/p&gt;
&lt;p&gt;Below is the code for using optim function with BFGS analytical method&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using BFGS inputs
BFGS_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim,method = &amp;quot;BFGS&amp;quot;)

# the outputs
BFGS_answer$par # estimated values for a, b
BFGS_answer$value # minimized function value 
BFGS_answer$counts  # see the documentation to understand
BFGS_answer$convergence # indicates successful completion
BFGS_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           BFGS_answer$par[1],BFGS_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cg-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CG method&lt;/h2&gt;
&lt;p&gt;The documentation indicates the Method CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak–Ribiere or Beale–Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.&lt;/p&gt;
&lt;p&gt;Reference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.&lt;/p&gt;
&lt;p&gt;Using CG method with optim function is explained below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using CG inputs
CG_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim,method = &amp;quot;CG&amp;quot;)

# the outputs
CG_answer$par # estimated values for a, b
CG_answer$value # minimized function value 
CG_answer$counts  # see the documentation to understand
CG_answer$convergence # indicates successful completion
CG_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,CG_answer$par[1],CG_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;l-bfgs-b-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;L-BFGS-B method&lt;/h2&gt;
&lt;p&gt;Method L-BFGS-B is that of Byrd et. al. (1995) which allows box constraints, that is each variable can be given a lower and/or upper bound. The initial value must satisfy the constraints. This uses a limited-memory modification of the BFGS quasi-Newton method. If non-trivial bounds are supplied, this method will be selected, with a warning.&lt;/p&gt;
&lt;p&gt;Reference : Byrd, R.H., Lu, P., Nocedal, J. and Zhu, C., 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5), pp.1190-1208.&lt;/p&gt;
&lt;p&gt;Refer the below code chunk to under the L-BFGS-B method from optim function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using L-BFGS-B inputs
L_BFGS_B_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim,method = &amp;quot;L-BFGS-B&amp;quot;)

# the outputs
L_BFGS_B_answer$par # estimated values for a, b
L_BFGS_B_answer$value # minimized function value 
L_BFGS_B_answer$counts  # see the documentation to understand
L_BFGS_B_answer$convergence # indicates successful completion
L_BFGS_B_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           L_BFGS_B_answer$par[1],L_BFGS_B_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sann-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SANN method&lt;/h2&gt;
&lt;p&gt;Method SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.&lt;/p&gt;
&lt;p&gt;By default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to &lt;span class=&#34;math inline&#34;&gt;\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the current iteration step and temp and tmax are specifiable via control.&lt;/p&gt;
&lt;p&gt;Note that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.&lt;/p&gt;
&lt;p&gt;Reference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.&lt;/p&gt;
&lt;p&gt;Below mentioned code chunk is simply using SANN method for optim function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using default inputs
SANN_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim,method = &amp;quot;SANN&amp;quot;)

# the outputs
SANN_answer$par # estimated values for a, b
SANN_answer$value # minimized function value 
SANN_answer$counts  # see the documentation to understand
SANN_answer$convergence # indicates successful completion
SANN_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           SANN_answer$par[1],SANN_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;brent-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brent method&lt;/h2&gt;
&lt;p&gt;Brent Method is for one-dimensional problems only, using optimize(). It can be useful in cases where optim() is used inside other functions where only method can be specified, such as in mle from package stats4. Brent method does not work for our situation.&lt;/p&gt;
&lt;p&gt;Reference : Brent, R.P., 2013. Algorithms for minimization without derivatives. Courier Corporation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-time-evaluation-for-different-analytical-methods-of-optim-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of Time evaluation for different Analytical methods of optim function&lt;/h1&gt;
&lt;p&gt;Below considered table will compare the system process time for different analytical methods. In order to do this time comparison it is necessary to use the &lt;a href=&#34;https://www.rdocumentation.org/packages/rbenchmark/versions/1.0.0/topics/benchmark&#34;&gt;benchmark&lt;/a&gt; function of &lt;a href=&#34;https://cran.r-project.org/package=rbenchmark&#34;&gt;rbenchmark&lt;/a&gt; package. Below written code chunk provides the output in a table form which includes the analytical methods and their respective time values. The estimation process of the parameters where each method has been replicated 1000 times to receive a more accurate table for time values.&lt;/p&gt;
&lt;p&gt;The table is in accordance to the elapsed time value column in the ascending order. According to this we can see that least time takes to the Nelder and Mead method and most time is taken to the SANN method. These times completely depends on the Negative Log Likelihood function you need to minimize, the data you provided, the number of estimators that needs to be estimated, the complexity of the function and finally computer’s processing power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rbenchmark)

Results1&amp;lt;-benchmark(
          &amp;quot;NelderMead&amp;quot;={ optim(par = c(0.1,0.2), fn = foroptim)},
          &amp;quot;BFGS&amp;quot;={optim(par = c(0.1,0.2), fn = foroptim,method = &amp;quot;BFGS&amp;quot;)},
          &amp;quot;CG&amp;quot;={optim(par = c(0.1,0.2), fn = foroptim,method = &amp;quot;CG&amp;quot;)},
          &amp;quot;L-BFGS-B&amp;quot;={optim(par = c(0.1,0.2), fn = foroptim,method = &amp;quot;L-BFGS-B&amp;quot;)},
          &amp;quot;SANN&amp;quot;={optim(par = c(0.1,0.2), fn = foroptim,method = &amp;quot;SANN&amp;quot;)},
          replications = 1000,
          columns = c(&amp;quot;test&amp;quot;,&amp;quot;replications&amp;quot;,&amp;quot;elapsed&amp;quot;,
                      &amp;quot;relative&amp;quot;,&amp;quot;user.self&amp;quot;,&amp;quot;sys.self&amp;quot;),
          order = &amp;#39;elapsed&amp;#39;
          )

kable(Results1,&amp;quot;html&amp;quot;,align = c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(full_width = T,bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 14) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: blue;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
test
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
replications
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
elapsed
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
relative
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
user.self
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
sys.self
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NelderMead
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.75
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
L-BFGS-B
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11.66
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.085
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11.64
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BFGS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19.10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.777
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
CG
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
37.83
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.519
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
37.80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
SANN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1710.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
159.135
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1684.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-results-after-using-the-optim-function-for-different-analytical-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of results after using the optim function for different analytical methods&lt;/h1&gt;
&lt;p&gt;After using the methods Nelder Mead, BFGS, CG, L-BFGS-B and SANN to estimate the shape parameters a, b we can use the estimated parameters in the function fitBetaBin. Using this function we can find expected frequencies for each of these analytical methods and compare p-values and over-dispersion. Further, understand if using different analytical methods had any effect on them.&lt;/p&gt;
&lt;p&gt;According to the below table there is no significant changes between the expected frequencies, except while using SANN method. All five methods generate different Over-dispersion values after the first three decimal places. Negative Log Likelihood values and p values are same for all 5 methods until first three decimal places. This is a clear indication of it does not matter what analytical method we use the estimation will occur effectively but only efficiency will be affected.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BinomialRandomVariable
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
NelderMead
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BFGS
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
CG
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
LBFGSB
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
SANN
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.61
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.89
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.52
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.04
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.96
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.05
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.77
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Total No of Observations
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
p-value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Estimated a and b
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7230707 b=0.5809894
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7228930 b=0.5807279
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229414 b=0.5808477
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229432 b=0.5808496
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7215669 b=0.5802982
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Negative Log Likelihood
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4573
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Over Dispersion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340165
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340992
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340675
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340668
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4344303
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;final-conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Conclusion&lt;/h1&gt;
&lt;p&gt;We had 6 methods to compare but choosing one over the other is completely harmless to the final result of estimation as seen by our tables. And our situation forces us to not use the Brent method. The only issue is time, therefore I would recommend choose the best analytical method from the optim function based on your needs of output and research objective.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 37: NYC Restaurants</title>
      <link>/post/week_37/week-37-nyc-restaurants/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/week_37/week-37-nyc-restaurants/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inspection-type&#34;&gt;Inspection Type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#critical-flag&#34;&gt;Critical Flag&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inspection-type-and-critical-flag-over-the-years&#34;&gt;Inspection Type and Critical Flag over the years&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cycle-inspection&#34;&gt;Cycle Inspection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pre-permit-operational&#34;&gt;Pre-permit Operational&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#administrative-miscellaneous&#34;&gt;Administrative Miscellaneous&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-inspected-restaurants&#34;&gt;Most Inspected Restaurants&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dunkin-donuts&#34;&gt;Dunkin Donuts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subway&#34;&gt;Subway&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcdonalds&#34;&gt;McDonalds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#starbucks&#34;&gt;Starbucks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kennedy-fried-chicken&#34;&gt;Kennedy Fried Chicken&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(magrittr)
library(ggthemr)
library(lubridate)
library(stringr)
library(kableExtra)

#using theme
ggthemr(&amp;quot;fresh&amp;quot;)

#load data
NYC&amp;lt;-read_csv(&amp;quot;nyc_restaurants.csv&amp;quot;, 
    col_types = cols(inspection_date = 
                       col_date(format = &amp;quot;%m/%d/%Y&amp;quot;)))
attach(NYC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data set is completed with more than 300000 records and several important variables such as inspection date, violation code, critical flag, score and violation description. In this article I will mainly focus on Inspection Type, boro, Inspection year, cuisine type and Critical Flag.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;So NYC Restaurants. Ha. My take on this topic. &lt;br&gt;The code in GitHub: &lt;a href=&#34;https://t.co/vbTREkq32Y&#34;&gt;https://t.co/vbTREkq32Y&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/AFDjiOfk9o&#34;&gt;pic.twitter.com/AFDjiOfk9o&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1073204074579968000?ref_src=twsrc%5Etfw&#34;&gt;December 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/Week_37&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;First see what type of inspections have occurred.&lt;/p&gt;
&lt;div id=&#34;inspection-type&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inspection Type&lt;/h1&gt;
&lt;p&gt;This ordered bar plot for inspection type clearly indicates that cycle inspection / initial inspection has the highest amount of counts with 171,657. while second place goes to cycle inspection/ re-inspection with 73207. Other types of inspection hold less than 21,000 counts, where there are more than 10 types of inspections which hold counts less than 1000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#summary.factor(inspection_type)  %&amp;gt;%
#  sort()

# Bar plot for Insepction type
ggplot(NYC,aes(x=fct_infreq(str_wrap(inspection_type,35))))+
  geom_bar()+coord_flip()+
  scale_y_continuous(breaks = seq(0,200000,25000),labels = seq(0,200000,25000))+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.005)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Type of Insepction&amp;quot;)+
  ggtitle(&amp;quot;Types of Inspection&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Inspection%20Type-1.png&#34; width=&#34;1248&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critical-flag&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Critical Flag&lt;/h1&gt;
&lt;p&gt;Critical flag has three types of categories which are Critical, Not Critical and Not Applicable. It is represented as a pie chart where 54.9% are critical(164,623) and second place goes to Not Critical(129,348) with 43.1%. Finally, only 6029 inspections have lead to Not Applicable which is represented by 2.0%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#summary for critical flag
#NYC$critical_flag %&amp;gt;%
#      summary.factor() %&amp;gt;%
#      sum()
value=c(164623,6029,129348)

# creating data frame for Critical flag
CF&amp;lt;-data.frame(
              group=c(&amp;quot;Critical&amp;quot;,&amp;quot;Not Applicable&amp;quot;,&amp;quot;Not Critical&amp;quot;),
              value=c(164623,6029,129348),
              per=round(value*100/300000,4)
              )

# pie chart for percentages
P1&amp;lt;-ggplot(CF,aes(x=&amp;quot;&amp;quot;,y=per,fill=group))+
    geom_col()+ theme_void()+
    geom_text(aes(label=scales::percent(per/100)),position=position_stack(vjust=0.5))+
    labs(title = &amp;quot;Critical Flag \nDistribution&amp;quot;,fill=&amp;quot;Type&amp;quot;)+
    coord_polar(theta = &amp;quot;y&amp;quot;,start = 0)

# pie chart for counts
P2&amp;lt;-ggplot(CF,aes(x=&amp;quot;&amp;quot;,y=value,fill=group))+
    geom_col()+theme_void()+
    labs(title = &amp;quot;Critical Flag\n Distribution&amp;quot;,fill=&amp;quot;Type&amp;quot;)+ 
    geom_text(aes(label = value), position = position_stack(vjust = 0.5)) +
    coord_polar(theta = &amp;quot;y&amp;quot;,start = 0)

gridExtra::grid.arrange(P1,P2,nrow=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Critical%20flag-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inspection-type-and-critical-flag-over-the-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inspection Type and Critical Flag over the years&lt;/h1&gt;
&lt;p&gt;Now facing the issue of inspection year, inspection type and critical flag, I have generated bar plots for identifying the behavior. Basically what we have is bar plots for years from 2012 to 2018 how the counts of Critical flag types have changed for different paired types of inspections.&lt;/p&gt;
&lt;p&gt;Inspection types considered&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cycle Inspection / Initial Inspection and Cycle Inspection / Re-inspection.&lt;/li&gt;
&lt;li&gt;Pre-permit (Operational) / Initial Inspection and Pre-permit (Operational) / Re-inspection.&lt;/li&gt;
&lt;li&gt;Administrative Miscellaneous / Initial Inspection and Administrative Miscellaneous / Re-inspection.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;cycle-inspection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cycle Inspection&lt;/h2&gt;
&lt;p&gt;Initial Inspection is always high for all the years while assigning Critical. The years 2012, 2013 and 2014 has very low amount of counts while the succeeding years have increasing counts. Initial inspection over the years from 2015 is increasing for the gap between Critical and Not Critical. In 2015 the above gap is close to 4000 but by 2018 it has increased to 10000. If we consider Re-inspection the gap for Critical and Not Critical is close 1000 in year 2015 but by year 2018 it is 4000. Not applicable is increasing over the years for both initial inspection and re-inspection, even though the amounts are in hundreds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting data 
# Specific insepction type, critical flag and year with bar plot 
subset(NYC,inspection_type==&amp;quot;Cycle Inspection / Initial Inspection&amp;quot; |
           inspection_type==&amp;quot;Cycle Inspection / Re-inspection&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(x=str_wrap(inspection_type,8),fill=critical_flag))+
      geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
      facet_wrap(~year(inspection_date)) +
      xlab(&amp;quot;Cycle Inspection&amp;quot;)+
      ylab(&amp;quot;Frequency&amp;quot;)+
      ggtitle(&amp;quot;Cycle Inspection over the years for Critical Flag&amp;quot;)+
      labs(fill=&amp;quot;Critical Flag&amp;quot;)+
       geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),
                 position = position_dodge(width = 1), vjust = -0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Cycle%20Inspection-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-permit-operational&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-permit Operational&lt;/h2&gt;
&lt;p&gt;Second most considered type of inspection is Pre-permit Operational, where it begins from 2014. Even though in year 2014 the amounts for initial inspection and re-inspection are less than 20 in all three critical flag categories. Considering the gap between Critical and Not Critical for initial inspection over the years there is a clear increase. Where as in year 2015 the gap is slightly less than 400, next year it is close to 1000, but by year 2018 this is gap is above than 2000.&lt;/p&gt;
&lt;p&gt;In re-inspection for the year 2015 the gap is almost 150, yet with more inspections by 2018 the gap increases to 600. For, Not Applicable the counts do not have a clear increasing or decreasing pattern over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting data 
# Specific insepction type, critical flag and year with bar plot 
subset(NYC,inspection_type==&amp;quot;Pre-permit (Operational) / Initial Inspection&amp;quot; |
           inspection_type==&amp;quot;Pre-permit (Operational) / Re-inspection&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+
       geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
      facet_wrap(~year(inspection_date)) +
      xlab(&amp;quot;Pre-permit (Operational)&amp;quot;)+
      ylab(&amp;quot;Frequency&amp;quot;)+
      ggtitle(&amp;quot;Pre-permit (Operational) over the years for Critical Flag&amp;quot;)+
      labs(fill=&amp;quot;Critical Flag&amp;quot;)+
      facet_wrap(~year(inspection_date)) +
       geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),
                 position = position_dodge(width = 1), vjust = -0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Pre-permit%20(operational)-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;administrative-miscellaneous&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Administrative Miscellaneous&lt;/h2&gt;
&lt;p&gt;Very odd bar plot here than previous two plots for inspection types. There is no Critical type restaurants in the Administrative Miscellaneous inspection type. The counts begin from year 2014 but the amounts are less than 10. Clearly for initial inspections over the years from 2015 to 2018 the count for Not Applicable are increasing, but this is not the case for Not Critical.&lt;/p&gt;
&lt;p&gt;In year 2015 the amount for the type Not Critical for initial inspection was 528, but in years 2016 and 2018 the amounts increased to respectively to 1038 and 1486. Even though the amount decreased to 1015 in year 2017. The same pattern of increase and decrease behavior occurs for Re-inspection type as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting data 
# Specific insepction type, critical flag and year with bar plot 
subset(NYC,inspection_type==&amp;quot;Administrative Miscellaneous / Initial Inspection&amp;quot; |
           inspection_type==&amp;quot;Administrative Miscellaneous / Re-inspection&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+
     geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
   facet_wrap(~year(inspection_date)) +
      xlab(&amp;quot;Administrative Miscellaneous&amp;quot;)+
      ylab(&amp;quot;Frequency&amp;quot;)+
      ggtitle(&amp;quot;Administrative Miscellaneous over the years for Critical Flag&amp;quot;)+
      labs(fill=&amp;quot;Critical Flag&amp;quot;)+
      facet_wrap(~year(inspection_date)) +
       geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),
                 position = position_dodge(width = 1), vjust = -0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Administrative%20Miscellaneous-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;most-inspected-restaurants&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Most Inspected Restaurants&lt;/h1&gt;
&lt;p&gt;First look at the top 5 restaurants which were inspected and clearly Dunkin’ Donuts has the highest amount with 3136, while second place goes to Subway with 2419 and third place goes to McDonald’s with 1783.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Most 5 restaurants which were inspected
kable(summary.factor(dba) %&amp;gt;%
              sort() %&amp;gt;%
              tail(5)
      ,col.names = c(&amp;quot;Frequency&amp;quot;),align = &amp;#39;c&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Frequency
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
KENNEDY FRIED CHICKEN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1031
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
STARBUCKS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1636
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MCDONALD’S
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1783
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUBWAY
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2419
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DUNKIN’ DONUTS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3136
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;dunkin-donuts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dunkin Donuts&lt;/h2&gt;
&lt;p&gt;Queens has close to 1000 observations of Dunkin Donuts and lowest amount goes to Staten Island with 308. Other three boros have counts in between 550 and 710.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting Dunkin Donuts with boro
subset(NYC, dba==&amp;quot;DUNKIN&amp;#39; DONUTS&amp;quot;) %&amp;gt;%
  ggplot(.,aes(x=boro))+
  geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..), vjust = -0.05)+
  ggtitle(&amp;quot;How many Dunkin Donuts in Boro&amp;quot;)+
  xlab(&amp;quot;Boro&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Dunkin%20Donuts-1.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are only 5 cuisine types and prominent ones are American, Donuts and drinks(Cafe/Coffee/Tea). Year 2014 is very low in amounts even for cuisine Donuts, but this is not the case over the next few years and the scores are mostly centered between 5 to 15. Bagels/Pretzels and Jewish/ Kosher have least amount of data where none of scores are above 25. It is safe to to say we have more Critical type data and they are mostly close to the score of 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Dunkin Donuts and scores with critical flag
subset(NYC, dba==&amp;quot;DUNKIN&amp;#39; DONUTS&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+
      geom_jitter(alpha=0.3)+labs(color=&amp;quot;Critical Flag&amp;quot;)+    
      ggtitle(&amp;quot;Dunkin Donuts score changes with Critical Flag for Cuisines&amp;quot;)+
      xlab(&amp;quot;Critical Flag&amp;quot;)+ylab(&amp;quot;Score&amp;quot;)+
      scale_y_continuous(breaks = seq(0,60,5),labels =seq(0,60,5))+
      facet_wrap(~cuisine_description) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Dunkin%20Donuts,%20score%20and%20critical%20flag%20with%20cuisine%20description%20over%20years-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subway&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subway&lt;/h2&gt;
&lt;p&gt;912 points from Manhattan with the highest count and lowest count goes to Staten Island with 141 counts. Bronx and Brooklyn has counts in between 320 and 365, but Queens boro has an amount of 683.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting Subway with boro
subset(NYC, dba==&amp;quot;SUBWAY&amp;quot;) %&amp;gt;% 
  ggplot(.,aes(x=boro))+
  geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..), vjust = -0.05)+
  ggtitle(&amp;quot;How many Subways in Boro&amp;quot;)+
  xlab(&amp;quot;Boro&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Subway-1.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subway has only 5 cuisines which are American, Other, Sandwiches, Sandwiches/Salads/Mixed Buffet and Soups &amp;amp; Sandwiches. In these five categories only Sandwiches has significant amount of data points. Oddly, we can see the year 1900 in the x axis and which means error.&lt;/p&gt;
&lt;p&gt;In Sandwiches category there are more points which are centered in between 5 to 15 in scores, while most of them are Not Critical. Oddly in 2018 there are Critical data points with scores above 55 in Sandwiches cuisine type.&lt;/p&gt;
&lt;p&gt;For the Other category there are only 4 observations which are in 2018 and 75% of them are Not Critical. Considering American Cuisines there are points in all 4 years and most of them are less than 25% and Not Critical.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Subway and scores with critical flag
subset(NYC, dba==&amp;quot;SUBWAY&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+
      geom_jitter(alpha=0.3)+ labs(color=&amp;quot;Critical Flag&amp;quot;)+    
      ggtitle(&amp;quot;Subway score changes with Critical Flag for Cuisines&amp;quot;)+
      xlab(&amp;quot;Critical Flag&amp;quot;)+ylab(&amp;quot;Score&amp;quot;)+
      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+
      facet_wrap(~cuisine_description) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Subway,%20score%20and%20critical%20flag%20with%20cuisine%20description%20over%20years-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcdonalds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;McDonalds&lt;/h2&gt;
&lt;p&gt;Close to 550 data points are from Manhattan boro, but Staten Island boro has points close to 75. Second place of 487 counts goes to Queens boro. Other two boros, which are Bronx and Brooklyn have counts in the range of 320 and 360.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting McDonalds with boro
subset(NYC, dba==&amp;quot;MCDONALD&amp;#39;S&amp;quot;) %&amp;gt;% 
  ggplot(.,aes(x=boro))+
  geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..), vjust = -0.05)+
  ggtitle(&amp;quot;How many McDonalds in Boro&amp;quot;)+
  xlab(&amp;quot;Boro&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/McDonalds-1.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Only the cuisines Other and Sandwiches/Salads/Mixed Buffet has data points in the years 2017 and 2018 and these points have an amount less than 15 in both cases. Considering the other two cuisines which are American and Hamburgers, there are more points in the latter than in the former. It should be noted that Hamburgers cuisine has mostly points centered close to the score range of 5 to 10, and these points are mostly Not Critical.&lt;/p&gt;
&lt;p&gt;In American cuisine for the year 2016 there are 5 points which have scores close to 70, even though in any other year this has not occurred. To be more precise, except those 5 points all the others have scores less than 45 for American Cuisine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# McDonalds and scores with critical flag
subset(NYC, dba==&amp;quot;MCDONALD&amp;#39;S&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+
      geom_jitter(alpha=0.3)+      
      labs(color=&amp;quot;Critical Flag&amp;quot;)+    
      ggtitle(&amp;quot;McDonalds score changes with Critical Flag for Cuisines&amp;quot;)+
      xlab(&amp;quot;Critical Flag&amp;quot;)+ylab(&amp;quot;Score&amp;quot;)+
      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+
      facet_wrap(~cuisine_description) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/McDonalds%20scores,%20score%20and%20critical%20flag%20with%20cuisine%20description%20over%20years-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starbucks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starbucks&lt;/h2&gt;
&lt;p&gt;There are more than 1100 Starbucks in Manhattan only while other boros have less than 220 and lowest amount goes to Bronx with 31. Second lowest goes to Staten Island with 41.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#subsetting Starbucks with boro
subset(NYC, dba==&amp;quot;STARBUCKS&amp;quot;) %&amp;gt;% 
  ggplot(.,aes(x=boro))+
  geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..), vjust = -0.05)+
  ggtitle(&amp;quot;How many Starbucks in Boro&amp;quot;)+
  xlab(&amp;quot;Boro&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Starbucks-1.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Starbucks is mainly focused on Cafe/Coffee/Tea rather than cuisines such as Pizza, Sandwiches, American and other. Clearly there are negligible amount of data points in those four categories, except American cuisine.&lt;/p&gt;
&lt;p&gt;If we focus on Drinks(Coffee/Cafe/Tea), evidently most of them are Not Critical and they are centered around the score of 0 to 10. Even though scores higher than 15 do occur they are spread out widely. This is a common occurrence for all four years, which is from 2015 to 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Starbucks and scores with critical flag
subset(NYC, dba==&amp;quot;STARBUCKS&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+
      geom_jitter(alpha=0.3)+
      labs(color=&amp;quot;Critical Flag&amp;quot;)+    
      ggtitle(&amp;quot;Starbucks score changes with Critical Flag for Cuisines&amp;quot;)+
      xlab(&amp;quot;Critical Flag&amp;quot;)+ylab(&amp;quot;Score&amp;quot;)+
      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+
      facet_wrap(~cuisine_description) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Starbucks,%20score%20and%20critical%20flag%20with%20cuisine%20description%20over%20years-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kennedy-fried-chicken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kennedy Fried Chicken&lt;/h2&gt;
&lt;p&gt;Bronx the has most amount observations to Kennedy Fried Chicken’s with 626 while lowest count of 5 is from Staten Island. Other three boros have counts in the range of 75 to 190.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#subsetting Kennedy Fried Chicken with boro
subset(NYC, dba==&amp;quot;KENNEDY FRIED CHICKEN&amp;quot;) %&amp;gt;% 
  ggplot(.,aes(x=boro))+
  geom_bar(position = &amp;quot;dodge&amp;quot;,stat = &amp;quot;count&amp;quot;)+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..), vjust = -0.05)+
  ggtitle(&amp;quot;How many Kennedy Fried Chicken&amp;#39;s in Boro&amp;quot;)+
  xlab(&amp;quot;Boro&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Kennedy%20Fried%20Chicken-1.png&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are only four cuisines which are related to Kennedy Fried Chicken, and they are American, Chicken, Hamburgers and other. The categories Hamburgers and Other have very less amount of counts and the year 1900 is also mentioned. For Hamburgers cuisine only the year 2017 has significant amount of data points, where the year 2015 has only one and year 2018 has only two points.&lt;/p&gt;
&lt;p&gt;Further, the scores for these points are always less than 20 and mostly Critical. Cuisines of Chicken has more data points than American but in both types there is no clear centering of data around a certain score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Kennedy Fried Chicken and scores with critical flag
subset(NYC, dba==&amp;quot;KENNEDY FRIED CHICKEN&amp;quot;) %&amp;gt;%
ggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+
      geom_jitter(alpha=0.3)+     
      labs(color=&amp;quot;Critical Flag&amp;quot;)+    
      ggtitle(&amp;quot;Kennedy Fried Chicken score changes with Critical Flag&amp;quot;)+
      xlab(&amp;quot;Critical Flag&amp;quot;)+ylab(&amp;quot;Score&amp;quot;)+
      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+
      facet_wrap(~cuisine_description) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_37/2018-12-13-week-37-nyc-restaurants_files/figure-html/Kennedy%20Fried%20Chicken,%20score%20and%20critical%20flag%20with%20cuisine%20description%20over%20years-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and a table in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;facet wrap is very useful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adding colors makes it more useful for the plots.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pie chart using bar chart looks good.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It is possible to focus on violation codes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further looking at the cuisines we can divide it based on the boros as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 36: Medium Posts</title>
      <link>/post/week_36/week-36-medium-posts/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/week_36/week-36-medium-posts/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#claps&#34;&gt;Claps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-10-authors-and-claps-for-their-posts&#34;&gt;Top 10 Authors and Claps for their posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-10-authors-with-most-posts&#34;&gt;Top 10 Authors with most posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-10-authors-who-have-posts-with-images&#34;&gt;Top 10 Authors who have posts with Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-10-authors-who-have-posts-without-images&#34;&gt;Top 10 Authors who have posts without Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-10-authors-and-reading-time-for-their-posts&#34;&gt;Top 10 Authors and Reading time for their posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-5-publications-with-most-posts&#34;&gt;Top 5 Publications with most posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#word-cloud-for-the-titles-from-top-10-authors&#34;&gt;Word Cloud for the Titles from Top 10 Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#word-cloud-for-the-titles-from-top-5-publications&#34;&gt;Word Cloud for the Titles from Top 5 publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#loading packages
#load data
library(readr)
#manipulate data
library(dplyr)
library(magrittr)
# format table with expense
library(formattable)
# knitting the document
library(knitr)
# another type of table
library(kableExtra)
# playing with strings
library(stringr)
# combining two plots
library(grid)
library(gridExtra)
# that theme you wanted
library(ggthemr)
# text analysis
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)

# adding theme called fresh for plots
ggthemr(&amp;quot;fresh&amp;quot;)
#loading the data
medium &amp;lt;- read_csv(&amp;quot;medium_datasci.csv&amp;quot;)
attach(medium)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Focusing on Claps with Authors and publications, where does writing alot of posts will get you popularity and claps. The code will focus on Top 10 Authors with most of the posts and Claps they have received. Further, does having an image in the post matter ?. Finally, word clouds for Top 10 authors and Top 5 publications with their titles.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;my take medium posts on week 36  &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/FTAZh0vKI8&#34;&gt;https://t.co/FTAZh0vKI8&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1071369742672429056?ref_src=twsrc%5Etfw&#34;&gt;December 8, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/Week_36&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;claps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Claps&lt;/h1&gt;
&lt;p&gt;Table indicates that 25,729 posts have 0 claps, while 7,093 posts with only one clap, and finally 3044 posts with 2 claps. The only odd one is posts with 50 claps where the count is 970.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extracting the top 15 with most claps
claps_table&amp;lt;-table(claps) %&amp;gt;%
                     sort() %&amp;gt;%
                     tail(15)

# table it up 
kable(t(claps_table),&amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;,full_width = T) %&amp;gt;%
  row_spec(0,bold = T,font_size = 13,color = &amp;quot;grey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
13
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
12
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
9
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
8
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
11
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
7
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
50
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
10
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
6
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
5
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;color: grey;font-size: 13px;&#34;&gt;
0
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
602
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
634
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
661
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
721
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
759
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
864
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
989
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1124
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1389
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1402
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7093
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25729
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-authors-and-claps-for-their-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 Authors and Claps for their posts&lt;/h1&gt;
&lt;p&gt;There are only two posts which do not have Images in their content. The highest number of claps is 60,000, a post written by Sophia Ciocca under the title “How Does Spotify Know You So Well?”. Second Place is for the article “Blockchain is not only crappy technology but a bad vision for the future” which was written by Kai Stinchombe with 53,000 claps. De Xun is the only author who has two articles which are in this list on the places 8 and 9 with claps respectively 37,000 and 36,000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# seperate medium with author, titles, claps and image
claps_A_I&amp;lt;-medium[,c(&amp;quot;author&amp;quot;,&amp;quot;title&amp;quot;,&amp;quot;subtitle&amp;quot;,&amp;quot;claps&amp;quot;,&amp;quot;image&amp;quot;)] %&amp;gt;%
              arrange(claps) %&amp;gt;%
              tail(10)
names(claps_A_I)&amp;lt;-c(&amp;quot;Author&amp;quot;,&amp;quot;Title&amp;quot;,&amp;quot;Subtitle&amp;quot;,&amp;quot;Claps&amp;quot;,&amp;quot;Image&amp;quot;)

# table it
formattable(claps_A_I[,-3],align=c(&amp;quot;l&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;c&amp;quot;),
            list(
              Claps=color_tile(&amp;quot;lightblue&amp;quot;,&amp;quot;blue&amp;quot;),
              Image=color_tile(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;)
            ))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Author
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Title
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Claps
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Image
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vishal Maini
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A Beginners Guide to AI/ML
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #add8e6&#34;&gt;36000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
De Xun
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SWIPE Bi-Weekly Update, 16th-27th July
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #add8e6&#34;&gt;36000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
De Xun
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
[PARTNERSHIP] SWIPE-Bluzelle: Building SWIPEs decentralized database
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #a5cfe7&#34;&gt;37000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Andrej Karpathy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Software 2.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #9ec6e8&#34;&gt;38000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ff0000&#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Radu Raicea
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Want to know how Deep Learning works? Heres a quick guide for everyone.
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #97bde9&#34;&gt;39000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Anything App
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fast-forward twenty years with Anything App.
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #81a2ec&#34;&gt;42000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ff0000&#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Michael Jordan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Artificial IntelligenceThe Revolution Hasnt Happened Yet
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #647ef0&#34;&gt;46000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Xiaohan Zeng
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #4f63f3&#34;&gt;49000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kai Stinchcombe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Blockchain is not only crappy technology but a bad vision for the future
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #323ff7&#34;&gt;53000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sophia Ciocca
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
How Does Spotify Know You So Well?
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #0000ff&#34;&gt;60000&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00ff00&#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-authors-with-most-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 Authors with most posts&lt;/h1&gt;
&lt;p&gt;Yves Mulkers has most amount of posts with 487 but only 3,779 claps. This is not the case for Corsairs publishing where for 156 posts the number of claps are 111,501. Even looking at other authors names we can see that writing alot of posts does not create claps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# finding who are the top 10 authors with claps
# summary.factor(medium$author) %&amp;gt;%
#  sort() %&amp;gt;%
#  tail(11)

# extracting posts only from the top 10 authors with most posts
Top10_author&amp;lt;-subset(medium,
       author ==&amp;quot;C Gavilanes&amp;quot; | author == &amp;quot;Jae Duk Seo&amp;quot; |
       author == &amp;quot;Corsair&amp;#39;s Publishing&amp;quot; | author==&amp;quot;Alibaba Cloud&amp;quot; |
       author == &amp;quot;Ilexa Yardley&amp;quot; | author == &amp;quot;Peter Marshall&amp;quot; |
       author == &amp;quot;AI Hawk&amp;quot; | author == &amp;quot;DEEP AERO DRONES&amp;quot; |
       author == &amp;quot;Synced&amp;quot; | author == &amp;quot;Yves Mulkers&amp;quot;)

# plotting the top 10 authors
g1_Top10_a&amp;lt;-ggplot(Top10_author,aes(x=author))+
            coord_flip()+ geom_bar()+
            xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
            ggtitle(&amp;quot;Top 10 Authors and number of posts they have written&amp;quot;)+
            geom_text(stat = &amp;#39;count&amp;#39;,aes(label=..count..),hjust=0.5)

# plotting the top 10 authors and their claps
g2_Top10_a&amp;lt;-Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;claps&amp;quot;)] %&amp;gt;%
            group_by(author) %&amp;gt;%
            summarise_each(funs(sum)) %&amp;gt;%
            ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = &amp;quot;identity&amp;quot;)+
            ggtitle(&amp;quot;Total number of Claps they got&amp;quot;)+
            xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Claps&amp;quot;)+
            coord_flip()+geom_text(aes(label=claps),hjust =0.5 )

# plotting two plots at same grid
grid.arrange(g1_Top10_a,g2_Top10_a,ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/Top%2010%20Authors%20in%20a%20table-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-authors-who-have-posts-with-images&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 Authors who have posts with Images&lt;/h1&gt;
&lt;p&gt;Extracting the top 10 authors with posts which have images it clear most of the posts do have Images and they do generate claps. This is true for Corsairs’s Publishing. With 154 posts it generates 109,906 claps. There are authors who have written more posts than totally received claps . It should be noted that two Authors did not add any Images for their post and they are Peter Marshall and C Gavilanes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plotting top 10 authors with Image
I1_g1_Top10_a&amp;lt;-subset(Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;image&amp;quot;)],image==1) %&amp;gt;%
            ggplot(.,aes(x=author))+ geom_bar()+coord_flip()+
            xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+  
            ggtitle(&amp;quot;Top 10 Authors and posts which has Images&amp;quot;)+
            geom_text(stat=&amp;#39;count&amp;#39;,aes(label=..count..),hjust =0.5 )

# plotting the claps for top 10 authors with Image 
I1_g2_Top10_a&amp;lt;-subset(Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;claps&amp;quot;,&amp;quot;image&amp;quot;)],image==1) %&amp;gt;%
               group_by(author,image) %&amp;gt;%
               summarise_each(funs(sum)) %&amp;gt;%
               ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = &amp;quot;identity&amp;quot;)+
               coord_flip()+
               xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Claps&amp;quot;)+
               ggtitle( &amp;quot;Total number of Claps they got&amp;quot;)+
               geom_text(aes(label=claps),hjust =0.5 )    

# top plots one grid
grid.arrange(I1_g1_Top10_a,I1_g2_Top10_a,ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/Top%2010%20Authors%20when%20there%20is%20Image-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-authors-who-have-posts-without-images&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 Authors who have posts without Images&lt;/h1&gt;
&lt;p&gt;Posts without images have very low amount of total claps. To be specific 14 posts by Jae Duk Seo have 1833 claps but 2 posts by Corsair’s publishing has 1595 claps. That is very Impressive. Further there are even posts which have claps less than 10 where the number of posts is less than 5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plotting top 10 authors with No Image
I0_g1_Top10_a&amp;lt;-subset(Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;image&amp;quot;)],image==0) %&amp;gt;%
               ggplot(.,aes(x=author))+ geom_bar()+coord_flip()+
               xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+  
               ggtitle(&amp;quot;Top 10 Authors and posts without Images&amp;quot;)+
               geom_text(stat=&amp;#39;count&amp;#39;,aes(label=..count..),hjust =0.5 )

# plotting the claps for top 10 authors with No Image 
I0_g2_Top10_a&amp;lt;-subset(Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;claps&amp;quot;,&amp;quot;image&amp;quot;)],image==0) %&amp;gt;%
               group_by(author,image) %&amp;gt;%
               summarise_each(funs(sum)) %&amp;gt;%
               ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = &amp;quot;identity&amp;quot;)+
               coord_flip()+
               xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Claps&amp;quot;)+               
               ggtitle(&amp;quot;Total number of claps they got&amp;quot;)+
               geom_text(aes(label=claps),hjust =0.5 )

# top plots one grid
grid.arrange(I0_g1_Top10_a,I0_g2_Top10_a,ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/Top%2010%20Authors%20when%20there%20is%20No%20Image-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-10-authors-and-reading-time-for-their-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 10 Authors and Reading time for their posts&lt;/h1&gt;
&lt;p&gt;Reading in minutes, does it has anything to do with number of posts?. Looking at the plot it is clear that posts from Synced has more total reading time than Yves Mulkers with highest number posts. The difference between posts is close 150, while difference between reading times is above 150 for Yves Mulkers and Synced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plotting top 10 authors with reading times
RT_g1_Top10_a&amp;lt;-Top10_author[,c(&amp;quot;author&amp;quot;,&amp;quot;reading_time&amp;quot;)] %&amp;gt;%
            group_by(author) %&amp;gt;%
            summarise_each(funs(sum)) %&amp;gt;%
            ggplot(.,aes(x=author,y=reading_time))+ geom_bar(stat = &amp;quot;identity&amp;quot;)+
            ggtitle(&amp;quot;Reading Time&amp;quot;)+
            xlab(&amp;quot;Author&amp;quot;)+ylab(&amp;quot;Reading Time in minutes&amp;quot;)+  
            coord_flip()+geom_text(aes(label=reading_time),hjust =0.5 )

# printing the above plot with number of posts 
grid.arrange(g1_Top10_a,RT_g1_Top10_a,ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/Top%2010%20Authors%20and%20reading%20time%20for%20their%20posts-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-5-publications-with-most-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 5 Publications with most posts&lt;/h1&gt;
&lt;p&gt;Publications with most number of posts has the highest number of claps and order achieved&lt;br /&gt;
in the “Top 5 publications and number of posts they have written” plot is maintained in the “Total number of Claps they got” plot as well. This simply refers, when you write alot of posts under a publication you will receive alot of claps because of the foundation this specific publications holds in Medium.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# finding who are the top 5 publications with claps
# summary.factor(medium$publication) %&amp;gt;%
#  sort() %&amp;gt;%
#  tail(11)

# extracting posts only from the top 5 publications with most posts
Top5_pub&amp;lt;-subset(medium,
       publication ==&amp;quot;Towards Data Science&amp;quot; | 
       publication == &amp;quot;Hacker Noon&amp;quot; |
       publication == &amp;quot;Becoming Human: Artificial Intelligence Magazine&amp;quot; |
       publication ==&amp;quot;Chatbots Life&amp;quot; |
       publication == &amp;quot;Data Driven Investor&amp;quot; )

# plotting the top 5 publications
g1_Top5_p&amp;lt;-ggplot(Top5_pub,aes(x=str_wrap(publication,15)))+
            coord_flip()+ geom_bar()+
            xlab(&amp;quot;Publication&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+  
            ggtitle(&amp;quot;Top 5 Publications and number of posts they have written&amp;quot;)+
            geom_text(stat = &amp;#39;count&amp;#39;,aes(label=..count..),hjust=0.5)

# plotting the top 5 publications and their claps
g2_Top5_p&amp;lt;-Top5_pub[,c(&amp;quot;publication&amp;quot;,&amp;quot;claps&amp;quot;)] %&amp;gt;%
            group_by(publication) %&amp;gt;%
            summarise_each(funs(sum)) %&amp;gt;%
            ggplot(.,aes(x=str_wrap(publication,15),y=claps))+
            geom_bar(stat=&amp;quot;identity&amp;quot;)+
            xlab(&amp;quot;Publication&amp;quot;)+ylab(&amp;quot;Claps&amp;quot;)+  
            ggtitle(&amp;quot;Total number of Claps they got&amp;quot;)+
            coord_flip()+geom_text(aes(label=claps),hjust =0.5 )

# plotting two plots at same grid
grid.arrange(g1_Top5_p,g2_Top5_p,ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/Top%205%20publications%20in%20a%20table-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-cloud-for-the-titles-from-top-10-authors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Word Cloud for the Titles from Top 10 Authors&lt;/h1&gt;
&lt;p&gt;Word cloud from the titles of the posts by Top 10 authors of most number of posts is below. The words thing, read, data, drone and new are with highest mentions. Where words such as big, telecom and tech are next in the line with higher amount of posts. In restrictions I have considered that this word cloud will have 1500 words and only if a word atleast holds the frequency of 10.&lt;/p&gt;
&lt;p&gt;Well, I could clearly see that there cannot be 1500 words here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#convert into data frame
Top10_author&amp;lt;-data.frame(Top10_author)

# Calculate Corpus
Top10_author.Corpus&amp;lt;-Corpus(VectorSource(Top10_author$title))

# clean the data
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Corpus,PlainTextDocument)
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Corpus,tolower)
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Clean,removeNumbers)
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Clean,removeWords,stopwords(&amp;quot;english&amp;quot;))
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Clean,removePunctuation)
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Clean,stripWhitespace)
Top10_author.Clean&amp;lt;-tm_map(Top10_author.Clean,stemDocument)

# save as png
#png(filename = &amp;quot;wordcloud1.png&amp;quot;,width = 1024,height = 768)
# plot the word cloud
wordcloud(Top10_author.Clean,max.words = 1500,min.freq = 10,
          colors = brewer.pal(11,&amp;quot;Spectral&amp;quot;),random.color = FALSE,
          random.order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/word%20cloud%20for%20Top%2010%20Authors-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-cloud-for-the-titles-from-top-5-publications&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Word Cloud for the Titles from Top 5 publications&lt;/h1&gt;
&lt;p&gt;This word cloud also has similar restrictions for number of words and minimum frequency for a word. Words such as data, learn, use, machin, network, deep, scienc and artifici have most amount of frequency. Further, words such as neural, intellig, chatbot, part and python are also with significant amount of frequency. Here we can see clearly see there can be more than 1000 words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#convert into data frame
Top5_pub&amp;lt;-data.frame(Top5_pub)

# Calculate Corpus
Top5_pub.Corpus&amp;lt;-Corpus(VectorSource(Top5_pub$title))

#clean the data
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Corpus,PlainTextDocument)
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Corpus,tolower)
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Clean,removeNumbers)
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Clean,removeWords,stopwords(&amp;quot;english&amp;quot;))
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Clean,removePunctuation)
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Clean,stripWhitespace)
Top5_pub.Clean&amp;lt;-tm_map(Top5_pub.Clean,stemDocument)

# save as png
#png(filename = &amp;quot;wordcloud2.png&amp;quot;,width = 1024,height = 768)
# plot the word cloud
wordcloud(Top5_pub.Clean,max.words = 1500,min.freq = 10,
          colors = brewer.pal(11,&amp;quot;Spectral&amp;quot;),random.color = FALSE,
          random.order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_36/2018-12-08-week-36-medium-posts_files/figure-html/word%20cloud%20for%20Top%205%20publications-1.png&#34; width=&#34;552&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and tables in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using dplyr to manipulate the datset was useful when there is complication.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;grid and gridExtra packages provide a safe way of combining multiple plots into one plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;formattable and kableExtra were crucial in generating tables which are informative.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word cloud or analyzing text is very useful and flexible when we use the above packages.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Similarly we can do the above analysis for Top 5 publications and other variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word clouds for subtitles also will be interesting to see, specially focusing on authors and publications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Please see that&lt;/em&gt;&lt;br /&gt;
This is my sixth post on the internet so my mistakes in grammar and spellings should be very less than previous posts. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking optimization functions in R</title>
      <link>/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-the-shape-parameters-by-maximizing-the-log-likelihood-value-of-beta-binomial-distribution.&#34;&gt;Estimating the shape parameters by Maximizing the Log Likelihood value of Beta-Binomial Distribution.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#optim-function&#34;&gt;optim Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nlm-function&#34;&gt;nlm Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nlminb-function&#34;&gt;nlminb Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ucminf-function&#34;&gt;ucminf Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maxlik-function&#34;&gt;maxLik Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mle-function&#34;&gt;mle Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mle2-function&#34;&gt;mle2 Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-the-seven-optimization-functions-in-r&#34;&gt;Summary of the seven optimization functions in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-time-evaluation-for-the-seven-optimization-r-functions&#34;&gt;Summary of Time evaluation for the seven optimization R functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-results-after-estimating-parameters-using-the-optimization-r-functions&#34;&gt;Summary of Results after estimating parameters using the optimization R functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-conclusion&#34;&gt;Final Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;estimating-the-shape-parameters-by-maximizing-the-log-likelihood-value-of-beta-binomial-distribution.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating the shape parameters by Maximizing the Log Likelihood value of Beta-Binomial Distribution.&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When we need to estimate parameters from a discrete distribution or continuous distribution or a function we can use the below mentioned R functions. We will be using the technique of maximizing the Log Likelihood function or minimizing the Negative Log Likelihood function. Based on this technique we will compare these R functions because it might benefit people who are struggling which one to choose. We have 7 functions in total by my knowledge when I was writing this blog post.&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt; package, I will use the Alcohol Consumption data to try and model it for the Beta-Binomial Distribution, which has two shape parameters to estimate. The process is that we find values for shape parameters a and b (or &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) which will maximize the Log Likelihood function of Beta Binomial Distribution or in our case minimize the Negative Log Likelihood function of Beta Binomial Distribution.&lt;/p&gt;
&lt;p&gt;Above mentioned Beta-Binomial distribution’s Probability Mass Function(PMF) is denoted as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{BetaBin}(x)= {n \choose x} \frac{B(\alpha+x,n+\beta-x)}{B(\alpha,\beta)} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n=0,1,2,...\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x=0,1,2,...,n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta &amp;gt; 0\)&lt;/span&gt;. Further &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the Binomial Random variable, a,b(or &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) are shape parameters and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the binomial trial value. Also B(&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) is the Beta function. In this distribution we have to estimate the values for a and b.&lt;/p&gt;
&lt;p&gt;Further, using the PMF we can construct the Likelihood function for &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{BB}=(\alpha,\beta)^T\)&lt;/span&gt; as given below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\Omega_{BB}|x)=\prod_{i=1}^{N} \binom{n}{x_i} \frac{B(\alpha+x_i,n+\beta-x_i)}{B(\alpha,\beta)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where N is the Number of observations. Then Negative Log Likelihood function is given as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[l(\Omega_{BB}|x)=-\sum_{i=1}^{N} log\binom{n}{x_i} + \sum_{i=1}^{N} log(B(\alpha+x_i,n+\beta-x_i)) - Nlog(B(\alpha,\beta))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the package fitODBOD we have the function EstMLEBetaBin which is constructed based on the above Negative Log Likelihood function, and we will use it.&lt;/p&gt;
&lt;p&gt;We take Log to transform the Likelihood function values, which will simplify the computation process and save time. The optimization functions we need to compare will use specific mathematical methods to find the most appropriate shape parameter values.&lt;/p&gt;
&lt;p&gt;The functions in question are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/optim&#34;&gt;optim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/nlm&#34;&gt;nlm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/nlminb&#34;&gt;nlminb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/ucminf/versions/1.1-4/topics/ucminf&#34;&gt;ucminf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/maxLik/versions/1.3-4/topics/maxLik&#34;&gt;maxLik&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle&#34;&gt;mle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2&#34;&gt;mle2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Further, I will focus on the attributes of the above functions. Focusing from which package, Number of Inputs, Number of Outputs, Time to complete optimization and Analytical Methods used with the assistance in two tables. Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fitODBOD)
kable(Alcohol_data,&amp;quot;html&amp;quot;,align=c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 14,full_width = F) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Days
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
49
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;optim-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;optim Function&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html#O&#34;&gt;optim&lt;/a&gt; is the first function in concern. &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/optim&#34;&gt;Documentation&lt;/a&gt; of the optim function is useful and it indicates that this function is only used on one input situations only. This means our EstMLEBetaBin function has to be modified. Reason for this is only the parameters that should be estimated need to be input values but our EstMLEBetaBin function has four parameters which are a,b,x(Binomial Random Variable) and freq(corresponding frequency values).&lt;/p&gt;
&lt;p&gt;While using optim function first index refers to shape parameter a and second index refers to shape parameter b. Further, we have to input the observations or in our case the Binomial random variable values and their respective frequencies. I think it is inconvenient to modify the EstMLEBetaBin function, because if we want to estimate parameters for different data-sets it would become tedious. After modification we have a new function foroptim which can be used for demonstration and comparison.&lt;/p&gt;
&lt;p&gt;Below is the code to estimation and going through the outputs. It should be noted that we have to provide initial parameter values as an input to the optim function, and it is best to provide values in the domain of&lt;br /&gt;
shape parameter values which we want to estimate.&lt;/p&gt;
&lt;p&gt;Here the shape parameters a and b are in the region of greater than zero but less than positive infinity(&lt;span class=&#34;math inline&#34;&gt;\(+\infty &amp;gt;a,b&amp;gt;0\)&lt;/span&gt;). So for the initial parameters of a=0.1 and b=0.2 we are finding parameters which would minimize the Negative Log Likelihood function of Beta-Binomial distribution with the Alcohol Consumption data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new function to facilitate optim criteria
# only one input but has two elements
foroptim&amp;lt;-function(a)
  {
  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])
  }

# optimizing values for a,b using default mathematical method
optim_answer&amp;lt;-optim(par=c(0.1,0.2),fn=foroptim)

# obtaining class of output
class(optim_answer)

#length of output
length(optim_answer)

# the outputs
optim_answer$par # estimated values for a, b
optim_answer$value # minimized function value 
optim_answer$counts  # see the documentation to understand
optim_answer$convergence # indicates successful completion
optim_answer$message # additional information

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           optim_answer$par[1],optim_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the foroptim function can be used as above and parameters are estimated for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (or a, b) for the Alcohol Consumption data week 1. Further we have scrutinized the function as below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : stats&lt;/li&gt;
&lt;li&gt;No of Inputs: 7&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list&lt;/li&gt;
&lt;li&gt;No of outputs: 5&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 6&lt;/li&gt;
&lt;li&gt;Default Method : Nelder-Mead&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;nlm-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;nlm Function&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html#N&#34;&gt;nlm&lt;/a&gt; function is also similar to optim but only one analytical method will be used, which is a Newton-type Algorithm. Here also there needs to be changes made to our EstMLEBetaBin function as previously. After making those changes we have called the new Negative Log-likelihood function of Beta-Binomial distribution as fornlm. Then we can use the nlm function and estimate a and b for the initial shape parameter values of 0.1 and 0.2 respectively. &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/nlm&#34;&gt;Documentation&lt;/a&gt; of nlm function is very useful so that we can understand how it works.&lt;/p&gt;
&lt;p&gt;Below is the code for using nlm function appropriately and fiddling with the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#new function to facilitate nlm criteria
fornlm&amp;lt;-function(a)
  {
  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])
  }

#optimizing values for a,b using the only analytical method
nlm_answer&amp;lt;-nlm(f=fornlm,p=c(0.1,0.2))

#obtaining class of output
class(nlm_answer)

#length of output
length(nlm_answer)

# the outputs
nlm_answer$estimate # estimated values for a, b
nlm_answer$minimum # minimized function value 
nlm_answer$gradient  # gradient at the estimated minimum of given funciton
nlm_answer$code # indicates successful completion
nlm_answer$iterations # number of iterations performed

# fitting the Beta-Binomial distribution with estimated shape parameter  values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           nlm_answer$estimate[1],nlm_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly for the fornlm function we have estimated values for a and b which would fit the Alcohol consumption data of week 1. Below is a point form summary of nlm function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : stats&lt;/li&gt;
&lt;li&gt;No of Inputs: 12&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list&lt;/li&gt;
&lt;li&gt;No of outputs: 5&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 1&lt;/li&gt;
&lt;li&gt;Default Method : Newton-type Algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;nlminb-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;nlminb Function&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html#N&#34;&gt;nlminb&lt;/a&gt; is also similar and requires the EstMLEBetaBin function to be restructured as similar to previous situations. After this task we now have a function called fornlminb. nlminb function is based on analytical method of unconstrained and box-constrained optimization using PORT routines.&lt;/p&gt;
&lt;p&gt;After choosing initial parameter values for a and b, which are respectively 0.1 and 0.2 the estimation was done following the process below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new function to facilitate nlminb criteria
fornlminb&amp;lt;-function(a)
  {
  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])
}

# optimizing values for a,b using default analytical method
nlminb_answer&amp;lt;-nlminb(start=c(0.1,0.2), objective=fornlminb)

# obtaining class of output
class(nlminb_answer)

# length of output
length(nlminb_answer)

# the outputs
nlminb_answer$par # estimated values for a, b
nlminb_answer$objective # minimized function value 
nlminb_answer$evaluations  # see the documentation to understand
nlminb_answer$convergence # indicates successful completion
nlminb_answer$message # additional information
nlminb_answer$iterations # number of iterations performed

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           nlminb_answer$par[1],nlminb_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/nlminb&#34;&gt;Documentation&lt;/a&gt; includes the information related to the function therefore referring it will be useful. After estimating values for a and b using nlminb function these were noticed regarding the function in concern&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : stats&lt;/li&gt;
&lt;li&gt;No of Inputs: 8&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list&lt;/li&gt;
&lt;li&gt;No of outputs: 6&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 1&lt;/li&gt;
&lt;li&gt;Default Method : Unconstrained and box-constrained optimization using PORT routines&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ucminf-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ucminf Function&lt;/h2&gt;
&lt;p&gt;Package &lt;a href=&#34;https://cran.r-project.org/package=ucminf&#34;&gt;uncminf&lt;/a&gt; produces the ucminf function, as previously mentioned functions here also we have to change the EstMLEBetaBin function. After making the changes we will be using the forucminf function to the estimation process of shape parameters a and b.&lt;/p&gt;
&lt;p&gt;When initial parameter values are set to a=0.1 and b=0.2 we will obtain results from ucminf function, which will minimize the Negative Log-likelihood value of Beta-Binomial distribution. Below is the code for estimation and using the results to understand the function ucminf.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ucminf)

# new function to facilitate ucminf criteria
forucminf&amp;lt;-function(a)
  {
  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])
}

# optimizing values for a,b using default analytical method
ucminf_answer&amp;lt;-ucminf(par=c(0.1,0.2),fn=forucminf)

#obtaining class
class(ucminf_answer)

# length of output
length(ucminf_answer)

# the outputs
ucminf_answer$par # estimated values for a, b
ucminf_answer$value # minimized function value 
ucminf_answer$invhessian.lt  # see the documentation understand
ucminf_answer$convergence # indicates successful completion
ucminf_answer$message # additional information
ucminf_answer$info

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           ucminf_answer$par[1],ucminf_answer$par[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the help of R &lt;a href=&#34;https://www.rdocumentation.org/packages/ucminf/versions/1.1-4/topics/ucminf&#34;&gt;Documentation&lt;/a&gt; the estimation was done for shape parameter values a and b using the ucminf function. Below is the initial understanding of the ucminf function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : ucminf&lt;/li&gt;
&lt;li&gt;No of Inputs: 5&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list&lt;/li&gt;
&lt;li&gt;No of outputs: 6&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 1&lt;/li&gt;
&lt;li&gt;Default Method : Quasi-Newton Algorithm type with BFGS updating&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;maxlik-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;maxLik Function&lt;/h2&gt;
&lt;p&gt;maxLik function is from the &lt;a href=&#34;https://cran.r-project.org/package=maxLik&#34;&gt;maxLik&lt;/a&gt; package, which only maximizes the Log Likelihood function. Therefore we have to restructure EstMLEBetaBin as previously mentioned, but as an addition a negative sign is added for the output. This new function will be called as formaxLik.&lt;/p&gt;
&lt;p&gt;For the initial parameter values where a=0.1 and b=0.2 the maxLik function will be used and results will be evaluated as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(maxLik)

# new function to facilitate maxLik criteria
formaxLik&amp;lt;-function(a)
  {
  -EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])
  }

# optimizing values for a,b using default analytical method
maxLik_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2))

# obtaining class of output
class(maxLik_answer)

# length of output
length(maxLik_answer)

# the outputs
maxLik_answer$estimate # estimated values for a, b
maxLik_answer$maximum # minimized function value 
maxLik_answer$iterations  # no of iterations to succeed
maxLik_answer$gradient # last gradient value which was calculated
maxLik_answer$message # additional information
maxLik_answer$hessian # hessian matrix
maxLik_answer$code # indicates successful completion
maxLik_answer$fixed # logical vector indicating which parameters are constants
maxLik_answer$type # type of maximization
maxLik_answer$last.step # list describing the last unsuccessful step
maxLik_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           maxLik_answer$estimate[1],maxLik_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://www.rdocumentation.org/packages/maxLik/versions/1.3-4/topics/maxLik&#34;&gt;Documentation&lt;/a&gt; of maxLik and the &lt;a href=&#34;https://www.rdocumentation.org/packages/maxLik/versions/1.3-4/topics/maxNR&#34;&gt;Documentation&lt;/a&gt; of maxNR function the above analysis was done for the maxLik function. According to the above code, below are the findings from the maxLik function in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : maxLik&lt;/li&gt;
&lt;li&gt;No of Inputs: 6&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list or class of maxim or class of maxLik&lt;/li&gt;
&lt;li&gt;No of outputs: 11&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 7&lt;/li&gt;
&lt;li&gt;Default Method : Automatically chosen&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;mle-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle Function&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats4/html/00Index.html&#34;&gt;stats4&lt;/a&gt; package is installed so that mle function can be operated for the purpose of estimating a and b parameters. Here also as previously we need to make some changes as below and create a new Negative Log Likelihood function called formle.&lt;/p&gt;
&lt;p&gt;For the initial parameter values of a=0.1 and b=0.2 the Negative Log Likelihood value of Beta-Binomial distribution has been minimized using mle function. Below is the code for estimation and investigation from the outputs of mle after estimation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)

# new function to facilitate mle criteria
formle&amp;lt;-function(a,b)
  {
  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)
  }

# optimizing values for a,b using default analytical method
mle_answer&amp;lt;-mle(minuslogl=formle,start = list(a=0.1,b=0.2))

# obtaining class
class(mle_answer)

# length of output
length(mle_answer)

# the outputs
mle_answer@call # inputs i have used 
mle_answer@coef # estimated values for a,b
mle_answer@fullcoef # all values, even the fixed values we did not want to estimate
mle_answer@vcov # variance covariance matrix for a,b
mle_answer@min # minimized function value
mle_answer@details # details after estimation process
mle_answer@nobs # number of observations to be used for computing only if given 
mle_answer@method # optimization methods used

# Methods used
confint(mle_answer) # confidence intervals for estimated values
logLik(mle_answer) # Negative loglikelihood value for estimated values 
profile(mle_answer) # Likelihood profile generation.
nobs(mle_answer) # number of observations to be used for computing only if given
show(mle_answer) # display object briefly
summary(mle_answer) # generate a summary
#update()   # updating if we have new data and need to estimate new values
vcov(mle_answer) # variance covariance matrix

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           mle_answer@coef[1],mle_answer@coef[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should be noted that in all 5 previous functions we had only outputs in the form of lists, but through mle we are seeing a class of mle output.The &lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle&#34;&gt;Documention&lt;/a&gt; explains the inputs and outputs of mle. &lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle-class&#34;&gt;Documentation&lt;/a&gt; for mle-class explains further about the methods that can be used. Below is a list of findings based on the outputs of the mle function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : stats4&lt;/li&gt;
&lt;li&gt;No of Inputs: 5&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : class of mle&lt;/li&gt;
&lt;li&gt;No of outputs: 9&lt;/li&gt;
&lt;li&gt;Methods for output: 8&lt;/li&gt;
&lt;li&gt;No of Analytical Methods :6&lt;/li&gt;
&lt;li&gt;Default Method : Nelder and Mead&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;mle2-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle2 Function&lt;/h2&gt;
&lt;p&gt;mle2 function is advanced than mle function and it is from the package &lt;a href=&#34;https://cran.r-project.org/package=bbmle&#34;&gt;bbmle&lt;/a&gt;. Here, there is no need to modify the EstMLEBetabin function from the fitODBOD package to estimation as all previous situations .&lt;/p&gt;
&lt;p&gt;For the initial parameter values of a=0.1 and b=0.2 Negative Log Likelihood function of Beta-Binomial distribution will be minimized where outputs will be investigated and methods related to output of class mle2 will be used.&lt;/p&gt;
&lt;p&gt;Below is the code for using mle2 function and scrutinizing the output and methods related to it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bbmle)

# optimizing values for a,b using default analytical method
mle2_answer&amp;lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),
                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))

# obtaining class
class(mle2_answer)

# length of output
length(mle2_answer)

# the outputs
mle2_answer@call # inputs generally considered 
mle2_answer@call.orig # inputs i have given
mle2_answer@coef # estimated values for a,b
mle2_answer@fullcoef # all values, even the fixed values we did not want to estimate
mle2_answer@vcov # variance covariance matrix for a,b
mle2_answer@min # minimized function value
mle2_answer@details # details after estimation process
mle2_answer@method # optimization methods used
mle2_answer@data # data used for estimation 
mle2_answer@formula # if a formula was specified in the input 
mle2_answer@optimizer # function used for optimizing

# Methods used
coef(mle2_answer) # extrat the estimated values
confint(mle2_answer) # confidence intervals for estimated values
show(mle2_answer) # display object briefly
summary(mle2_answer) # generate a summary
#update()   #updating if we have new data and need to estimate new values
vcov(mle2_answer) # variance covariance matrix
#formula(mle2_answer) # if a formula was specified in the input 
#plot(mle2_answer) # plot the profile
logLik(mle2_answer) # Negative loglikelihood value for estimated values 
profile(mle2_answer) # profile of estimated values

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           mle2_answer@coef[1],mle2_answer@coef[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2&#34;&gt;Documention&lt;/a&gt; of mle2 function and &lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2-class&#34;&gt;Documentation&lt;/a&gt; of mle2-class&lt;br /&gt;
provide a few findings as mentioned below in point form.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : bbmle&lt;/li&gt;
&lt;li&gt;No of Inputs: 22&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 3&lt;/li&gt;
&lt;li&gt;Class of output : class of mle2&lt;/li&gt;
&lt;li&gt;No of outputs: 12&lt;/li&gt;
&lt;li&gt;Methods for output: 9&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 6&lt;/li&gt;
&lt;li&gt;Default Method : Nelder and Mead&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-the-seven-optimization-functions-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of the seven optimization functions in R&lt;/h1&gt;
&lt;p&gt;After completing a brief fact finding of the R functions optim, nlm, nlminb, ucminf, maxLik, mle and mle2. We can record the facts and results in two tables. Using them we can choose the best suitable function for our needs of estimation from optimization.&lt;/p&gt;
&lt;p&gt;Mostly I prefer mle2 function than others because it provides special methods to handle the mle2 outputs, and the output themselves are very thorough than other R function outputs.&lt;/p&gt;
&lt;p&gt;It can be seen that estimated a and b shape parameter values are different only from the third decimal point in all outputs. But this does not effect the minimized Negative Log Likelihood value of Beta-Binomial distribution for our Alcohol Consumption data, which is same for all outputs.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 12px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Function
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
optim
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
nlm
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
nlminb
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
ucminf
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
maxLik
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
mle
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
mle2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Package
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
stats
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
stats
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
stats
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ucminf
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
maxLik
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
stats4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
bbmle
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
No of Inputs
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Minimum No of Inputs
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Analytical Methods
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Output Type
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
List
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
List
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
List
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
List
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
List class maxLik class maxim
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
class mle
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
class mle2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
No of Outputs
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
No of Methods
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
None
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Initial value(a,b)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.1 b=0.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Estimated value(a,b)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7230707 b=0.5809894
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229384 b=0.5808448
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229404 b=0.5808469
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229390 b=0.5808458
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229428 b=0.5808488
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7228930 b=0.5807279
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7228930 b=0.5807279
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Negative Log Likelihood value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
813.4571
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-time-evaluation-for-the-seven-optimization-r-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of Time evaluation for the seven optimization R functions&lt;/h1&gt;
&lt;p&gt;Further at the beginning I have mentioned to evaluate the system process time for the seven functions. In order to do this time comparison it is possible to use the &lt;a href=&#34;https://www.rdocumentation.org/packages/rbenchmark/versions/1.0.0/topics/benchmark&#34;&gt;benchmark&lt;/a&gt; function of &lt;a href=&#34;https://cran.r-project.org/package=rbenchmark&#34;&gt;rbenchmark&lt;/a&gt; package, and below mentioned code chunk provides the output in a table form. It includes the functions and their respective time values. The estimation process of the parameters where each function has been replicated 1000 times to receive a more accurate table of time values.&lt;/p&gt;
&lt;p&gt;The table is in ascending order according to the elapsed time values column. According to this we can see that least time takes to the nlminb function and most time is taken to the mle2 function. These times completely depends on the Negative Log Likelihood function you need to minimize, the data you provided, the number of estimators that needs to be estimated, the complexity of the function and finally your computer’s processing power.&lt;/p&gt;
&lt;p&gt;Therefore based on the needs of your output and the function which needs estimation choose the best function out of the above seven. Because as I said related to the earlier table the estimated values are slightly different but does not make any strong influence on the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rbenchmark)

Results2&amp;lt;-benchmark(
          &amp;quot;optim&amp;quot;={optim(par = c(0.1,0.2), fn = foroptim)},
          &amp;quot;nlm&amp;quot;={nlm(f = fornlm, p = c(0.1,0.2))},
          &amp;quot;nlminb&amp;quot;={nlminb(start = c(0.1,0.2), objective = fornlminb)},
          &amp;quot;ucminf&amp;quot;={ucminf(par = c(0.1,0.2), fn = forucminf)},
          &amp;quot;maxLik&amp;quot;={maxLik(logLik = formaxLik, start = c(0.1,0.2))},
          &amp;quot;mle&amp;quot;={mle(minuslogl = formle, start = list(a=0.1,b=0.2))},
          &amp;quot;mle2&amp;quot;={mle2(minuslogl = EstMLEBetaBin, start = list(a=0.1, b=0.2),
                       data = list(x=Alcohol_data$Days, freq=Alcohol_data$week1))},
          replications = 1000,
          columns = c(&amp;quot;test&amp;quot;,&amp;quot;replications&amp;quot;,&amp;quot;elapsed&amp;quot;,
                      &amp;quot;relative&amp;quot;,&amp;quot;user.self&amp;quot;,&amp;quot;sys.self&amp;quot;),
          order = &amp;#39;elapsed&amp;#39;
          )

kable(Results2,&amp;quot;html&amp;quot;,align = c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(full_width = F,bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 10) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: blue;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
test
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
replications
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
elapsed
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
relative
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
user.self
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
sys.self
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
nlminb
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.64
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
ucminf
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.64
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.151
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7.61
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
nlm
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.398
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
optim
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.533
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
mle
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
22.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.402
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
22.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
maxLik
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
35.52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.349
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
35.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
mle2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.113
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-results-after-estimating-parameters-using-the-optimization-r-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of Results after estimating parameters using the optimization R functions&lt;/h1&gt;
&lt;p&gt;After using the functions optim, nlm, nlminb, ucminf, maxLik, mle and mle2 to estimate the shape parameters a, b we can use the estimated parameters in the function fitBetaBin. Using this function we can find expected frequencies for each of the estimated parameters a,b and compare p-values and over-dispersion and understand if using different optimization functions had any effect on them.&lt;/p&gt;
&lt;p&gt;According to the below table there is no significant changes between the expected frequencies, p-values or the over dispersion values. This is a clear indication that it does not matter what function we use for the optimization. Because the process will occur effectively only efficiency(time) will be affected.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BinomialRandomVariable
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
optim
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
nlm
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
nlminb
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
ucminf
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
maxLik
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
mle
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
mle2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.61
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.99
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.77
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Total No of Observations
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
p-value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Over Dispersion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340165
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340686
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340679
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340683
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.434067
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340992
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340992
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;final-conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Conclusion&lt;/h1&gt;
&lt;p&gt;We had 7 functions to compare but choosing one over the other is completely harmless to the final result of estimation as seen by our tables. The only issue is time, therefore I would recommend choose the best function based on your needs of output and research objective.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 35: Baltimore Bridges</title>
      <link>/post/week_35/week-35-baltimore-bridges/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/week_35/week-35-baltimore-bridges/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bridge-data-and-baltimore&#34;&gt;Bridge Data and Baltimore&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#counties-which-have-bridges-owned-by-state-highway-agency&#34;&gt;Counties which have bridges owned by State Highway Agency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counties-which-have-bridges-owned-by-county-highway-agency&#34;&gt;Counties which have bridges owned by County Highway Agency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counties-which-have-bridges-owned-by-state-toll-authority&#34;&gt;Counties which have bridges owned by State Toll Authority&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-amount-of-bridges-built-based-on-year&#34;&gt;Most amount of bridges Built based on Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-traffic-less-than-or-equal-to-100000-for-counties-with-bridge-condition&#34;&gt;Average Traffic Less than or equal to 100,000 for Counties with Bridge Condition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-traffic-more-than-100000-for-counties-with-bridge-condition&#34;&gt;Average Traffic More than 100,000 for Counties with Bridge Condition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improvement-and-bridge-conditions-with-counties&#34;&gt;Improvement and Bridge Conditions with Counties&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the packages
library(readr)
library(tidyverse)
library(stringr)
library(ggthemr)
library(gganimate)
library(formattable)
# load the theme flat dark
ggthemr(&amp;quot;flat dark&amp;quot;)
# reading the data
bridges &amp;lt;- read_csv(&amp;quot;baltimore_bridges.csv&amp;quot;)
#View(bridges)
# naming the columns
names(bridges)&amp;lt;-c(&amp;quot;lat&amp;quot;,&amp;quot;long&amp;quot;,&amp;quot;County&amp;quot;,&amp;quot;Carries&amp;quot;,&amp;quot;Year Built&amp;quot;,
                  &amp;quot;Condition&amp;quot;,&amp;quot;Average Daily Traffic&amp;quot;,&amp;quot;Total Improvement&amp;quot;,
                  &amp;quot;Month&amp;quot;,&amp;quot;Year&amp;quot;,&amp;quot;Owner&amp;quot;,&amp;quot;Responsibility&amp;quot;,&amp;quot;Vehicles&amp;quot;)
attach(bridges)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;bridge-data-and-baltimore&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bridge Data and Baltimore&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2018-11-27/baltimore_bridges.csv&#34;&gt;Data&lt;/a&gt; for the analysis and &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2018-11-27&#34;&gt;description&lt;/a&gt; about the Baltimore bridges are hyper-linked.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Bridges and Jitter plots. Ownership, Counties, and Condition with Average Daily Traffic. My take of the Bridge Data.  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  The further code here &lt;a href=&#34;https://t.co/L6zjRP8OKA&#34;&gt;https://t.co/L6zjRP8OKA&lt;/a&gt; &lt;a href=&#34;https://t.co/9RrBFSyAPD&#34;&gt;pic.twitter.com/9RrBFSyAPD&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1067650979129118720?ref_src=twsrc%5Etfw&#34;&gt;November 28, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/Week_35&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data on bridges is of week 35 from TidyTuesday. Trying to explain the data using maps is obvious, yet I will use animated jitter plots. There are 13 variables and 2079 observations. Brave choice of limiting my self to less than 10 variables, where latitude, longitude and Vehicles will not be taken into account.&lt;/p&gt;
&lt;p&gt;So with the help of packages tidyverse, ggthemr, gganimate,formattable and readr I will complete this analysis. Most of the bridges are owned by several agencies, but I will only focus on the top three ownership holders.&lt;/p&gt;
&lt;div id=&#34;counties-which-have-bridges-owned-by-state-highway-agency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counties which have bridges owned by State Highway Agency&lt;/h2&gt;
&lt;p&gt;Close to 1000 bridges are owned by State Highway Agency, where most of them are in Baltimore County. High amount of bridges are in good condition, further more bridges are in Fair condition and only around 10 bridges in Poor condition.&lt;/p&gt;
&lt;p&gt;Considering the Average Daily Traffic only one bridge in Poor condition has the amount of close to 110,000, while all the other poor condition bridges have Average Daily Traffic less than 30,000. While counties Anne Arundel and Hartford have no Poor condition bridges at all.&lt;/p&gt;
&lt;p&gt;Most of the bridges are from Baltimore County and around 20 bridges have count of more than 150,000 Average Daily Traffic for both Fair and Good conditions. Hartford and Carroll Counties have their Average Daily Traffic which does not exceed 80,000 at any condition of the bridge.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to State Highway Agency
ggplot(subset(bridges,Owner==&amp;quot;State Highway Agency&amp;quot;)
       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+
  xlab(&amp;quot;County&amp;quot;)+
  ggtitle(&amp;quot;Condition of Bridges owned by State Highway Agency \nand their Average Daily Traffic&amp;quot;)+
  scale_y_continuous(labels =seq(0,230000,10000) ,breaks = seq(0,230000,10000))+
  transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)+geom_jitter()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/County%20with%20Condition%20and%20Average%20Daily%20Traffic%20SHA-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;counties-which-have-bridges-owned-by-county-highway-agency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counties which have bridges owned by County Highway Agency&lt;/h2&gt;
&lt;p&gt;County Highway Agency owns the second most amount of bridges in this data-set. Therefore using jitter plots we are going to check how the condition of the bridges and counties are explained in the simplest manner.&lt;/p&gt;
&lt;p&gt;Less amount of poor condition bridges in all counties except Anne Roundel County. All bridges owned by County Highway Agency have a limited Average Daily Traffic less than 50,000. Clearly we have more Fair bridges than Good ones. In the Poor condition category only two have Average Daily Traffic more than 20,000, while other two have more than 10 bridges.&lt;/p&gt;
&lt;p&gt;Most of these bridges are in Baltimore County even it is in any one of three conditions. There are few bridges which have values more than 40,000 Average Daily Traffic and they are also in Baltimore County.&lt;/p&gt;
&lt;p&gt;There are bridges which have Zero Average Daily Traffic. In all three Conditions only Hartford County has bridges which has Average Daily Traffic less than 10,000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to County Highway Agency
ggplot(subset(bridges,Owner==&amp;quot;County Highway Agency&amp;quot;)
       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+
  xlab(&amp;quot;County&amp;quot;)+ geom_jitter()+
  ggtitle(&amp;quot;Condition of Bridges owned by County Highway Agency \nand their Average Daily Traffic&amp;quot;)+
  scale_y_continuous(labels =seq(0,40000,5000) ,breaks = seq(0,40000,5000))+
  transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/County%20with%20Condition%20and%20Average%20Daily%20Traffic%20CHA-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;counties-which-have-bridges-owned-by-state-toll-authority&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counties which have bridges owned by State Toll Authority&lt;/h2&gt;
&lt;p&gt;There is only bridge which is in Poor condition and it is in Baltimore County, while counties Howard and Anne Arundel have no Good condition bridges. Further there is only 3 Fair condition bridges in Howard County while they have values for Average Daily Traffic less than 10,000.&lt;/p&gt;
&lt;p&gt;The highest Average Daily Traffic is close to 170,000 which are only 4 and are in Good and Fair conditions. Further, Anne Arundel County has only one Good bridge and in Hartford it is six bridges. Only few of the bridges have Average Daily Traffic close to zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to State tolll authority
ggplot(subset(bridges,Owner==&amp;quot;State Toll Authority&amp;quot;)
       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+
  xlab(&amp;quot;County&amp;quot;)+geom_jitter()+
  ggtitle(&amp;quot;Condition of Bridges owned by State Toll Authority \nand their Average Daily Traffic&amp;quot;)+
  scale_y_continuous(labels =seq(0,170000,10000) ,breaks = seq(0,170000,10000))+
  transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/County%20with%20Condition%20and%20Average%20Daily%20Traffic%20STA-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-amount-of-bridges-built-based-on-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most amount of bridges Built based on Year&lt;/h2&gt;
&lt;p&gt;Years 1957, 1970, 1975, 1991, 1963 and 1961 have the top 6 spots for building more than 50 bridges in those years. If we consider the conditions of Fair and Good only the year 1991 is suitable to be mentioned, while all other years has at-least one Poor condition bridge. Further There are more Poor condition bridges in 1961 than in 1957. While all Poor condition bridges has Average Daily Traffic less than 50,000.&lt;/p&gt;
&lt;p&gt;Finally, there are only few bridges which have Average Daily Traffic above 100,000 and only 3 are in Good condition. There are Bridges which can have Average Daily Traffic close to zero in all 6 years and all conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to years based on Most amount of bridges built
ggplot(subset(bridges,`Year Built`==&amp;quot;1957&amp;quot; | `Year Built`==&amp;quot;1970&amp;quot; | 
                `Year Built`==&amp;quot;1975&amp;quot; | `Year Built`==&amp;quot;1991&amp;quot; |
                `Year Built`==&amp;quot;1963&amp;quot; | `Year Built`==&amp;quot;1961&amp;quot;)
       ,aes(color=Condition,y=`Average Daily Traffic`,x=factor(`Year Built`)))+
  xlab(&amp;quot;Year Built&amp;quot;)+ylab(&amp;quot;Average Daily Traffic&amp;quot;)+
  ggtitle(&amp;quot;Most amount of Bridges built based on Years \nand their Conditions&amp;quot;)+
  geom_jitter()+legend_bottom()+
  transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/Year%20Built%20with%20Condition%20and%20Average%20Daily%20Traffic-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-traffic-less-than-or-equal-to-100000-for-counties-with-bridge-condition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Average Traffic Less than or equal to 100,000 for Counties with Bridge Condition&lt;/h2&gt;
&lt;p&gt;While obtaining summary for county variable there is one issue because there are two observations which say “Baltimore city” than “Baltimore City” and I don’t want to change them.&lt;/p&gt;
&lt;p&gt;If we focus on Average Daily Traffic less than or equal to 100,000 based on County and Condition. It is clear that Poor condition bridges are part of this criteria and mostly Average Daily Traffic is less than 5000 for Counties Howard, Hartford and Carroll. While Baltimore County has highest amount up-to 75,000, but Baltimore County has highest amount close to 40,000 for Average Daily Traffic. Finally Anne Arundel County has only one Poor condition bridge which has Average Daily Traffic close to zero.&lt;/p&gt;
&lt;p&gt;We can see that there are more Fair Condition bridges than Good ones. In Baltimore County most of the Fair condition bridges have Average Daily Traffic less than 15000. Similarly Carroll county and Hartford county also behave under such criteria. But for Good condition bridges this is not the case where there is no certain strong dense region as similar to Fair condition bridges.&lt;/p&gt;
&lt;p&gt;Previously when we looked into county we did not see Baltimore City often as a factor, but here that is not the case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to average daily Traffic less than or equal 1000000
ggplot(subset(bridges,`Average Daily Traffic`&amp;lt;=100000 &amp;amp; County!=&amp;quot;Baltimore city&amp;quot;),
       aes(x=County,y=`Average Daily Traffic`,color=Condition))+
  xlab(&amp;quot;County&amp;quot;)+ylab(&amp;quot;Averag Daily Traffic&amp;quot;)+
  ggtitle(&amp;quot;Average Daily Traffic Less than 100,000 \nFor Counties&amp;quot;)+
  scale_y_continuous(labels = seq(0,100000,5000),breaks = seq(0,100000,5000))+
  theme(axis.text.x = element_text(angle = -90))+coord_flip()+ geom_jitter()+
  transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/Average%20Traffic%20less%20than%20100000-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-traffic-more-than-100000-for-counties-with-bridge-condition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Average Traffic More than 100,000 for Counties with Bridge Condition&lt;/h2&gt;
&lt;p&gt;This Jitter plot is completely different than previous one, because there are no clear dense regions for any counties and conditions of the bridge. There is only one Poor condition bridge in Baltimore County where the Average Daily Traffic is close to 115,000. In Fair condition bridges also Baltimore County holds the most, while they are slightly dense in the region of 175,000 to 190,000. while for Howard County similar density occurs between 190,000 to 205,000. Bridges in Good condition have more higher values in Baltimore County than Anne Arundel County.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# jitter plot to average daily Traffic more than  1000000
ggplot(subset(bridges, `Average Daily Traffic` &amp;gt; 100000 &amp;amp; County != &amp;quot;Baltimore city&amp;quot;),
       aes(x=County,y=`Average Daily Traffic`,color=Condition))+
  xlab(&amp;quot;County&amp;quot;)+ylab(&amp;quot;Average Daily Traffic&amp;quot;)+
  ggtitle(&amp;quot;Average Daily Traffic More than 100,000 \nFor Counties&amp;quot;)+
  scale_y_continuous(labels=seq(100000,230000,5000),breaks=seq(100000,230000,5000))+
  coord_flip()+ theme(axis.text.x = element_text(angle = -90))+
  geom_jitter()+transition_states(Condition,transition_length = 2,state_length = 3)+
  enter_fade()+exit_shrink()+ease_aes(&amp;quot;back-in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_35/2018-11-28-week-35-baltimore-bridges_files/figure-html/Average%20Traffic%20more%20than%20100000-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improvement-and-bridge-conditions-with-counties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improvement and Bridge Conditions with Counties&lt;/h2&gt;
&lt;p&gt;In the variable of Total Improvement there are 1438 missing values, 42 values are zero and the rest are actual values. I am going to look at Total Improvement in two tables. First table will include where Bridges have Total Improvement higher than 9,999,000 and less than 30,000,000. Second table is for Bridges which have Total Improvement higher than or equal to 30,000,000.&lt;/p&gt;
&lt;p&gt;Further to make these tables interesting I will be using the package formattable, and colors and tiles for numerical values. In the first table there are 7 bridges while only Anne Arundel County holds 3 and Baltimore City holds 4. One bridge is from 1953, and others are from the period of 1977 to 1983. Conditions of these bridges are mostly Fair and two bridges are in Good condition. Lowest Average Daily Traffic is 11760, while highest is 124193, where both bridges are in Fair Condition, and the amount spent on them for Total Improvement are respectively 18,163,000 and 16,264,000. The bridge with Highest amount of Average Daily traffic is built in 1953.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# removing unnecessary columns and setting restriction to 
# Total Improvement
Top10&amp;lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], 
              `Total Improvement` &amp;gt; 9999 &amp;amp; `Total Improvement` &amp;lt; 30000)

# setting colours
customRed0 = &amp;quot;#FF8080&amp;quot;
customRed = &amp;quot;#7F0000&amp;quot;

customyellow0 = &amp;quot;#FFFF80&amp;quot;
customyellow = &amp;quot;#BFBF00&amp;quot;

customblue0 = &amp;quot;#6060BF&amp;quot;
customblue =  &amp;quot;#00007F&amp;quot;

# creating the table for above data set
formattable(Top10,align=c(&amp;quot;l&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;),
            list(
              County =formatter(&amp;quot;span&amp;quot;,style= ~style(color=&amp;quot;grey&amp;quot;)),
            `Total Improvement`=color_tile(customblue0,customblue),
            `Average Daily Traffic`=color_tile(customyellow0,customyellow),
            `Year Built`=color_tile(customRed0,customRed)
            ))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
County
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Carries
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Year Built
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Condition
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Average Daily Traffic
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Total Improvement
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Anne Arundel County&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MD 2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #7f0000&#34;&gt;1983&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #e7e750&#34;&gt;53221&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #6060bf&#34;&gt;13504&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Anne Arundel County&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
US 50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ff8080&#34;&gt;1953&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #bfbf00&#34;&gt;124193&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #3a3aa5&#34;&gt;16264&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Anne Arundel County&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UPPER LEVEL ROADWA
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #981919&#34;&gt;1977&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ffff80&#34;&gt;11760&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #1f1f94&#34;&gt;18163&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Baltimore City &lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS 95 SB
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #981919&#34;&gt;1977&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #cfcf21&#34;&gt;94765&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #4040aa&#34;&gt;15785&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Baltimore City &lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS 95 VIADUCT SB
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #8b0c0c&#34;&gt;1980&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Good
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #e1e144&#34;&gt;63650&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #3c3ca7&#34;&gt;16051&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Baltimore City &lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS 95 VIADUCT NB
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #8b0c0c&#34;&gt;1980&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #e7e751&#34;&gt;52850&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00007f&#34;&gt;20484&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: grey&#34;&gt;Baltimore City &lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS 95 SB
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #8b0c0c&#34;&gt;1980&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Good
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #e6e64e&#34;&gt;55621&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00007f&#34;&gt;20484&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When I did try to plot the top ten bridges with most Total improvement values there was one issue, which is the distance between first two values and the next 8 values. Therefore I divided the table into two.&lt;/p&gt;
&lt;p&gt;In this second table We can see there are two bridges which are from Baltimore City and are built in 1980 and 1971, but the amount spent on Total Improvement is 300,000,000 each. But their Average Daily Traffic is respectively 56280 and 30600.&lt;/p&gt;
&lt;p&gt;While we have another bridge from Baltimore City and built in 1907, but Total Improvement amount is 35,026,000. Here, the Average Daily Traffic is 3900.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# removing unnecessary columns and setting restriction to 
# Total Improvement
Top3&amp;lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` &amp;gt;= 30000)

# setting colours
customRed0 = &amp;quot;#FF8080&amp;quot;
customRed = &amp;quot;#7F0000&amp;quot;

customyellow0 = &amp;quot;#FFFF80&amp;quot;
customyellow = &amp;quot;#BFBF00&amp;quot;

customblue0 = &amp;quot;#6060BF&amp;quot;
customblue =  &amp;quot;#00007F&amp;quot;

# creating the table for above data set
formattable(Top3,align=c(&amp;quot;l&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;c&amp;quot;),
            list(
              County =formatter(&amp;quot;span&amp;quot;,style= ~style(color=&amp;quot;black&amp;quot;)),
            `Total Improvement`=color_tile(customblue0,customblue),
            `Average Daily Traffic`=color_tile(customyellow0,customyellow),
            `Year Built`=color_tile(customRed0,customRed)
            ))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
County
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Carries
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Year Built
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Condition
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Average Daily Traffic
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Total Improvement
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: black&#34;&gt;Baltimore City&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
US 40 EDMONDSON AV
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ff8080&#34;&gt;1907&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Poor
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #ffff80&#34;&gt;3900&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #6060bf&#34;&gt;35026&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: black&#34;&gt;Baltimore City&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS 95 VIADUCT SB
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #7f0000&#34;&gt;1980&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #bfbf00&#34;&gt;56280&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00007f&#34;&gt;300000&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;color: black&#34;&gt;Baltimore City&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EASTERN AVENUE
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #8e0f0f&#34;&gt;1971&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fair
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #dede3e&#34;&gt;30600&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;display: block; padding: 0 4px; border-radius: 4px; background-color: #00007f&#34;&gt;300000&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and tables in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Jitter plots and animation are useful in explaining one continuous variable with multiple categorical variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sub-setting the data set and applying formattable package is useful to explain different continuous values with in a table.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Similarly we can use mapping to point out the locations of the bridges and use animation to make it more clear.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Please see that&lt;/em&gt;&lt;br /&gt;
This is my fifth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 34 : Thanksgiving </title>
      <link>/post/week_34/week-34-thanksgiving/</link>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/week_34/week-34-thanksgiving/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#people-who-do-and-do-not-celebrate-thanksgiving&#34;&gt;People who do and do not celebrate Thanksgiving&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#age-distribution&#34;&gt;Age Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#age-with-other-factors&#34;&gt;Age with Other Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gender-distribution&#34;&gt;Gender Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gender-with-other-factors&#34;&gt;Gender with Other Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#family-income-distribution&#34;&gt;Family Income Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#family-income-with-other-factors&#34;&gt;Family Income with Other Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#us-region-distribution&#34;&gt;US Region Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load the packges
library(ggplot2)
library(ggthemr)
library(stringr)
library(gridExtra)
library(tidyverse)
library(tweenr)
library(gganimate)
library(kableExtra)
library(magrittr)
library(knitr)
library(readr)

#load the data
Thanksgiving&amp;lt;-read_csv(&amp;quot;thanksgiving_meals.csv&amp;quot;)

# apply the theme grape
ggthemr(&amp;quot;grape&amp;quot;)

#subset the people who said yes for celebrating thanksgiving
Thanksgiving_Yes&amp;lt;-subset(Thanksgiving,celebrate==&amp;quot;Yes&amp;quot;)

#subset the people who said no for celebrating thanksgiving
Thanksgiving_No&amp;lt;-subset(Thanksgiving,celebrate==&amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data set was provided on week 34 for TidyTuesday analysis. As it is Thanksgiving week this is understandable. You can receive the data set &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2018-11-20/thanksgiving_meals.csv&#34;&gt;here&lt;/a&gt;. There are more than 65 variables and 1058 observations. The data was acquired buy a survey conducted online and information about them are &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2018-11-20&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My take on Thanksgiving, Praying and Celebrating based on US regions from this data set. &lt;a href=&#34;https://twitter.com/hashtag/Tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/dhCJEYWOlO&#34;&gt;https://t.co/dhCJEYWOlO&lt;/a&gt; &lt;a href=&#34;https://t.co/4Kij5PF4oV&#34;&gt;pic.twitter.com/4Kij5PF4oV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1065462771817607168?ref_src=twsrc%5Etfw&#34;&gt;November 22, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/Week%2034%20-%20ThanksGiving&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;people-who-do-and-do-not-celebrate-thanksgiving&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;People who do and do not celebrate Thanksgiving&lt;/h1&gt;
&lt;p&gt;In this ThanksGiving data set 980 are celebrating, and 78 are not celebrating Thanksgiving. I will use plots to understand their composition and tables to explain them further.&lt;/p&gt;
&lt;p&gt;Variables in consideration for this task is none other than Age, Gender, Family Income and US regions. Finally my aim is to create animated plots and interactive tables for the above variables through the help of packages gganimate and kable.&lt;/p&gt;
&lt;div id=&#34;age-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Age Distribution&lt;/h2&gt;
&lt;p&gt;First the age distribution has only 4 groups, where people who celebrate Thanksgiving in the age category of 18-29 is the very least. Highest count goes to the age category of 45-59 with 269. There are 33 missing observations and they were removed.&lt;/p&gt;
&lt;p&gt;Considering the people who do not celebrate Thanksgiving the least count of 6 goes to category of 60+, but here the category of 18-29 has the highest count of 31. No missing observations were recorded here.&lt;/p&gt;
&lt;p&gt;Below is an animated bar plot where the counts change for their respective 4 categories. As 90% of respondents have answered Yes for celebrating Thanksgiving and rest have answered No we can clearly see the count differences&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)
attach(Thanksgiving_No)
# people who do not celebrate 
dont_age&amp;lt;-as.data.frame(summary.factor(Thanksgiving_No$age))
# people who do celebrate
do_age&amp;lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$age)))
# people who do celebrate
data_do_age&amp;lt;-data.frame(group=c(&amp;quot;18-29&amp;quot;,&amp;quot;30-44&amp;quot;,&amp;quot;45-59&amp;quot;,&amp;quot;60+&amp;quot;),
                      values=do_age$`summary.factor(na.omit(Thanksgiving_Yes$age))`,
                        frame=rep(&amp;quot;Do Celebrate&amp;quot;,4))
# people who do not celebrate
data_dont_age&amp;lt;-data.frame(group=c(&amp;quot;18-29&amp;quot;,&amp;quot;30-44&amp;quot;,&amp;quot;45-59&amp;quot;,&amp;quot;60+&amp;quot;),
                          values=dont_age$`summary.factor(Thanksgiving_No$age)`,
                          frame=rep(&amp;quot;Do not Celebrate&amp;quot;,4))
# combining both
data_age&amp;lt;-rbind(data_do_age,data_dont_age)

# animated bar plot for people who do celebrate and who do not celebrate 
ggplot(data_age,aes(x=factor(group),values))+
  geom_bar(stat = &amp;#39;identity&amp;#39;,position = &amp;quot;identity&amp;quot;)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Age Group&amp;quot;)+
  ggtitle(&amp;quot;Animated plot how Do and Do not people prefer \naccording to Age&amp;quot;)+
  geom_text(aes(label=values), vjust=1)+
  transition_states(frame,transition_length = 2,state_length = 3)+
  enter_fade()+
  exit_shrink()+
  ease_aes(&amp;#39;cubic-in-out&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_34/2018-11-26-week-34-thanksgiving_files/figure-html/Age%20with%20do%20people%20and%20do%20not%20people-1.gif&#34; width=&#34;75%&#34; height=&#34;75%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)
detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;age-with-other-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Age with Other Factors&lt;/h2&gt;
&lt;p&gt;First table is Age vs Gender for people who celebrate Thanksgiving. All age categories have a percentage range in between 19 and 29. Highest percentage of 28.4055 is for Age category 45 - 59. Female have a higher percentage of 54.3823.&lt;/p&gt;
&lt;p&gt;Female who are 60+ have the highest percentage of 15.2059, while lowest percentage of 8.7645 is for male in the age category of 18-29.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(age,gender))) %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(4,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(age,gender)),6)*100),&amp;quot;html&amp;quot;) %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
  column_spec(4,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Female
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Male
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.7709
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.7645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
19.5354
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.4108
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.4044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
24.8152
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.9947
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.4108
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
28.4055
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15.2059
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.0380
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
27.2439
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
54.3823
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
45.6177
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When considering the people who do not celebrate Thanksgiving, highest percentage of 62.8205 is for Male, while age category of 18-29 have the highest percentage of 39.7436.&lt;/p&gt;
&lt;p&gt;Male who are in between 18 and 29 have the highest percentage of 25.6410, while Female who are above 60 have the lowest percentage of 2.5641.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No)

#kable(addmargins(table(age,gender))) %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(4,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(age,gender)),6)*100),&amp;quot;html&amp;quot;) %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
  column_spec(4,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Female
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Male
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25.6410
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
39.7436
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.5385
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.2308
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
30.7693
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.9744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.8205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
21.7949
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.6923
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
37.1796
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
62.8205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0001
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With relative to people who celebrate Thanksgiving in the Family Income category highest percentage goes to USD 25,000 to 49,999.&lt;/p&gt;
&lt;p&gt;People who have Family Income USD 25,000 to 49,999 and age above 60 have the highest percentage of 4.96, while lowest percentage of 0.11 is for people who have Family Income in between USD 175,000 to 199,999 of age category of 18-29.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(age,family_income)),&amp;quot;html&amp;quot;) %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(age,family_income)),6)*100),&amp;quot;html&amp;quot;) %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = T,font_size = 9) %&amp;gt;%
  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(0,bold = T,angle = 270,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 9px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$0 to $9,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$10,000 to $24,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$100,000 to $124,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$125,000 to $149,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$150,000 to $174,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$175,000 to $199,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$200,000 and up
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$25,000 to $49,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$50,000 to $74,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$75,000 to $99,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Prefer not to answer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.4847
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1119
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9504
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2112
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5280
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6336
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.6959
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1119
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0063
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.6959
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
19.5354
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3728
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5839
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8448
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6336
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3168
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.8574
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.0127
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.2239
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.1679
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
24.8153
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3168
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3728
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.1183
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0063
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1616
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.1679
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.0127
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.1679
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.6959
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.8511
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
28.4056
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3168
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.9071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4784
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8448
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1616
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9567
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.9630
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.1183
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.4847
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.7455
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
27.2441
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.4911
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.3358
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
11.5101
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.0687
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.0127
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.7456
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.0254
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.5290
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.4108
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.4108
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
12.4604
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0004
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of people who do not celebrate Thanksgiving the Family Income category has the highest percentage which goes to People who prefer not to answer.&lt;/p&gt;
&lt;p&gt;15 cells in this table are zero which is the lowest percentage that can occur, while highest percentage goes to people who are in the age category 18 -29 while Family Income is USD 0 to 9,999 and prefer not to answer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No) 

#kable(addmargins(table(age,family_income))) %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(age,family_income)),6)*100),&amp;quot;html&amp;quot;) %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = T,font_size = 9) %&amp;gt;%
  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(0,bold = T,angle = 270,align = &amp;#39;center&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 9px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$0 to $9,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$10,000 to $24,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$100,000 to $124,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$125,000 to $149,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$150,000 to $174,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$175,000 to $199,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$200,000 and up
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$25,000 to $49,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$50,000 to $74,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$75,000 to $99,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Prefer not to answer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.8205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.8205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
39.7436
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.9744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
30.7695
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
21.7951
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.6925
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.9487
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
10.2565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.5642
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.5642
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.9488
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
10.2565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.6924
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
23.0770
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0007
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the people who celebrate Thanksgiving highest percentage of 21.80 goes to US region of South Atlantic. While lowest percentage goes to Mountain with 4.41.&lt;/p&gt;
&lt;p&gt;People who are from South Atlantic in the age categories of 45-59 and 60+ have the highest percentage of 6.55. While the lowest percentage of 0.64 goes to people who are in the age category of 18-29 and from East South Central.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(age,us_region))) %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(age,us_region)),4)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = T,font_size = 9) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 9px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
19.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.51
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
24.71
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.69
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
28.79
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
27.49
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.41
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
21.80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
9.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of people who do not celebrate Thanksgiving 23.5294% are from Pacific, while lowest percentage is for people who are from New England and West North Central with 4.4118.&lt;/p&gt;
&lt;p&gt;10 cells have zero values which is the lowest percentage value. While highest percentage of 11.7647 occurs to people from Pacific and in the age category 30-44.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No)

#kable(addmargins(table(age,us_region)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(age,us_region)),6)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = T,font_size = 9) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(5,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;2cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 9px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
18 - 29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8235
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.3529
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
38.2354
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
30 - 44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.7647
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
32.3531
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
45 - 59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
22.0590
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
60+
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.3530
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 2cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.3530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
20.5883
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
23.5294
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
16.1766
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0005
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gender-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gender Distribution&lt;/h2&gt;
&lt;p&gt;We have two types of gender categories in this data set which are male and female. According to the people who celebrate Thanksgiving 515 are Female, while only 432 are male. Here also there are 33 missing observations and they have been removed.&lt;/p&gt;
&lt;p&gt;But this is not the case for those who do not celebrate Thanksgiving. Female have a count of only 29, where males have a count of 49. There were no missing observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)
attach(Thanksgiving_No)
# people who do not celebrate
dont_sex&amp;lt;-as.data.frame(summary.factor(Thanksgiving_No$gender))
# people who do celebrate
do_sex&amp;lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$gender)))
# people who do celebrate
data_do_sex&amp;lt;-data.frame(group=c(&amp;quot;Female&amp;quot;,&amp;quot;Male&amp;quot;),
                  values=do_sex$`summary.factor(na.omit(Thanksgiving_Yes$gender))`,
                  frame=rep(&amp;quot;Do Celebrate&amp;quot;,2))
# people who do not celebrate
data_dont_sex&amp;lt;-data.frame(group=c(&amp;quot;Female&amp;quot;,&amp;quot;Male&amp;quot;),
                          values=dont_sex$`summary.factor(Thanksgiving_No$gender)`,
                          frame=rep(&amp;quot;Do not Celebrate&amp;quot;,2))
# combining both 
data_sex&amp;lt;-rbind(data_do_sex,data_dont_sex)

# animated plot for people who do celebrate and who do not celebrate
ggplot(data_sex,aes(group,values))+
  geom_bar(stat = &amp;#39;identity&amp;#39;,position = &amp;quot;identity&amp;quot;)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Gender&amp;quot;)+
  ggtitle(&amp;quot;Animated plot how Do and Do not people prefer \naccording to Gender&amp;quot;)+
  scale_y_continuous(labels= seq(0,520,10),breaks = seq(0,520,10))+
  geom_text(aes(label=values), vjust=1)+
  transition_states(frame,transition_length = 2,state_length = 3)+
  enter_fade()+
  exit_shrink()+
  ease_aes(&amp;#39;elastic-in-out&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_34/2018-11-26-week-34-thanksgiving_files/figure-html/Gender%20with%20do%20people%20and%20do%20not%20people-1.gif&#34; width=&#34;75%&#34; height=&#34;75%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)
detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gender-with-other-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gender with Other Factors&lt;/h2&gt;
&lt;p&gt;Of people who do celebrate Thanksgiving highest percentage of 10.14 goes to Females where Family Income is USD 25,000 to 49,999. While lowest percentage of 1.27 goes to Males of Family Income category USD 175,000 to 199,999.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(gender,family_income)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(gender,family_income)),4)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = T,font_size = 9) %&amp;gt;%
  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;2cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,angle = 270,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 9px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$0 to $9,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$10,000 to $24,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$100,000 to $124,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$125,000 to $149,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$150,000 to $174,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$175,000 to $199,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$200,000 and up
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$25,000 to $49,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$50,000 to $74,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$75,000 to $99,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Prefer not to answer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
54.39
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2cm; font-weight: bold;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
45.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 2cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
11.51
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.41
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.41
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
12.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3 cells in the below table are zero values, which is the lowest percentage value. Highest percentage of 14.1026 goes to Males who chose not to answer regarding Family Income where they do not celebrate Thanksgiving.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No)

#kable(addmargins(table(gender,family_income)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(gender,family_income)),6)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F,font_size = 10) %&amp;gt;%
  column_spec(13,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;1.5cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,angle = 270,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$0 to $9,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$10,000 to $24,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$100,000 to $124,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$125,000 to $149,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$150,000 to $174,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$175,000 to $199,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$200,000 and up
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$25,000 to $49,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$50,000 to $74,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
$75,000 to $99,999
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Prefer not to answer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;-webkit-transform: rotate(270deg); -moz-transform: rotate(270deg); -ms-transform: rotate(270deg); -o-transform: rotate(270deg); transform: rotate(270deg);&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.4103
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1282
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.9744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
37.1798
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.5385
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.4103
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.8205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.4103
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
62.8208
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 1.5cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.9488
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
10.2565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.5642
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.5641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.2821
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.1283
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.9487
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
10.2565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.6923
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
23.0770
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0006
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Female from South Atlantic who celebrate Thanksgiving have a highest percentage of 12.14. Where respondents from Mountain region and Males have the lowest percentage of 1.29.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(gender,us_region)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(gender,us_region)),4)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F,font_size = 10) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;1.5cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
54.55
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.41
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.69
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.67
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
45.45
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 1.5cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
21.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
9.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Male respondents who do not celebrate Thanksgiving where they are from Middle Atlantic have a highest percentage of 16.1765. Even though Females of West South Central have the lowest percentage of 1.4706 and Males from West North Central also have the same percentage value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No)

#kable(addmargins(table(gender,us_region)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(gender,us_region)),6)*100),&amp;quot;html&amp;quot;)  %&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F,font_size = 10) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(3,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;1.5cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8235
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.3529
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
38.2354
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 1.5cm; font-weight: bold;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16.1765
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.7059
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8235
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.3529
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
61.7648
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 1.5cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.3530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
20.5883
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
23.5294
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
16.1764
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8235
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0002
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;family-income-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Family Income Distribution&lt;/h2&gt;
&lt;p&gt;There are 11 categories when it comes to Family Income. The option of Prefer Not to answer is given and has been chosen by people who celebrate and people who do not celebrate Thanksgiving.&lt;/p&gt;
&lt;p&gt;Considering the people the who celebrate Thanksgiving, highest count of 166 goes to the category of 25,000 to 49,999 USD. While least count goes to 175,000 to 199,999 USD and the count is 26. Further, 118 people have chosen not to answer this question. 33 Missing observations were removed.&lt;/p&gt;
&lt;p&gt;Where as in people who do not celebrate Thanksgiving, second highest count goes to the categories of 0 to 9,999 USD and 25,000 to 49,999 USD, where the count is 14. Similarly, for the least count of 1 also there are two Family Income categories, which are 125,000 to 149,999 USD and 175,000 to 199,999 USD. Prefer not to answer is the choice of 18 respondents who participated in this survey. No missing observations were recorded.&lt;/p&gt;
&lt;p&gt;As before here also an animated bar plot is used to explain this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)
attach(Thanksgiving_No)
# people who do not celebrate
dont_FI&amp;lt;-as.data.frame(summary.factor(Thanksgiving_No$family_income))
# people who do celebrate
do_FI&amp;lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$family_income)))
# people who do celebrate
data_do_FI&amp;lt;-data.frame(group=c(&amp;quot;0-9,999&amp;quot;,&amp;quot;10,000-24,999&amp;quot;,&amp;quot;25,000-49,999&amp;quot;,
                               &amp;quot;50,000-74,999&amp;quot;,&amp;quot;75,000-99,999&amp;quot;,&amp;quot;100,000-124,999&amp;quot;,
                               &amp;quot;125,000-149,999&amp;quot;,&amp;quot;150,000-174,999&amp;quot;,
                               &amp;quot;175,000-199,999&amp;quot;,&amp;quot;200,000 and up&amp;quot;,&amp;quot;Not to answer&amp;quot;),
                       values=c(52,60,166,127,127,109,48,38,26,76,118),
                       frame=rep(&amp;quot;Do Celebrate&amp;quot;,11))
# people who do not celebrate
data_dont_FI&amp;lt;-data.frame(group=c(&amp;quot;0-9,999&amp;quot;,&amp;quot;10,000-24,999&amp;quot;,&amp;quot;25,000-49,999&amp;quot;,
                                 &amp;quot;50,000-74,999&amp;quot;,&amp;quot;75,000-99,999&amp;quot;,&amp;quot;100,000-124,999&amp;quot;,
                               &amp;quot;125,000-149,999&amp;quot;,&amp;quot;150,000-174,999&amp;quot;,&amp;quot;175,000-199,999&amp;quot;,
                               &amp;quot;200,000 and up&amp;quot;,&amp;quot;Not to answer&amp;quot;),
                         values=c(14,8,14,8,6,2,1,2,1,4,18),
                         frame=rep(&amp;quot;Do not Celebrate&amp;quot;,11))
# combine the dataset
data_FI&amp;lt;-rbind(data_do_FI,data_dont_FI)

# animated plot for people who do celebrate and who do not celebrate
ggplot(data_FI,aes(group,values))+
  geom_bar(stat = &amp;#39;identity&amp;#39;,position = &amp;quot;identity&amp;quot;)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Family Income in dollars&amp;quot;)+
  ggtitle(&amp;quot;Animated plot how Do and Do not people prefer \naccording to Family Income&amp;quot;)+
  scale_y_continuous(labels= seq(0,170,10),breaks = seq(0,170,10))+
  geom_text(aes(label=values), vjust=1)+coord_flip()+
  transition_states(frame,transition_length = 2,state_length = 3)+
  enter_fade()+
  exit_shrink()+
  ease_aes(&amp;#39;bounce-in&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_34/2018-11-26-week-34-thanksgiving_files/figure-html/Family%20Income%20with%20do%20people%20and%20do%20not%20people-1.gif&#34; width=&#34;100%&#34; height=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)
detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;family-income-with-other-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Family Income with Other Factors&lt;/h2&gt;
&lt;p&gt;Of people who do celebrate Thanksgiving the lowest percentage of zero is for people from West North Central with Family Income USD 175,000 to 199,999, people from West South Central with Family Income USD 175,000 to 199,999, people from Mountain with Family Income USD 150,000 to 174,999 and people from Mountain with Family Income USD 175,000 to 199,999. Highest percentage of 4.9409 goes to people from South Atlantic with Family Income USD 25,000 to 49,999.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)

#kable(addmargins(table(family_income,us_region)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(12,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do celebrate
kable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),&amp;quot;html&amp;quot;)%&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F,font_size = 10) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(12,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;2.5cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$0 to $9,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.9409
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$10,000 to $24,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3963
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.2299
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$100,000 to $124,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.7927
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3963
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4296
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.6112
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.2556
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
11.7078
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$125,000 to $149,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.8260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.1558
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$150,000 to $174,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7519
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.0816
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$175,000 to $199,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.7926
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$200,000 and up
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7519
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4296
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0408
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7519
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1815
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.0559
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$25,000 to $49,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.2556
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1815
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5779
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6445
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1482
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.9409
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.4006
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$50,000 to $74,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5779
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0408
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.8260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.7927
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3963
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.5339
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$75,000 to $99,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.7927
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.9334
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8593
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2889
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.4705
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2889
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.5339
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
Prefer not to answer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1482
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7519
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1815
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9001
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2889
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
12.5671
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 2.5cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.5748
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
6.0149
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
15.5747
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4039
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.9076
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
13.9635
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
21.8044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.6261
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
9.1301
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a lot of cell values which have zero therefore I am not going to state them. Further, Highest percentage value of 5.8824 is from people of Middle Atlantic and Family Income categories of USD 0 to 9,999 and USD 25,000 to 49,999.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_No)

#kable(addmargins(table(family_income,us_region)))  %&amp;gt;% 
#  kable_styling(&amp;quot;striped&amp;quot;,full_width = F) %&amp;gt;%
#  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
#  row_spec(12,bold = T,color = &amp;quot;red&amp;quot;)

# table of percentages for people who do not celebrate
kable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),&amp;quot;html&amp;quot;)%&amp;gt;% 
  kable_styling(&amp;quot;striped&amp;quot;,full_width = F,font_size = 10) %&amp;gt;%
  column_spec(11,bold = T,color = &amp;quot;red&amp;quot;) %&amp;gt;%
  row_spec(12,bold = T,color = &amp;quot;red&amp;quot;)%&amp;gt;%
  column_spec(1,bold = T,width = &amp;#39;2.5cm&amp;#39;) %&amp;gt;%
  row_spec(0,bold = T,align = &amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 10px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
East South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Middle Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Mountain
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
New England
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Pacific
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
South Atlantic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West North Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
West South Central
&lt;/th&gt;
&lt;th style=&#34;text-align:right;font-weight: bold;text-align: center;&#34;&gt;
Sum
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$0 to $9,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
17.6472
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$10,000 to $24,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$100,000 to $124,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$125,000 to $149,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$150,000 to $174,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$175,000 to $199,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$200,000 and up
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$25,000 to $49,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
19.1178
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$50,000 to $74,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
11.7648
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
$75,000 to $99,999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 2.5cm; font-weight: bold;&#34;&gt;
Prefer not to answer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4706
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
19.1178
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: red;width: 2.5cm; font-weight: bold;&#34;&gt;
Sum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
7.3530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
5.8824
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
20.5884
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
23.5296
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
16.1766
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
4.4118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;&#34;&gt;
8.8236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: red;font-weight: bold;color: red;&#34;&gt;
100.0008
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;us-region-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;US Region Distribution&lt;/h2&gt;
&lt;p&gt;There are 9 regions in both sides, and also both sides have missing values. People who do celebrate Thanksgiving have 49 missing values, while only 10 are missing values for people who do not celebrate Thanksgiving.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Thanksgiving_Yes)
attach(Thanksgiving_No)
# people who do not celebrate
dont_USR&amp;lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_No$us_region)))
# people who do celebrate
do_USR&amp;lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$us_region)))
# people who do celebrate
data_do_USR&amp;lt;-data.frame(group=c(&amp;quot;East North Central&amp;quot;, &amp;quot;East South Central&amp;quot;,
                                &amp;quot;West South Central&amp;quot;, &amp;quot;West North Central&amp;quot;,
                                &amp;quot;Middle Atlantic&amp;quot;,&amp;quot;South Atlantic&amp;quot;, &amp;quot;Mountain&amp;quot;, 
                                &amp;quot;New England&amp;quot;, &amp;quot;Pacific&amp;quot;),
                       values=c(145,56,85,71,145,203,41,55,130),
                       frame=rep(&amp;quot;Do Celebrate&amp;quot;,9))
# people who do not celebrate
data_dont_USR&amp;lt;-data.frame(group=c(&amp;quot;East North Central&amp;quot;, &amp;quot;East South Central&amp;quot;,
                                  &amp;quot;West South Central&amp;quot;, &amp;quot;West North Central&amp;quot;,
                                &amp;quot;Middle Atlantic&amp;quot;,&amp;quot;South Atlantic&amp;quot;, &amp;quot;Mountain&amp;quot;, 
                                &amp;quot;New England&amp;quot;, &amp;quot;Pacific&amp;quot;),
                         values=c(5,4,6,3,14,11,6,3,16),
                         frame=rep(&amp;quot;Do not Celebrate&amp;quot;,9))
# combine both datasets
data_USR&amp;lt;-rbind(data_do_USR,data_dont_USR)

# animated plot for people who do celebrate and who do not celebrate
ggplot(data_USR,aes(x=str_wrap(group,7),values))+
  geom_bar(stat = &amp;#39;identity&amp;#39;,position = &amp;quot;identity&amp;quot;)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;US Regions&amp;quot;)+
  ggtitle(&amp;quot;Animated plot how Do and Do not people prefer \naccording to US Regions&amp;quot;)+
  scale_y_continuous(labels= seq(0,210,10),breaks = seq(0,210,10))+
  geom_text(aes(label=values), vjust=1)+
  transition_states(frame,transition_length = 2,state_length = 3)+
  enter_fade()+
  exit_shrink()+
  ease_aes(&amp;#39;cubic-in-out&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Week_34/2018-11-26-week-34-thanksgiving_files/figure-html/US%20region%20with%20do%20people%20and%20do%20not%20people-1.gif&#34; width=&#34;100%&#34; height=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(Thanksgiving_Yes)
detach(Thanksgiving_No)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I shall conclude my findings in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can use gganimate to make bar plots interesting and useful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;kable is very useful because of styling options.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There are more than 50 variables therefore much more can be done than describing the data-set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can use advanced methods such as clustering and model fitting.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Please see that&lt;/em&gt;&lt;br /&gt;
This is my fourth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
