<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Consulting Statistician on Consulting Statistician</title>
    <link>/</link>
    <description>Recent content in Consulting Statistician on Consulting Statistician</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tree of Binomial Distribution</title>
      <link>/post/binomialdistribution/binomialdistribution/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/binomialdistribution/binomialdistribution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Week 7: Spending On Science Stuff</title>
      <link>/post/tidytuesday2019/week7/week7/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week7/week7/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#climate-change-research&#34;&gt;Climate Change Research&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#energy&#34;&gt;Energy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#federal&#34;&gt;Federal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages 
library(readr)
library(tidyverse)
library(gganimate)
library(dplyr)
library(magrittr)

# load the data
climate &amp;lt;- read_csv(&amp;quot;climate_spending.csv&amp;quot;)

energy &amp;lt;- read_csv(&amp;quot;energy_spending.csv&amp;quot;, 
                    col_types = cols(year = col_integer()))

federal &amp;lt;- read_csv(&amp;quot;fed_r_d_spending.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though I can go further and do an investigative plotting from the rest data it is not done here. I was more focused on the scientific notation values in the plotting and scales, which were bothering me a lot.&lt;/p&gt;
&lt;p&gt;3 Data sets are given here, they are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Global Climate Change Research Program Spending. - climate&lt;/li&gt;
&lt;li&gt;Energy Departments Data. - energy&lt;/li&gt;
&lt;li&gt;Total Federal R &amp;amp; D Spending by Department. - federal&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Oddly though climate data-set did not have year values, I checked the downloaded csv file and the GitHub upload as well. Well, That did not stop me from doing some tidy plotting.&lt;/p&gt;
&lt;p&gt;You can obtain the data from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-12&#34;&gt;here.&lt;/a&gt; It should be noted that I am not going to rename the abbreviation of departments with their full names, so below is a screen shot which would come in handy.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;department.jpg&#34; alt=&#34;Department Full Names with Abbreviations&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Department Full Names with Abbreviations&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Scientific notation of numbers and plotting them was fun. Except for the R and D budget, other 3 have a steady increase over the years. Further, the R and D budget is very small than the others. Code: &lt;a href=&#34;https://t.co/jmzfTGMRaT&#34;&gt;https://t.co/jmzfTGMRaT&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/3WfBU72kWW&#34;&gt;pic.twitter.com/3WfBU72kWW&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1095152521507733505?ref_src=twsrc%5Etfw&#34;&gt;February 12, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week7&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;climate-change-research&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Climate Change Research&lt;/h1&gt;
&lt;p&gt;As I mentioned earlier for the climate data there are no values in the year column, but according to summary I was able to deduce that we have 18 years of information. When we do plot it is going to be the summation for each department in a bar.&lt;/p&gt;
&lt;p&gt;Clearly NASA has the most amount ( above than 2.5 x 10^10) of spending because rockets are expensive, second place goes to NSF (5 x 10^9) and third place to NOAA. Lowest amount of spending is to the department of interior (8.47 x 10^8).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(climate,aes(x=fct_inorder(department),y=gcc_spending,fill=department))+
  geom_bar(stat=&amp;quot;identity&amp;quot;,show.legend = FALSE)+
  ggtitle(&amp;quot;Total GCC Spending for 18 Years&amp;quot;)+
  scale_y_continuous(labels = scales::scientific,breaks = seq(0,2.75e+10,0.25e+10))+
  xlab(&amp;quot;Sub Agency / Department&amp;quot;)+ylab(&amp;quot;GCC Spending (in USD)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Global%20Climate%20change%20Research-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;energy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Energy&lt;/h1&gt;
&lt;p&gt;Since 1997 to 2018 how Energy Department funding has changed with sub agency/ department. Office of Science R &amp;amp; D and Atomic Energy Defense are competitive over the years and for a short period of time the latter has less funding than the former, this was between 2006 to 2010.&lt;/p&gt;
&lt;p&gt;Other agencies oscillates over the years while reaching new highs and lows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(energy,aes(x=department,y=energy_spending,fill=year))+
          geom_bar(stat=&amp;quot;identity&amp;quot;,position =&amp;quot;identity&amp;quot;)+
          transition_time(year)+
          geom_text(aes(label=scales::scientific(energy_spending)),
                    vjust = &amp;quot;inward&amp;quot;, hjust = &amp;quot;inward&amp;quot;)+
          ease_aes(&amp;quot;linear&amp;quot;)+coord_flip()+
          ylab(&amp;quot;Energy Spending (in USD)&amp;quot;)+
          theme(legend.position = &amp;quot;right&amp;quot;)+
          xlab(&amp;quot;Sub Agency / Department&amp;quot;)+
          scale_fill_continuous(breaks = seq(1997,2018,3))+
          scale_y_continuous(labels = scales::scientific)+
          ggtitle(&amp;quot;Energy Spending Of Year : {frame_time}&amp;quot;)

animate(p,fps=1,nframes=22)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Energy%20Funding-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;federal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Federal&lt;/h1&gt;
&lt;p&gt;Data of Federal funding has four different types to be compared and they are mentioned below in the description image which would make explanation more easier.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;description.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Except rd_budget other have a very clear increasing amount between 1976 to 2018. Further, all four plots have different scales and the limits are widely different for each plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-federal %&amp;gt;%
    gather(funding,amount,c(rd_budget,total_outlays,discretionary_outlays,gdp)) %&amp;gt;%
    ggplot(.,aes(x=factor(department),y=amount,color=year))+
           geom_jitter()+transition_time(year)+
           ease_aes(&amp;quot;linear&amp;quot;)+coord_flip()+
           shadow_mark()+
           theme(legend.position = &amp;quot;right&amp;quot;)+
           ylab(&amp;quot;Spending in USD&amp;quot;)+xlab(&amp;quot;Department&amp;quot;)+
           ggtitle(&amp;quot;Total Federal R&amp;amp;D for Year : {frame_time}&amp;quot;)+
           scale_color_continuous(breaks = seq(1976,2018,6),labels=seq(1976,2018,6))+
           scale_y_continuous(labels = scales::scientific)+
           facet_wrap(~funding,scales = &amp;quot;free&amp;quot;)

animate(p,fps=1,nframes=42)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week7/index_files/figure-html/Federal%20Funding-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 6 : Mortgage, Recession and States</title>
      <link>/post/tidytuesday2019/week6/week6/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week6/week6/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mortgage&#34;&gt;Mortgage&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-rate-30-years-from-1971-to-2018&#34;&gt;Fixed Rate 30 Years from 1971 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-rate-15-years-from-1991-to-2018&#34;&gt;Fixed Rate 15 Years from 1991 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fees-and-points-of-30-years-from-1971-to-2018&#34;&gt;Fees and Points of 30 Years from 1971 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fees-and-points-of-15-years-from-1991-to-2018&#34;&gt;Fees and Points of 15 Years from 1991 to 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#states&#34;&gt;States&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#new-england-region&#34;&gt;New England Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mideast-region&#34;&gt;Mideast Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#great-lakes-region&#34;&gt;Great Lakes Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plains-region&#34;&gt;Plains Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#southeast-region&#34;&gt;Southeast Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#southwest-region&#34;&gt;Southwest Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rocky-mountain-region&#34;&gt;Rocky Mountain Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#far-west-region&#34;&gt;Far West Region&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(bbplot)
library(gganimate)
library(magrittr)
library(lubridate)

# load the data
mortgage &amp;lt;- read_csv(&amp;quot;mortgage.csv&amp;quot;, 
                     col_types = cols(adjustable_margin_5_1_hybrid = col_double(), 
                     adjustable_rate_5_1_hybrid = col_double(), 
                     fees_and_pts_15_yr = col_double(), fees_and_pts_30_yr = col_double(), 
                     fees_and_pts_5_1_hybrid = col_double(), 
                     fixed_rate_15_yr = col_double(), 
                     spread_30_yr_fixed_and_5_1_adjustable = col_double())
                     )
recessions &amp;lt;- read_csv(&amp;quot;recessions.csv&amp;quot;)
state_hpi &amp;lt;- read_csv(&amp;quot;state_hpi.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Week 6 has three data-sets, which are mortgage, recession and state_hpi. Number variables in each data-set is less than 10. You can acquire the data-set from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-05&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week6&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  What happened in the year 2000 !, and the 2007 recession has made some drastic changes in US avg and the Price index for all regions.  What is happening to Hawaii? Code:  &lt;a href=&#34;https://t.co/WuZD9k8X3S&#34;&gt;https://t.co/WuZD9k8X3S&lt;/a&gt; &lt;a href=&#34;https://t.co/0ecmqnUsrJ&#34;&gt;pic.twitter.com/0ecmqnUsrJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1093047376879775744?ref_src=twsrc%5Etfw&#34;&gt;February 6, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;According to the description there is not much of variation in the recession data-set, but this is not the case in other two data-sets.&lt;/p&gt;
&lt;div id=&#34;mortgage&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mortgage&lt;/h1&gt;
&lt;p&gt;Mortgage data-set has 9 variables with 8 of them are related to the financial sector and one is refereed to date. So the below analysis or interpretation will be values changing over time. These values will be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fixed Rate 30 Years&lt;/li&gt;
&lt;li&gt;Fixed Rate 15 Years&lt;/li&gt;
&lt;li&gt;Fees and Percentage Points (30 Years) of the loan amount.&lt;/li&gt;
&lt;li&gt;Fees and Percentage Points (15 Years) of the loan amount.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;fixed-rate-30-years-from-1971-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Rate 30 Years from 1971 to 2018&lt;/h2&gt;
&lt;p&gt;Each week the Fixed Rate of 30 Years has been set and I am exploring how it changes in each year from 1971 to 2018. We can clearly see in the early Weeks of 1980 it has significantly increased higher than 17.5%, but in early 1970 it was only 7.5%.&lt;/p&gt;
&lt;p&gt;By 1990 it has dropped to 7.5% and this pattern continues further until year 2018 where in December the Fixed Rate of 30 Years is slightly less than 5%.&lt;/p&gt;
&lt;p&gt;Each year there can be on of the below patterns I mentioned if the year is divided into two half’s.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First and Second Half of the Year hold the same Percentage points.&lt;/li&gt;
&lt;li&gt;First Half of the Year has Higher percentage Points than the second half.&lt;/li&gt;
&lt;li&gt;Vice versa of 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(mortgage,aes(x=factor(year(date)),y=fixed_rate_30_yr,color=week(date)))+
          geom_jitter()+transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
          shadow_mark()+xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fixed Rate 30 Year Mortgage (%)&amp;quot;)+
          ggtitle(&amp;quot;Fixed Rate 30 Year Morgage Change by the Year: {round(frame_time)}&amp;quot;)+
          labs(color=&amp;quot;Week of the Year&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;,
                axis.text.x =element_text(angle = 90, hjust = 1))
    
animate(p,nframes=48, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fixed%20rate%2030%20years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-rate-15-years-from-1991-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Rate 15 Years from 1991 to 2018&lt;/h2&gt;
&lt;p&gt;From 1991 only we have Fixed Rate for 15 Years and in the beginning we can see the percentage slightly above 8. and over the years it is decreasing while some fluctuations occur. This fluctuations happen in the years of 2000, 2006, 2007 and 2018, where they brake pattern of decreasing.&lt;/p&gt;
&lt;p&gt;In the year 2018 it reaches slightly less than 4% in the first 20 or so weeks, but the last 20 weeks the percentage is above 4%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(mortgage,year(date)&amp;gt;=1991),
          aes(x=factor(year(date)),y=fixed_rate_15_yr,color=week(date)))+
          geom_jitter()+transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
          shadow_mark()+xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fixed Rate 15 Year Mortgage (%)&amp;quot;)+
          ggtitle(&amp;quot;Fixed Rate 15 Year Morgage Change by the Year: {round(frame_time)}&amp;quot;)+
          labs(color=&amp;quot;Week of the Year&amp;quot;)+
          theme(legend.position = &amp;quot;bottom&amp;quot;,
                axis.text.x =element_text(angle = 90, hjust = 1))
    
animate(p,nframes=28, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fixed%20rate%2015%20years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fees-and-points-of-30-years-from-1971-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fees and Points of 30 Years from 1971 to 2018&lt;/h2&gt;
&lt;p&gt;Highest peek occurs in 1983 which is 2.7 and it decreases over the years gradually. While in the year 1971 the points were close to 1. The gradual decrease is not in effect between the years 1995 and 1996 and it clear in the plot. Yet, we can see no other anomaly in the next few years after 1996, while in 2007 it reaches its lowest point of slightly less than 0.3 (Could be related to the Great recession)&lt;/p&gt;
&lt;p&gt;Anyway by year 2018 after this 2007 recession the points have increased but has not reached 1 and is always oscillating between 0.4 and 0.6 in the years of 2015 to 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1&amp;lt;-ggplot(mortgage,aes(x=factor(year(date)),y=factor(fees_and_pts_30_yr),color=week(date)))+
       geom_jitter()+ theme(legend.position = &amp;quot;bottom&amp;quot;,
                            axis.text.x =element_text(angle = 90, hjust = 1))+
       xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fees and Percentage points of the Loan Amount&amp;quot;)+
       labs(color=&amp;quot;Week of the Year&amp;quot;)+
       ggtitle(&amp;quot;Fess and Percentage points (30 Years) of the Loan Amount \n 
                by the Year : {round(frame_time)}&amp;quot;)+
       transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
       shadow_mark()

animate(p1,nframes=48, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fees%20and%20pts%20of%2030%20year-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fees-and-points-of-15-years-from-1991-to-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fees and Points of 15 Years from 1991 to 2018&lt;/h2&gt;
&lt;p&gt;In 1991 the points are close to 1.9 and it wavers in between 1.6 and 1.8 until 1997. There is a significant drop from 1997 to 1998 where the points end up averaged around 1 and over the years it slowly decreases until year 2007. Where the lowest point of 0.3 occurs.&lt;/p&gt;
&lt;p&gt;After this new low it struggles to maintain any steady increase and rather holds below 0.8 over the next years until 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(mortgage,year(date)&amp;gt;=1991),
          aes(x=factor(year(date)),y=factor(fees_and_pts_15_yr),color=week(date)))+
       geom_jitter()+ theme(legend.position = &amp;quot;bottom&amp;quot;,
                            axis.text.x =element_text(angle = 90, hjust = 1))+
       xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Fees and Percentage points of the Loan Amount&amp;quot;)+
       labs(color=&amp;quot;Week of the Year&amp;quot;)+
       ggtitle(&amp;quot;Fees and Percentage points (15 Year)of the Loan Amount by the Year : {round(frame_time)}&amp;quot;)+
       transition_time(year(date))+ease_aes(&amp;quot;linear&amp;quot;)+
       shadow_mark()

animate(p,nframes=28, fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/mortgage%20fees%20and%20pts%20of%2015%20year-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;states&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;States&lt;/h1&gt;
&lt;p&gt;United States of America has 50 states and comparing all of them at the same time is a ludicrous idea. Therefore, I decided to combine few states and compare and them as regions. In order to do this clustering I chose the Wikipedia page which was helpful for me.&lt;/p&gt;
&lt;p&gt;There are multiple reasons to make different regions out of the 50 states of USA. But according to the Wikipedia page I figured it would be best to focus on the financial side or to be precise cluster states based on the “Bureau of Economic Analysis Regions”.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States&#34;&gt;Wikipedia for US Regions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So according to the above choice we have 8 regions clustering 50 states and they are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;New England&lt;/li&gt;
&lt;li&gt;Mideast&lt;/li&gt;
&lt;li&gt;Great Lakes&lt;/li&gt;
&lt;li&gt;Plains&lt;/li&gt;
&lt;li&gt;Southeast&lt;/li&gt;
&lt;li&gt;Southwest&lt;/li&gt;
&lt;li&gt;Rocky Mountain&lt;/li&gt;
&lt;li&gt;Far West&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;new-england-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New England Region&lt;/h2&gt;
&lt;p&gt;Clear visibility of 2007 recession where US Avg and Price Index declining until 2010 and then improving over the next few years. All states begin very closely but end up very differently in 2018 and in troubled times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;CT&amp;quot;|state==&amp;quot;ME&amp;quot;|state==&amp;quot;MA&amp;quot;|
                           state==&amp;quot;NH&amp;quot;| state==&amp;quot;RI&amp;quot;|state==&amp;quot;VT&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20New%20England-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mideast-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mideast Region&lt;/h2&gt;
&lt;p&gt;After the 2007 recession there is clear difference among DC and other states and the gap cannot be ignored at all.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;DE&amp;quot;|state==&amp;quot;DC&amp;quot;|state==&amp;quot;MD&amp;quot;|
                           state==&amp;quot;NJ&amp;quot;| state==&amp;quot;NY&amp;quot;|state==&amp;quot;PA&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Mideast%20Region-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;great-lakes-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Great Lakes Region&lt;/h2&gt;
&lt;p&gt;After year 2000 there is clear difference among the 5 states and it becomes more complex with the 2007 recession and recovery periods. But this is not the case in year 2018 because all five states are now closely intact with the increase with both variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;IL&amp;quot;|state==&amp;quot;OH&amp;quot;|state==&amp;quot;WI&amp;quot;|
                           state==&amp;quot;IN&amp;quot;| state==&amp;quot;MI&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Great%20Lakes-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plains-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plains Region&lt;/h2&gt;
&lt;p&gt;Before the 2007 recession all states behaved very similarly, but this is not the case after year 2011 where North Dakota has a higher Price index and US Average than other states which is clearly seen in the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;IO&amp;quot;|state==&amp;quot;MN&amp;quot;|state==&amp;quot;NE&amp;quot;|
                           state==&amp;quot;KS&amp;quot;| state==&amp;quot;MS&amp;quot;|state==&amp;quot;ND&amp;quot;|state==&amp;quot;SD&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Plains-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;southeast-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Southeast Region&lt;/h2&gt;
&lt;p&gt;Southeast region has alot of states therefore it would be time consuming to compare. Clearly the 2007 recession has a toll on both variables, but not as the effect from year 2000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AL&amp;quot;|state==&amp;quot;FL&amp;quot;|state==&amp;quot;KY&amp;quot;|
                           state==&amp;quot;AR&amp;quot;|state==&amp;quot;GA&amp;quot;|state==&amp;quot;MS&amp;quot;|
                           state==&amp;quot;LA&amp;quot;|state==&amp;quot;NC&amp;quot;|state==&amp;quot;SC&amp;quot;|
                           state==&amp;quot;TN&amp;quot;|state==&amp;quot;VA&amp;quot;|state==&amp;quot;WV&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Southeast-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;southwest-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Southwest Region&lt;/h2&gt;
&lt;p&gt;Before the 2007 recession and after also we can see the clear changes. Before that in year 2000 also we can see rapid changes which lead up-to the recession. The damage done by the recession have not been recovered in some states even by 2018 according to the gap in Price index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AZ&amp;quot;|state==&amp;quot;OK&amp;quot;|
                           state==&amp;quot;TX&amp;quot;|state==&amp;quot;NM&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Southwest-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rocky-mountain-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rocky Mountain Region&lt;/h2&gt;
&lt;p&gt;Changes after 2000 are very different for the 5 states in this region and after the 2007 recession also we can see the rapid set back in Us avg and price index. But this is not the case after 2013 even though it has already made significant amount of divide between the state of MO and other states, which is clearly seen at the end of year 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;CO&amp;quot;|state==&amp;quot;MO&amp;quot;|state==&amp;quot;WY&amp;quot;|
                           state==&amp;quot;ID&amp;quot;| state==&amp;quot;UT&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Rocky%20Mountain-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;far-west-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Far West Region&lt;/h2&gt;
&lt;p&gt;Early 1990 has a sudden raise and it quickly settles down close to year 1998. Where by 2000 all six states share the same price index value, but this changes over time with clear difference among two groups. Each group containing 3 states, but this progress entirely changes by the 2007 recession and its recovery. Because clearly after 2013 there is no more 2 groups, it is now 3 groups. Where state of Hawaii has the highest pricing index and lowest goes to Alaska, this is by the end of year 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(subset(state_hpi,state==&amp;quot;AL&amp;quot;|state==&amp;quot;NV&amp;quot;|state==&amp;quot;OR&amp;quot;|
                           state==&amp;quot;CA&amp;quot;| state==&amp;quot;HI&amp;quot;|state==&amp;quot;WA&amp;quot;),
       aes(x=us_avg,y=price_index,color=state))+
       geom_point()+xlab(&amp;quot;US Average&amp;quot;)+ylab(&amp;quot;Price Index&amp;quot;)+
       ggtitle(&amp;quot;Price Index vs Us Avg change over Year: {round(frame_time)}&amp;quot;)+
       shadow_mark()+
       transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
animate(p,nframes = 44,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week6/index_files/figure-html/Region%20Far%20West-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;It might look that I have not enough justice for the changes which occurred before the year 2000, and I do agree with you. But if I do add them into my consideration this article would be very long. Hopefully, the animate plots clearly indicate the strong changes which occurred in the pre-y2k era.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 5: Dairy Products in USA</title>
      <link>/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fluid-milk-sales&#34;&gt;Fluid Milk Sales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-milk-production&#34;&gt;State Milk Production&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#us-states-and-milk-production-over-the-years&#34;&gt;US States and Milk Production Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summing-states-of-same-regions-over-the-years&#34;&gt;Summing States of Same Regions Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#averaging-regions-considering-all-the-states-over-the-years&#34;&gt;Averaging Regions Considering All the States Over the Years&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese&#34;&gt;Cheese&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-other&#34;&gt;Cheese with Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-total&#34;&gt;Cheese with Total&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cheese-with-known-type-names&#34;&gt;Cheese with known Type Names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(tidyverse)
library(magrittr)
library(ggthemr)
library(readr)
library(gganimate)
library(usmap)

# load the data
fluid_milk_sales &amp;lt;- read_csv(&amp;quot;fluid_milk_sales.csv&amp;quot;)
state_milk_production &amp;lt;- read_csv(&amp;quot;state_milk_production.csv&amp;quot;)
clean_cheese &amp;lt;- read_csv(&amp;quot;clean_cheese.csv&amp;quot;)

# load the theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;5 data-sets are given here, but I will be only discussing 3. They are Fluid Milk Sales, State Milk production and Clean Cheese. Clean Cheese has only few rows (48) and few columns (17). This is not the case in Fluid Milk Sales and State Milk production.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Code: &lt;a href=&#34;https://t.co/3bSXxttydA&#34;&gt;https://t.co/3bSXxttydA&lt;/a&gt; Wisconsin and California have a milk interest and it is very high. &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/Ide8jfalyd&#34;&gt;pic.twitter.com/Ide8jfalyd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1090266660785729536?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week5&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fluid-milk-sales&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fluid Milk Sales&lt;/h1&gt;
&lt;p&gt;Fluid Milk Sales has information for 9 different types of milk and how their Consumption in Pounds has changed over time from 1970 to 2017. We can see how Whole and Reduced Fat(2%) type milk are changing over the years with significant amount. Further, We can see how other types are changing from the initial order in 1970 of lower(Eggnog) amount to higher(Whole) amount in pounds over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(fluid_milk_sales)

fluid_milk_sales$pounds&amp;lt;-fluid_milk_sales$pounds/(10^7)

# how sales change over the years for 9 different types of milk
p&amp;lt;-ggplot(fluid_milk_sales,aes(x=fct_inorder(milk_type) ,y=pounds,fill=year))+
       geom_bar(stat=&amp;quot;identity&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+
       xlab(&amp;quot;Milk Type&amp;quot;)+ylab(&amp;quot;Pounds (in 10^7)&amp;quot;)+
       coord_flip()+
       ggtitle(&amp;quot;Milk Type vs Pounds in year: {round(frame_time)}&amp;quot;)

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Fluid%20milk%20Sales-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(fluid_milk_sales)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;state-milk-production&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;State Milk Production&lt;/h1&gt;
&lt;p&gt;In the 48 years we are separating them by 24 years each for a plot. Using &lt;a href=&#34;&#34;&gt;usmap&lt;/a&gt; package I am going to plot it into their respective states in perspective of Milk Produced lbs in 10^6.&lt;/p&gt;
&lt;div id=&#34;us-states-and-milk-production-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;US States and Milk Production Over the Years&lt;/h2&gt;
&lt;p&gt;In the first half of 1970 to 1993 we can see how a few states are having steady increase over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(state_milk_production)

# dividing the milk produced by 10^6
#summary(state_milk_production$milk_produced)
state_milk_production$milk_produced&amp;lt;-state_milk_production$milk_produced/(10^6)

# plot in us map the milk produced by state for years 1970 to 1993
plot_usmap(data=subset(state_milk_production,year &amp;lt;=&amp;quot;1993&amp;quot;),values = &amp;quot;milk_produced&amp;quot;)+
          facet_wrap(~factor(year),ncol = 4)+
          ggtitle(&amp;quot;Over the years Milk production changing in USA&amp;quot;)+
          theme(legend.position = &amp;quot;left&amp;quot;)+
          labs(fill=&amp;quot;lbs in 10^6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/State%20Milk%20production%20Us%20Map-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is similar to the next half years which is from 1994 to 2017 as well. Similar increase occurs for the above same states as I see in the below plot. Well it is not very accurately described in the two plots for us to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot in us map the milk produced by state for years 1994 to 2017
plot_usmap(data=subset(state_milk_production,year &amp;gt;&amp;quot;1993&amp;quot;),values = &amp;quot;milk_produced&amp;quot;)+
          facet_wrap(~factor(year),ncol = 4)+
          ggtitle(&amp;quot;Over the years Milk production changing in USA&amp;quot;)+
          theme(legend.position = &amp;quot;left&amp;quot;)+
          labs(fill=&amp;quot;lbs in 10^6&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/State%20Milk%20production%20Us%20Map%201-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To understand the above same change over the years clearly I have created a bar plot how the increase occurs. This plot also indicates how much change has occurred over the 48 years for each state who produces milk. Clearly the states California and Wisconsin have higher increase over the years, which is very strong. There are some states which have not produced more amount each year their previous years.&lt;/p&gt;
&lt;p&gt;The states Wyoming, Rhode island, Hawaii, Delaware and Alaska have very low amount of milk production over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# over the years how milk production has changed for each year in a bar plot
p&amp;lt;-ggplot(state_milk_production,aes(x=state,y=milk_produced,
                                    fill=year))+
      geom_bar(stat=&amp;quot;identity&amp;quot;)+coord_flip()+
      transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()+
      xlab(&amp;quot;State&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
      ggtitle(&amp;quot;States vs Milk Produced in year: {round(frame_time)}&amp;quot;)

animate(p,nframes = 48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/State%20Milk%20production%20over%20the%20years%20for%20states-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-states-of-same-regions-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summing States of Same Regions Over the Years&lt;/h2&gt;
&lt;p&gt;There are 50 states but only 10 regions and not all regions have equal amount of states. Therefore I am going to sum up the milk production for all regions over the years and try to understand if there is any pattern.&lt;/p&gt;
&lt;p&gt;In order to do this I have used the &lt;a href=&#34;&#34;&gt;dplyr&lt;/a&gt; package and created a function which would sum up the production for each region of each year. Similarly, this function has the ability to get the average production for each region of each year as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# manipulating the data by sum and mean
by_region_sum&amp;lt;-function(i,ch_sum)
{
  if(ch_sum==TRUE)
  {
    # subsetting by summation over all years for each region
    temp&amp;lt;-subset(state_milk_production,year==i,select=c(&amp;quot;region&amp;quot;,&amp;quot;milk_produced&amp;quot;)) %&amp;gt;%
                  group_by(region) %&amp;gt;%
                  summarise_each(funs(sum))
  
  output&amp;lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)
  }
  else
  {
    # subsetting by Average over all years for each region    
    temp&amp;lt;-subset(state_milk_production,year==i,select=c(&amp;quot;region&amp;quot;,&amp;quot;milk_produced&amp;quot;)) %&amp;gt;%
                  group_by(region) %&amp;gt;%
                  summarise_each(funs(mean))
  
    output&amp;lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)
  }
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the by_region_sum function I am now finding the sum as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting by summation
milk_region_data&amp;lt;-rbind.data.frame(by_region_sum(1970,T),by_region_sum(1971,T),
                                   by_region_sum(1972,T),by_region_sum(1973,T),
                                   by_region_sum(1974,T),by_region_sum(1975,T),
                                   by_region_sum(1976,T),by_region_sum(1977,T),
                                   by_region_sum(1978,T),by_region_sum(1979,T),
                                   by_region_sum(1980,T),by_region_sum(1981,T),
                                   by_region_sum(1982,T),by_region_sum(1983,T),                   
                                   by_region_sum(1984,T),by_region_sum(1985,T),               
                                   by_region_sum(1986,T),by_region_sum(1987,T),                 
                                   by_region_sum(1988,T),by_region_sum(1989,T),
                                   by_region_sum(1990,T),by_region_sum(1991,T), 
                                   by_region_sum(1992,T),by_region_sum(1993,T), 
                                   by_region_sum(1994,T),by_region_sum(1995,T), 
                                   by_region_sum(1996,T),by_region_sum(1997,T), 
                                   by_region_sum(1998,T),by_region_sum(1999,T), 
                                   by_region_sum(2000,T),by_region_sum(2001,T),
                                   by_region_sum(2002,T),by_region_sum(2003,T),   
                                   by_region_sum(2004,T),by_region_sum(2005,T),                 
                                   by_region_sum(2006,T),by_region_sum(2007,T), 
                                   by_region_sum(2008,T),by_region_sum(2009,T),
                                   by_region_sum(2010,T),by_region_sum(2011,T),
                                   by_region_sum(2012,T),by_region_sum(2013,T),
                                   by_region_sum(2014,T),by_region_sum(2015,T),
                                   by_region_sum(2016,T),by_region_sum(2017,T)                  
                                   )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we consider the summation we can see clearly how centered and very limited variation is there for some regions such as Southeast, Northern Plains, Delta States, Corn Belt and Appalachian. There is some variation in the Northeast region. Clear and highest variation is in for Pacific, Mountain and Lake States region.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise total milk production changing  over the year 
ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Total Milk Produced by Year in All Regions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Summation%20by%20region%20for%20jitter-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below is the same graph with points animated by year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise total milk production changing  over the year animated
p&amp;lt;-ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Total Milk Produced for All Regions for Year: {frame_time}&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()
        
animate(p,nframes=48,fps=1)        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Summation%20by%20region%20for%20jitter%20by%20year%20animation-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;averaging-regions-considering-all-the-states-over-the-years&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Averaging Regions Considering All the States Over the Years&lt;/h2&gt;
&lt;p&gt;If we consider the same approach but for the average of each region we can develop the same two plots. Here also we can see the same variation and centering for points for the same regions over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subsetting by average
milk_region_data_new&amp;lt;-rbind.data.frame(by_region_sum(1970,F),by_region_sum(1971,F),
                                   by_region_sum(1972,F),by_region_sum(1973,F),
                                   by_region_sum(1974,F),by_region_sum(1975,F),
                                   by_region_sum(1976,F),by_region_sum(1977,F),
                                   by_region_sum(1978,F),by_region_sum(1979,F),
                                   by_region_sum(1980,F),by_region_sum(1981,F),
                                   by_region_sum(1982,F),by_region_sum(1983,F),                   
                                   by_region_sum(1984,F),by_region_sum(1985,F),               
                                   by_region_sum(1986,F),by_region_sum(1987,F),                 
                                   by_region_sum(1988,F),by_region_sum(1989,F),
                                   by_region_sum(1990,F),by_region_sum(1991,F), 
                                   by_region_sum(1992,F),by_region_sum(1993,F), 
                                   by_region_sum(1994,F),by_region_sum(1995,F), 
                                   by_region_sum(1996,F),by_region_sum(1997,F), 
                                   by_region_sum(1998,F),by_region_sum(1999,F), 
                                   by_region_sum(2000,F),by_region_sum(2001,F),
                                   by_region_sum(2002,F),by_region_sum(2003,F),   
                                   by_region_sum(2004,F),by_region_sum(2005,F),                 
                                   by_region_sum(2006,F),by_region_sum(2007,F), 
                                   by_region_sum(2008,F),by_region_sum(2009,F),
                                   by_region_sum(2010,F),by_region_sum(2011,F),
                                   by_region_sum(2012,F),by_region_sum(2013,F),
                                   by_region_sum(2014,F),by_region_sum(2015,F),
                                   by_region_sum(2016,F),by_region_sum(2017,F)                  
                                   )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is the plot for the average of regions over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise average milk production changing  over the year 
ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+ 
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Average Milk Produced by Year in All Regions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/average%20by%20region%20for%20jitter-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The same plot is now animated for each year and all regions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Region wise average milk production changing  over the year animated
p&amp;lt;-ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+
        geom_jitter()+ coord_flip()+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)+ shadow_mark()+       
        xlab(&amp;quot;Region&amp;quot;)+ylab(&amp;quot;Milk Produced (in 10^6)&amp;quot;)+
        ggtitle(&amp;quot;Average Milk Produced for All Regions for Year: {frame_time}&amp;quot;)+
        transition_time(year)+ease_aes(&amp;quot;linear&amp;quot;)
        
animate(p,nframes=48,fps=1)        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/average%20by%20region%20for%20jitter%20by%20year%20animation-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(state_milk_production)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cheese&lt;/h1&gt;
&lt;p&gt;16 types of cheese are provided in this clean cheese data-set. I will divide these types into 3 types and will not consider few types of cheese. The unit of measurement for the consumption is lbs per person.&lt;/p&gt;
&lt;div id=&#34;cheese-with-other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with Other&lt;/h2&gt;
&lt;p&gt;I am going to consider the types American Other, Italian Other and Swiss for this plot. Red color indicates to American Other, yellow color refers to Italian other and blue for Swiss. Alot of fluctuation for American other type, but this is not the case for Swiss type cheese. There is steady increase for the Italian other type cheese over the years. All of these are less than 4 lbs per person and it is animated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=`American Other`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=`Italian other`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=Swiss),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          transition_time(Year)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Cheese%20with%20other-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese-with-total&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with Total&lt;/h2&gt;
&lt;p&gt;This is also an animated plot but for the cheese types which has the word Total. They are Total American Cheese, Total Italian Cheese, Total Natural Cheese and Total Processed Cheese Products with the colors respectively red, yellow, blue and green.&lt;/p&gt;
&lt;p&gt;All the Consumption units are in between 0 to 40 lbs per person. Clearly Total Natural Cheese has a steady amount of increase from 1970(slightly above 10) to 2017(approximately less than 40). Considering the other three types we can see it is not the same order that it is in 1970 over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=`Total American Chese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=`Total Italian Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=`Total Natural Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          geom_point(aes(y=`Total Processed Cheese Products`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;green&amp;quot;)+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+  
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          transition_time(Year)+ 
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Cheese%20with%20Total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cheese-with-known-type-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cheese with known Type Names&lt;/h2&gt;
&lt;p&gt;Next group of cheese types include Cheddar, Mozzarella, Brick, Processed Cheese and Foods &amp;amp; spreads for the colors respectively red, yellow, blue, green and white. Clearly Cheddar and mozzarella type cheese are are mostly consumed by 2017 above 10 lbs per person, but this is not the case in 1970 where consumption is less than 6 lbs per person.&lt;/p&gt;
&lt;p&gt;Well Processed Cheese and Foods &amp;amp; Spreads have changed very small over the years. The consumption is always less than 6 lbs per person. This is not the case for Brick type cheese where the consumption is close to zero over the years from 1970 until 2017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5 types of cheese change per person over the year in lbs
p&amp;lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ 
          geom_point(aes(y=Cheddar),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;red&amp;quot;)+
          geom_point(aes(y=Mozzarella),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;yellow&amp;quot;)+
          geom_point(aes(y=`Brick`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;blue&amp;quot;)+
          geom_point(aes(y=`Processed Cheese`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;green&amp;quot;)+
          geom_point(aes(y=`Foods and spreads`),stat=&amp;quot;identity&amp;quot;,color=&amp;quot;white&amp;quot;)+
          xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Consumption in lbs per person&amp;quot;)+
          theme(axis.text.x =element_text(angle = 90, hjust = 1))+
          ggtitle(&amp;quot;Cheese Consumption Over the Years&amp;quot;)+
          transition_time(Year)+ 
          ease_aes(&amp;quot;linear&amp;quot;)+shadow_mark()

animate(p,nframes=48,fps=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week5/2019-01-29-week-5-dairy-products-in-usa_files/figure-html/Cheese%20with%20known-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build Your Own Website or Blog Using R, RStudio and R Packages.</title>
      <link>/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#materials&#34;&gt;Materials&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.&#34;&gt;“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#up-and-running-with-blogdown-by-alison-presmanes-hill.&#34;&gt;“Up and Running with Blogdown by Alison Presmanes Hill.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown-websites.&#34;&gt;“Rmarkdown Websites.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.&#34;&gt;“How to make an RMarkdown Website by Nick Strayer &amp;amp; Lucy D’Agostino McGowan.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-websites-in-r-by-emily-c-zabor.&#34;&gt;“Creating websites in R by Emily C Zabor.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-blogdown-by-david-selby.&#34;&gt;“Getting Started with Blogdown by David Selby.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-blogdown-by-danielle-navarro.&#34;&gt;“Getting Started with Blogdown by Danielle Navarro.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blogdown-by-peters-blog.&#34;&gt;“Blogdown by Peter’s Blog.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#academic-by-george-cushen&#34;&gt;“Academic by George Cushen”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#happy-git-and-github-for-the-user.&#34;&gt;*“Happy Git and GitHub for the User.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.&#34;&gt;*“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.&#34;&gt;*“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#my-personal-website-blog&#34;&gt;My Personal Website / Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;R enthusiasts are focused on developing packages and websites to promote their profile and share their knowledge to the world. Recently released packages such as hugo, blogdown, Rmarkdown, bookdown has played a significant amount of role in this popularity for R statistical software among generals users and academics in every field of statistics.&lt;/p&gt;
&lt;p&gt;Due to this reason, I also wanted to develop my own R package to solve problem in hand and share it with the #rstats community. Even though Social Media is a strong way of sharing this amount of information, it is not sturdy over time. To resolve this only I chose to develop my own website using R and supportive tools from R. Your reading this post on my website which I developed in a very short period of time and have being maintaining regularly by posting articles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;materials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Materials&lt;/h1&gt;
&lt;p&gt;I could have written an extensive and long article describing how I developed this website with screenshots and explanatory steps. As it should be a valuable experience I am only going to give you the materials which were used with important points with facts. Further, I shall give you certain specifics of my own website.&lt;/p&gt;
&lt;div id=&#34;making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://slides.yihui.name/2017-rstudio-webinar-blogdown-Yihui-Xie.html#1&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Presentation of 20 slides.&lt;/li&gt;
&lt;li&gt;Most of the basic information for packages which are necessary for website development.&lt;/li&gt;
&lt;li&gt;Brief introduction about the website structure and process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;up-and-running-with-blogdown-by-alison-presmanes-hill.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Up and Running with Blogdown by Alison Presmanes Hill.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://alison.rbind.io/post/up-and-running-with-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brief information for blogdown and other development materials.&lt;/li&gt;
&lt;li&gt;Described information Deployment and maintaining the website with other tools related to R and Rstudio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown-websites.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Rmarkdown Websites.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-13.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Briefest description about using Rmarkdown/Rmd files for website development.&lt;/li&gt;
&lt;li&gt;There are few other links which could be considered useful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“How to make an RMarkdown Website by Nick Strayer &amp;amp; Lucy D’Agostino McGowan.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nickstrayer.me/RMarkdown_Sites_tutorial/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One example sample website developed and explained briefly by the authors.&lt;/li&gt;
&lt;li&gt;Several links to spark curiosity about website development using Rmarkdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-websites-in-r-by-emily-c-zabor.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Creating websites in R by Emily C Zabor.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation on different type of websites which can be produced by R.&lt;/li&gt;
&lt;li&gt;Deployment and additional requirements for website development.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started-with-blogdown-by-david-selby.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Getting Started with Blogdown by David Selby.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://selbydavid.com/wrugdown/2017/05/10/getting-started-with-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Limited amount of information regarding website development.&lt;/li&gt;
&lt;li&gt;This was written for a talk for the “Warwick R User Group Talk” in 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started-with-blogdown-by-danielle-navarro.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Getting Started with Blogdown by Danielle Navarro.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://djnavarro.net/post/2018-04-27-starting-blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extensive amount of information about website development using blogdown.&lt;/li&gt;
&lt;li&gt;More than enough information about the insides of the website.&lt;/li&gt;
&lt;li&gt;Detailed steps of writing posts and changing elements of the website.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;blogdown-by-peters-blog.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Blogdown by Peter’s Blog.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://notes.peter-baumgartner.net/tags/blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Tutorials explaining from scratch about how to develop your website using blogdown.&lt;/li&gt;
&lt;li&gt;Tutorial 1 explains about website themes and setting up R and Rstudio.&lt;/li&gt;
&lt;li&gt;Tutorial 2 is about hosting the website locally or sharing the work with others.&lt;/li&gt;
&lt;li&gt;Tutorial 3 will be information about getting the site live.&lt;/li&gt;
&lt;li&gt;Tutorial 4 describes how to bring the website online through Netlify or GitHub.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;academic-by-george-cushen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“Academic by George Cushen”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the information about the academic theme this is very popular among people in rstats community.&lt;/li&gt;
&lt;li&gt;Descriptive amount of information about changing elements in the academic theme to make it more homely fo the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;happy-git-and-github-for-the-user.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;*“Happy Git and GitHub for the User.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Git and GitHub with R with all the information that anyone needs to know.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;*“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Book with all the information related to Rmarkdown files.&lt;/li&gt;
&lt;li&gt;No need to look anywhere for clarification regarding Rmarkdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;*“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar to the previous two books this is also the most useful for blogdown.&lt;/li&gt;
&lt;li&gt;No need to look anywhere else for further understanding regarding blogdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-personal-website-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;My Personal Website / Blog&lt;/h1&gt;
&lt;p&gt;Few minute information related to my website / blog will be discussed here. Mostly encouraging other people to make necessary changes in the original template to satisfy their curiosity and interest.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I also used the academic theme but made alot of changes.&lt;/li&gt;
&lt;li&gt;These changes include with a new css, color changing, font changing and others.&lt;/li&gt;
&lt;li&gt;In the Home page I explored adding icons, heading names and changing the header successfully.&lt;/li&gt;
&lt;li&gt;While writing blog posts I was able to learn inventive ways to make them interesting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now that is all I have done in related to developing my own website. It should be your own choice what you are going to develop, a website / blog. Mine is a website where articles are regularly posted. Further, it should be noted that the above materials were active when I was writing this post, therefore if it is not active do use google and try to find it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Olympic : Rshiny Approach</title>
      <link>/post/olympicrshiny/olympic-rshiny-approach/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/olympicrshiny/olympic-rshiny-approach/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#material-useful-for-rsiny-development&#34;&gt;Material Useful for Rsiny Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-use-the-olympic-rshiny-app&#34;&gt;How To Use The Olympic Rshiny App ?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1&#34;&gt;Step 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2&#34;&gt;Step 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3&#34;&gt;Step 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4&#34;&gt;Step 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5&#34;&gt;Step 5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6&#34;&gt;Step 6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7&#34;&gt;Step 7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-8&#34;&gt;Step 8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Rshiny is very popular in the rstats community. The glamourous interface and functionality has helped for this level of popularity. In perspective of using an Rshiny App anyone can use it with minimal amount of knowledge. Which is very useful in bringing statistical analysis to consumers or general public without any trouble.&lt;/p&gt;
&lt;p&gt;I initially wanted to develop an Rshiny App for my fitODBOD package, but I thought it would be best to test the waters. That is what I have done here. Using the Olympic data from kaggle I have found a very convenient way to understand specific results for a choosen country from the Rshiny App.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/screenshots/olympicrshiny.PNG&#34; /&gt; At the beginning I wanted to compare between diferent countries or sports or seasons and come to a conclusion. Well, what kind of a conclusion would make sense bothered me, therefore I turned towards an Rshiny Approach.&lt;/p&gt;
&lt;p&gt;This data-set includes information from 1896 to 2016. Analyzing the data-set would take tedious amount of time and in my opinion unnecessary amount of complications will arise when it comes to concluding. Information from the data-set includes about Medals, participants name, country, sports, events, season and year.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results/home&#34;&gt;Kaggle Olympic Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://amalan-con-stat.shinyapps.io/olympic/&#34;&gt;Olympic Rshiny App&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/Olympic-Data-Rshiny-&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;material-useful-for-rsiny-development&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Material Useful for Rsiny Development&lt;/h2&gt;
&lt;p&gt;Easiest way to build your own shiny app is to refer the official website. It provides extensive amount of information regarding Rshiny development. Already developed Rshiny Apps and Templates are also available, which would come in handy. Further, when you do start an Rshiny App through Rstudio you will initially receive a sample App with its code. A few tweaks and changes would lead to necessary changes that you need.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Official Rshiny Website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-use-the-olympic-rshiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How To Use The Olympic Rshiny App ?&lt;/h2&gt;
&lt;p&gt;Instructions are also listed in the Rshiny App panel.&lt;/p&gt;
&lt;div id=&#34;step-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 1&lt;/h3&gt;
&lt;p&gt;First Choose a country that you want to study and find the three letter NOC CODE from the “NOC CODE” tab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 2&lt;/h3&gt;
&lt;p&gt;Choose the “GRAPH” tab to understand how medals were won for a chosen country over the years with respective to Gender.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 3&lt;/h3&gt;
&lt;p&gt;Choose the “DATA” tab to look at the data for the chosen. Further you can scroll through this data and find specific attendee’s Name, Sex, Age, Year, Season, City, Sport, Event and Medal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 4&lt;/h3&gt;
&lt;p&gt;Using “DESCRIBE” tab you can simply study the descriptive statistics for the data of the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 5&lt;/h3&gt;
&lt;p&gt;“G/Years” tab is there to explain the Gender representation over the years of the chosen country through a bar plot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-6&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 6&lt;/h3&gt;
&lt;p&gt;“S/Years” tab shows a bar plot which has the representation of the Gender of the Sports event participants of the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-7&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 7&lt;/h3&gt;
&lt;p&gt;“H/W/Sport” tab explores how participants Height and Weight relationship for each Sporting event with respective to Gender for the chosen country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-8&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 8&lt;/h3&gt;
&lt;p&gt;Repeat the Steps 1 to 7 and be amused of the results from different countries.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PLEASE NOTE&lt;/em&gt; - You should remember that as a user of this Rshiny Application not all countries have won atleast one medal at the Olympics. At these occurences “MEDAL GRAPH” tab does not show any graph but only an error. This can be confirmed by the “DESCRIBE” tab which will produce the summary for that chosen country.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 4: Prison Data</title>
      <link>/post/tidytuesday2019/week4/week-4-prison-data/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week4/week-4-prison-data/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary&#34;&gt;Prison Summary&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-gender&#34;&gt;Prison Summary With Gender&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-ethnicity&#34;&gt;Prison Summary with Ethnicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prison-summary-with-other-and-total&#34;&gt;Prison Summary with Other and Total&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pretrial-summary-with-gender-and-total&#34;&gt;Pretrial Summary with Gender and Total&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-data-of-incarceration-trends&#34;&gt;Complete Data of Incarceration Trends&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-rural-area&#34;&gt;Rape Crimes over the Years in States of Rural Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-small-or-mid-area&#34;&gt;Rape Crimes over the Years in States of Small or Mid Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-suburban-area&#34;&gt;Rape Crimes over the Years in States of Suburban Area&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rape-crimes-over-the-years-in-states-of-urban-area&#34;&gt;Rape Crimes over the Years in States of Urban Area&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the packages
library(readr)
library(tidyverse)
library(magrittr)
library(gganimate)
library(ggthemr)

# load the theme
ggthemr(&amp;quot;flat dark&amp;quot;)

# load the data
pretrial_summary &amp;lt;- read_csv(&amp;quot;pretrial_summary.csv&amp;quot;)

prison_summary &amp;lt;- read_csv(&amp;quot;prison_summary.csv&amp;quot;)

incarceration_trends&amp;lt;-read_csv(&amp;quot;incarceration_trends.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TidyTuesday Week 4 of 2019 is focused on prison data. You can find the data &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-22&#34;&gt;here&lt;/a&gt;. There are 5 csv files, clearly 2 files are a summary of the main data, which are Prison Summary and Pretrial Summary.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Code: &lt;a href=&#34;https://t.co/x1Wiq1JzYS&#34;&gt;https://t.co/x1Wiq1JzYS&lt;/a&gt;  . opinion: Decline of prisoners rate after certain periods for different regions. &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/VkOJDJJBoq&#34;&gt;pic.twitter.com/VkOJDJJBoq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1087702502240407552?ref_src=twsrc%5Etfw&#34;&gt;January 22, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I have mainly focused on these two data-sets here. Further in order of curiosity I did take a peak at a main data file, which is incarceration_trends.csv.&lt;/p&gt;
&lt;div id=&#34;prison-summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Prison Summary&lt;/h1&gt;
&lt;p&gt;Prison summary data-set has 4 variables. The year of records begin from 1983 and ends in 2015. The unit of incarceration in is Rate per 100,000. Considering the population categories there are clearly 4 sub groups. Each of these sub groups have been plotted here. Further the variable ‘urbanicity’ is simply grouping the observations according to the developed status. Such as rural, small/mid, suburban and urban.&lt;/p&gt;
&lt;div id=&#34;prison-summary-with-gender&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary With Gender&lt;/h2&gt;
&lt;p&gt;The population increase has an effect on it according to the below plot. Urban area has an increase in these prisoners over the years but after mid 1990 there is a decline. This is true for males. Next considering the suburban area this is quite similar as before, only difference is that the decline begins in year 2005.&lt;/p&gt;
&lt;p&gt;Considering rural area there is a clear increase of prisoners for both genders in the years. There is an anomaly in year 1986 with alot of prisoners for males. Both rural and small/mid areas behave similarly for both genders as the increase rate gets somewhat slower after 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category==&amp;quot;Male&amp;quot; | pop_category==&amp;quot;Female&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
  facet_wrap(~urbanicity)+geom_area()+
  transition_reveal(year)+ labs(fill=&amp;quot;Gender&amp;quot;)+
  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
  theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
  scale_y_continuous(breaks = seq(0,1750,250),labels=seq(0,1750,250))+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
  ggtitle(&amp;quot;Gender change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/prison%20summary%20Male%20and%20Female-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prison-summary-with-ethnicity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary with Ethnicity&lt;/h2&gt;
&lt;p&gt;5 ethnicity types are considered here which are Asian, Black, Latino, White and Native American. From 1980 only we can see the active prisoners of Latino and Native American ethnicity. Over the year we can the increase of prisoners for African American Community. The increase is very high considering the other ethnicity types.&lt;/p&gt;
&lt;p&gt;Asian ethnicity people have prisoners but it is only negligible considering the other ethnicity types. Except the suburban area others have an increase rate until 2005 and there is a decline followed in the next years. This is not the case for suburban area. Here less change after year 2000 and the decline begins only in year 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category == &amp;quot;Asian&amp;quot; | pop_category==&amp;quot;Black&amp;quot; |
                      pop_category == &amp;quot;Latino&amp;quot; |pop_category==&amp;quot;White&amp;quot; |
                      pop_category == &amp;quot;Native American&amp;quot; ) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
  facet_wrap(~urbanicity)+geom_area()+
  transition_reveal(year)+ labs(fill=&amp;quot;Ethnicity&amp;quot;)+
  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
  theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
  scale_y_continuous(breaks = seq(0,5250,250),labels=seq(0,5250,250))+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
  ggtitle(&amp;quot;Ethnicity change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/prison%20summary%20Ethnicity-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prison-summary-with-other-and-total&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prison Summary with Other and Total&lt;/h2&gt;
&lt;p&gt;The Other category is no longer active since 1989, but before that we can see different anomalies in all four areas. Clearly urban area has more prisoners and final place goes to suburban according to the below area plot.&lt;/p&gt;
&lt;p&gt;Rural area has an increase in prisoners over the years and there is no decline. This is not the case for small/mid and suburban areas. Urban areas has a sudden decline in between year 1995 to 2000 and again there is a steep decline after year 2005.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(prison_summary,pop_category == &amp;quot;Other&amp;quot; | pop_category==&amp;quot;Total&amp;quot;) %&amp;gt;%
ggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+
    facet_wrap(~urbanicity)+geom_area()+
    transition_reveal(year)+ labs(fill=&amp;quot;Category&amp;quot;)+
    scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+
    theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
    scale_y_continuous(breaks = seq(0,1000,100),labels=seq(0,1000,100))+
    xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
    ggtitle(&amp;quot;Total and Other category change over the years from 1983-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/prison%20summary%20Other%20and%20Total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pretrial-summary-with-gender-and-total&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pretrial Summary with Gender and Total&lt;/h1&gt;
&lt;p&gt;Year 1970 to 2015 is the range of time considered here and the genders male and female are considered with the total. The data-set is for Pretrial prisoners. There is sudden increase after mid 1980s to all the areas. This sudden increase occurs to both genders and the total as well.&lt;/p&gt;
&lt;p&gt;Here, also we can see an odd behavior for urban area in the time entire range with sudden steeps and peaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(pretrial_summary,aes(x=year,y=rate_per_100000,fill=pop_category))+
      geom_area()+facet_wrap(~urbanicity)+
      transition_reveal(year)+ labs(fill=&amp;quot;Category&amp;quot;)+
      scale_x_continuous(breaks=c(1970:2015),labels=c(1970:2015))+
      theme(axis.text.x = element_text(angle = 90),legend.position = &amp;quot;bottom&amp;quot;)+
      scale_y_continuous(breaks = seq(0,800,50),labels=seq(0,800,50))+
      xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Rate per 100,000&amp;quot;)+
      ggtitle(&amp;quot;Total and Gender category change over the years from 1970-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/pretrial%20summary%20gender%20and%20total-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-data-of-incarceration-trends&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Complete Data of Incarceration Trends&lt;/h1&gt;
&lt;p&gt;This data-set has all the necessary information related to Incarceration. Further, it includes data for 11 different crime types. Rather than exploring all the crimes I have explored only one crime here, which is Rape.&lt;/p&gt;
&lt;p&gt;There are four areas in concern are rural, suburban, mid/small and urban. 51 states and 4 regions are considered to see the diversity of these prisoners. We have dropped the years from 1970 to 1976, 2015 and 2016 because they had no data. Even the years 1979 and 1993 has missing data but still I am including this in the plot.&lt;/p&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-rural-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Rural Area&lt;/h2&gt;
&lt;p&gt;I developed these plots to understand a patters in the states or regions wise apparently itsvery difficult but still I am keeping these plots here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;rural&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Rural Areas on the Year : {round(frame_time)}&amp;quot;)

animate(p1,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/All%20Data%20Rape%20Crime%20but%20rural-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-small-or-mid-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Small or Mid Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;small/mid&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Small or Mid Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p2,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/All%20Data%20Rape%20Crime%20but%20small%20or%20mid-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-suburban-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Suburban Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;suburban&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Suburban Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p3,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/All%20Data%20Rape%20Crime%20but%20suburban-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rape-crimes-over-the-years-in-states-of-urban-area&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rape Crimes over the Years in States of Urban Area&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p4&amp;lt;-subset(incarceration_trends,year!=&amp;quot;1970&amp;quot; &amp;amp; year!=&amp;quot;1971&amp;quot; &amp;amp; year!=&amp;quot;1972&amp;quot; &amp;amp;
                            year!=&amp;quot;1973&amp;quot; &amp;amp; year!=&amp;quot;1974&amp;quot; &amp;amp; year!=&amp;quot;1975&amp;quot; &amp;amp;
                            year!=&amp;quot;1976&amp;quot; &amp;amp; year!=&amp;quot;2015&amp;quot; &amp;amp; year!=&amp;quot;2016&amp;quot; &amp;amp;
                            urbanicity == &amp;quot;urban&amp;quot;,
       select = c(&amp;quot;year&amp;quot;,&amp;quot;state&amp;quot;,&amp;quot;rape_crime&amp;quot;,&amp;quot;urbanicity&amp;quot;,&amp;quot;region&amp;quot;)) %&amp;gt;%
ggplot(.,aes(x=factor(state),y=rape_crime,color=region))+
      geom_jitter(width = 0.1)+transition_time(year)+
      coord_flip()+ 
      ylab(&amp;quot;Rape Crime&amp;quot;)+xlab(&amp;quot;State&amp;quot;)+
      theme(legend.position = &amp;quot;bottom&amp;quot;)+
      labs(title=&amp;quot;Rape Crime for Urban Areas on the Year: {round(frame_time)}&amp;quot;)

animate(p4,fps=1,duration=38)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week4/2019-01-22-week-4-prison-data_files/figure-html/All%20Data%20Rape%20Crime%20but%20Urban-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 3: Space Agencies and Launches</title>
      <link>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agencies&#34;&gt;AGENCIES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agency-vs-count&#34;&gt;Agency vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#type-vs-count&#34;&gt;Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-vs-count&#34;&gt;Class vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agency-type-vs-count&#34;&gt;Agency Type vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-count&#34;&gt;State Code vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location-vs-count&#34;&gt;Location vs Count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#start-year-and-end-year-vs-agency&#34;&gt;Start Year and End Year vs agency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#launches&#34;&gt;LAUNCHES&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-of-these-missions-vs-category-variables&#34;&gt;Success or Failure of these missions vs Category Variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-launch-year&#34;&gt;Success or Failure vs Launch Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-agency-type&#34;&gt;Success or Failure vs Agency Type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-or-failure-vs-state-code&#34;&gt;Success or Failure vs State Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#state-code-vs-category-over-time-for-success-and-failure&#34;&gt;State Code vs Category Over time for Success and Failure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(tidyverse)
library(ggalt)
library(magrittr)
library(dplyr)
library(ggthemr)
library(gganimate)

# Load the Agency data
agencies&amp;lt;-read_csv(&amp;quot;agencies.csv&amp;quot;)

# Load the Launches data
launches&amp;lt;-read_csv(&amp;quot;launches.csv&amp;quot;)

attach(agencies)
attach(launches)

# load a theme
ggthemr(&amp;quot;flat dark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;agencies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;AGENCIES&lt;/h1&gt;
&lt;p&gt;Space related agencies of 74 are in the world from this data set. Another, data set is for launches from the agencies in concern. In the agencies data set there are 19 variables and launches data set has 11 variables. You can find the data set and information regarded to it &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-15&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Early years of space launches had more mistakes and they were owned by the state. After the cold war, there is short of enthusiasm, but now we have an increase in launches. Code: &lt;a href=&#34;https://t.co/R0BLjFOH4U&#34;&gt;https://t.co/R0BLjFOH4U&lt;/a&gt;    &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/6dX338hpPD&#34;&gt;pic.twitter.com/6dX338hpPD&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1085387691393458179?ref_src=twsrc%5Etfw&#34;&gt;January 16, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week3&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;agency-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency vs Count&lt;/h2&gt;
&lt;p&gt;Most amount of launches are from Rakentiye Voiska Strategicheskogo Naznacheniye (RVSN) and it is 1528 and next is Upravleniye Nachalnika Kosmicheskikh Sredstv (UNKS) with 904. Top 10 places considering the most launches it clear that class D agencies has the most amount of 5, 3 class C agencies and the rest with class B. NASA is in third place with 469 launches and it is a class C agency. My favorite agency Space X (SPX) has launched 65 times and it is a class B agency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=class,fill=class))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(label=agencies$count, hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
       ggtitle(&amp;quot;Space Agency vs Frequency By Class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20vs%20Count%20and%20class-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, for the same bar plot if we change color according to agency type we have different insight. Top 10 agencies is 90% filled with state ownership and 10% is with private ownership. It should be noted that there are only two start-ups and close to 10 have private ownership, rest is state owned. The highest amount of launches for a private ownership is from Arian Space(AE) and for start-up its Space X (SPX). Respectively, their counts are 258 and 65.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(agencies,aes(x=fct_inorder(agency),y=count,
                    color=agency_type,fill=agency_type))+
       geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.75)+coord_flip()+
       geom_text(aes(label=count), hjust=-0.15)+
       xlab(&amp;quot;Space Agency&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
      labs(color=&amp;quot;Agency Type&amp;quot;,fill=&amp;quot;Agency Type&amp;quot;)+
      ggtitle(&amp;quot;Space Agency vs Frequency By Agency Type&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20vs%20Count%20and%20agency%20type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Type vs Count&lt;/h2&gt;
&lt;p&gt;Type of agencies is very complex because an agency can play multiple roles. Highest amount of count is for O/LA type with 3227 and second count is for LA type with 821 counts. There are 145 agencies with the highest combination of types this category is O/LA/LV/PL/E/S.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(type) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),hjust=-0.15)+coord_flip()+
  xlab(&amp;quot;Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Type vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;class-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class vs Count&lt;/h2&gt;
&lt;p&gt;Class C and B has similar amounts of count which is close to 1100 and most launches are from D class agencies with the count of 3584.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;class&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(class) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(class),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Class&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Class vs Frequency&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Class%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;agency-type-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Agency Type vs Count&lt;/h2&gt;
&lt;p&gt;In perspective of agency type there are 4765 state owned launches, but only 67 launches from start-ups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;agency_type&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(agency_type) %&amp;gt;% 
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(agency_type),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+
  geom_text(aes(label=count),vjust=-0.15)+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Agency%20Type%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Count&lt;/h2&gt;
&lt;p&gt;Close to 2500 missions were launched by Soviet Union and 1709 were done by Unite States.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;state_code&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(state_code) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(state_code),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/State%20Code%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location-vs-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Location vs Count&lt;/h2&gt;
&lt;p&gt;More than 1500 launches are from Mosvka? and exactly 1204 launches from Moskva. Further, 469 launches from Washington D.C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agencies[,c(&amp;#39;location&amp;#39;,&amp;#39;count&amp;#39;)] %&amp;gt;% group_by(location) %&amp;gt;%
  summarise_each(funs(sum)) %&amp;gt;% arrange(count) %&amp;gt;%
ggplot(.,aes(fct_inorder(location),count))+
  geom_bar(stat = &amp;quot;identity&amp;quot;)+ coord_flip()+
  geom_text(aes(label=count),hjust=-0.15)+
  xlab(&amp;quot;Location&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Location vs Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Location%20vs%20Count-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;start-year-and-end-year-vs-agency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Start Year and End Year vs agency&lt;/h2&gt;
&lt;p&gt;Below is a Dumbbell plot to see at the agencies which are no longer active. Before 1960 there was very small activity and they are all owned by the state. With the American and Russian Space race we have private sector also being part of this adventure, but most of them are ending their service around the first half of 1990. There is more activity after this regularly but they are short lived for these agencies. Royal Aircraft Establishment (RAE) has long life for space adventure which was begun around late 1915, and ends its service in around 1990.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;-&amp;quot; &amp;amp; 
                 substr(tstop,1,4) != &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=as.numeric(substr(tstart,1,4)),xend=as.numeric(substr(tstop,1,4)),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 2.75,size=1.25)+ 
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+ 
  scale_x_continuous(breaks=seq(1910,2020,5),labels=seq(1910,2020,5))+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If We Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Start%20year%20and%20End%20Year%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It should be effectively noted that “-” mean still active and “*&amp;quot; means unknown in my perspective. Here we cannot consider the years as numeric because of the characters used. Agencies like NASA and Space X are still active according to my knowledge therefore I considered the above assumption for characters. Most of these agencies are state owned and after Space X there is Rocket Lab USA (RLABU). Most of these agencies were launched after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(agencies, substr(tstart,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;-&amp;quot; | 
                 substr(tstop,1,4) == &amp;quot;*&amp;quot; ) %&amp;gt;%
ggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),
           x=substr(tstart,1,4),
           xend=substr(tstop,1,4),
           fill=agency_type,color=agency_type))+
  geom_dumbbell(size_x = 2,size_xend = 3,size=1.25)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Agency&amp;quot;)+
  labs(fill=&amp;quot;Agency Type&amp;quot;,color=&amp;quot;Agency Type&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle(&amp;quot;Start Year and End Year vs Agency If Do Not Know When&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Start%20year%20and%20End%20Year%20Unknown%20vs%20Agency-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;launches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;LAUNCHES&lt;/h1&gt;
&lt;p&gt;Counts of above missions are mentioned here thoroughly.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-of-these-missions-vs-category-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Success or Failure of these missions vs Category Variables&lt;/h2&gt;
&lt;p&gt;There are few categorical variables which could be associated with the success or failure of these missions.&lt;/p&gt;
&lt;div id=&#34;success-or-failure-vs-launch-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Launch Year&lt;/h3&gt;
&lt;p&gt;Less mistakes over the year with technologies improving and in between 1960 to 1990 we can see alot of launches always above 100 per year. This enthusiasm no longer exists until 2005. After 2005 there is positive increase in launches and failures also less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=factor(launch_year),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;Years&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Years vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,150,10),breaks=seq(0,150,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20Launch%20Year-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-agency-type&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs Agency Type&lt;/h3&gt;
&lt;p&gt;State owned agencies has more failures than private and start-ups because it would be costly. More than 4750 launches are from state owned agencies but in them more than 500 launches are failures. Even though private owned agencies has a history from 1990 yet they have an amount of less than 1000 counts for launches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(agency_type)),fill=category))+
  geom_bar()+
  xlab(&amp;quot;Agency Type&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Agency Type vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,5000,250),breaks=seq(0,5000,250))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20Agency%20Type-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-or-failure-vs-state-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Success or Failure vs State Code&lt;/h3&gt;
&lt;p&gt;Soviet Union (SU) and United States (US) has the most dominant appearance in this field. More than 2400 launches from SU and for US it is more than 1700 launches. Failures also considerably higher for SU and US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(launches,aes(x=fct_infreq(factor(state_code)),fill=category))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab(&amp;quot;State Code&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;State Code vs Frequency&amp;quot;)+
  scale_y_continuous(labels=seq(0,2500,100),breaks=seq(0,2500,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/Success%20or%20Failure%20vs%20State%20Code-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;state-code-vs-category-over-time-for-success-and-failure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;State Code vs Category Over time for Success and Failure&lt;/h2&gt;
&lt;p&gt;Animated jitter plot here explains how over the years these launches occur based on States and Success(O) or Failure(F).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(launches,aes(y=category,x=state_code,color=agency_type))+
       geom_jitter()+
       labs(title = &amp;quot;States vs Success or Failure by : {round(frame_time,0)}&amp;quot;,
            x=&amp;quot;State Code&amp;quot;,y= &amp;quot;Success or Failure&amp;quot;)+
       transition_time(launch_year)+ease_aes(&amp;#39;linear&amp;#39;)+
       labs(color=&amp;quot;Agency Type&amp;quot;)

animate(p,fps=2,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week3/2019-01-16-week-3-space-agencies-and-launches_files/figure-html/State%20code%20vs%20Category%20by%20time-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Developing an R package</title>
      <link>/post/yourownpackage/developing-an-r-package/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/yourownpackage/developing-an-r-package/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#credit-to-people-of-r-community&#34;&gt;Credit to People of R Community&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coding-standards-coding-to-understand&#34;&gt;1) Coding Standards (Coding to Understand)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#package-structure&#34;&gt;2) Package Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#description-file&#34;&gt;3) DESCRIPTION file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#readme-file&#34;&gt;4) README file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-directory&#34;&gt;5) /R directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-directory&#34;&gt;6) /data directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tests-directory&#34;&gt;7) /tests directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#man-directory&#34;&gt;8) /man directory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#namsespace-file&#34;&gt;9) NAMSESPACE file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbuildignore-file&#34;&gt;10) .Rbuildignore file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gitignore-file&#34;&gt;11) .gitignore file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-package&#34;&gt;Building the Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributing-the-package&#34;&gt;Distributing the Package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;R package development is no longer as it was before 2010 because now most of the work can be done by just a simple mouse-click or with the use of a function. My intention of writing this blog post is not to give a thorough demonstration of how to develop your own R package. But it will briefly explain the process with the most important steps, and will include valuable blog posts and websites which helped me to develop my own R package &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credit-to-people-of-r-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Credit to People of R Community&lt;/h1&gt;
&lt;p&gt;I would definitely recommend to read all of these books and start your package development. Or at least make step by step progress in your work while reading them. If you have a basic knowledge regarding R, R studio, CRAN and writing programs, they are more than enough for you to start.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R packages by Hadley Wickam&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This website contains everything that is in the book. The basic things related to an R package development process are structured properly here. It is very useful to read this book/website. The package structure, the type of files necessary, how should the writing be in these files and further, what kind of ways can we use to achieve the final outputs. The book is mainly focused on producing an R package which can be updated with the highest standards using reproducing ability. Such as CRAN standards and GitHub releases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;Writing R Extensions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything related to package development is included here with clear instructions. This document is provided by the CRAN project to make package development more friendlier. It includes the official standards for files and naming conventions related to R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf&#34;&gt;Creating R packages: A Tutorial by Friedrich Leish&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A brief article explaining about writing functions, classes and methods for R package development. This is very abstract and useful in package development specially as it is focusing on object oriented programming and S formulas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf&#34;&gt;R for Beginners by Emmanuel Paradis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A book explaining R and its ability in detail, for example regarding functions, data, abilities and limitations of R. There is also a section for R packages, which has valuable information. Writing functions is very crucial in R package development therefore going through this document is worth. Several packages also include data-sets in them. While you develop functions similarly we can develop data-sets as well. There are sections which includes information regarding data-sets as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf&#34;&gt;The Art of R Programming by Norman Matloff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another book that will be useful in understanding how R functions and data-sets can be used in R package development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.burns-stat.com/pages/Tutor/R_inferno.pdf&#34;&gt;The R Inferno by Patrick Burns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Useful book related to object oriented programming, functions and data objects related to R. Very thorough and scrutinized information with valuable explanation which makes things more clearer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;Cheat Sheets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Easy implementation of packages mentioned in these cheatsheets. Very essential for someone who is interested in doing R related stuff efficient and eloquent.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/&#34;&gt;R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes related to RMarkdown, very useful for vignette building.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Happy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using GitHub for package development is very useful, specially when it comes to sharing and version control. This book explains it all with simplicity.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:900px;height:480px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph TB;\n           A(Code with Standards and comment)--&gt;C(Create R package);\n           C--&gt;D(R package &lt;br&gt; Structure);\n           D--&gt;E(DESCRIPTION file);\n           D--&gt;F(README file);\n           D--&gt;G(/R directory);\n           D--&gt;H(/data directory);\n           D--&gt;I(/tests directory);\n           D--&gt;J(/man directory);\n           D--&gt;K(NAMESPACE file);\n           D--&gt;L(/vignettes &lt;br&gt; directory);\n           D--&gt;M(NEWS.md file);\n           E--&gt;O(Build &lt;br&gt; the package);\n           O--&gt;N(Source,Bundle,Binary,Installed,In Memory);\n           F--&gt;O; G--&gt;O; H--&gt;O; I--&gt;O; J--&gt;O; K--&gt;O;  L--&gt;O; M--&gt;O;&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;coding-standards-coding-to-understand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1) Coding Standards (Coding to Understand)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Focus on naming conventions.&lt;/li&gt;
&lt;li&gt;Focus on input parameters and outputs.&lt;/li&gt;
&lt;li&gt;Focus on indentation.&lt;/li&gt;
&lt;li&gt;Comment regularly to make sense of the functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure1.PNG&#34; /&gt; Sample Code&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;package-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2) Package Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very Important.&lt;/li&gt;
&lt;li&gt;Initially few files be originated in the designated project folder.&lt;/li&gt;
&lt;li&gt;Over time we might add folders or create files manually.&lt;/li&gt;
&lt;li&gt;Example - tests directory, README.Rmd, …&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure1.PNG&#34; /&gt; Package structure inside your project folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;description-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3) DESCRIPTION file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;File explaining basic things related to your package.&lt;/li&gt;
&lt;li&gt;Example - package name, other packages needed, authors name, …&lt;/li&gt;
&lt;li&gt;Can edit manually or use specific R package.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure1.PNG&#34; /&gt; After changes the DESCRIPTION file&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readme-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4) README file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very much optional.&lt;/li&gt;
&lt;li&gt;Only used in related to GitHub submission.&lt;/li&gt;
&lt;li&gt;Using Rmarkdown to generate a GitHub output document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure1.PNG&#34; /&gt; Rmarkdown document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure2.PNG&#34; /&gt; GitHub document&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/readme/figure3.PNG&#34; /&gt; Preview of GitHub document&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5) /R directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most important directory.&lt;/li&gt;
&lt;li&gt;The place where all your R code is written by you.&lt;/li&gt;
&lt;li&gt;Best to have separate R script files for each function.&lt;/li&gt;
&lt;li&gt;Need to have a R script file for Data as well.&lt;/li&gt;
&lt;li&gt;R scripts can be modified further in order to create RDocumentation files(Rd files).&lt;/li&gt;
&lt;li&gt;These RDocumentation files will explain about the function.&lt;/li&gt;
&lt;li&gt;Processed R script files will automatically generate Rd files in the man directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure2.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure3.PNG&#34; /&gt; R script file with necessary roxygen tags to develop RDocumentation files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6) /data directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Not compulsory.&lt;/li&gt;
&lt;li&gt;Easy to use your own data therefore its worth it.&lt;/li&gt;
&lt;li&gt;This directory will include the data-sets.&lt;/li&gt;
&lt;li&gt;R directory can have an R script to generate Rd files for these data-sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure3.PNG&#34; /&gt; data directory which includes data-sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure1.PNG&#34; /&gt; Rscript file which includes necessary roxygen tags to generate Rd files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;7) /tests directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If your package is going to be in CRAN or going to be in a platform with large range of users this would be useful.&lt;/li&gt;
&lt;li&gt;Unit tests to check if functions are working properly.&lt;/li&gt;
&lt;li&gt;Testing if the data sets are in proper form.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure4.PNG&#34; /&gt; tests directory and files in side that directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/directory/figure5.PNG&#34; /&gt; sub directory testthat which includes test R scripts for all functions and data sets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure1.PNG&#34; /&gt; R script to test a function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/checktest/figure2.PNG&#34; /&gt; R script to test a data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;man-directory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;8) /man directory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This directory will include the Rd files for all functions and data sets.&lt;/li&gt;
&lt;li&gt;If you use roxygen tags there is no need to manually type them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure6.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure7.PNG&#34; /&gt; With the help of R script files these RDocumentation files will be generated for each function and will be in the man directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/codingstandards/figure4.PNG&#34; /&gt; &lt;img src=&#34;/YourOwnPackage/codingstandards/figure5.PNG&#34; /&gt; The RDocumentation files can be processed into html outputs or into a pdf manual.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;&#34; /&gt; Rd file of a data-set which is created with the help of data R script.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/data/figure2.PNG&#34; /&gt; Html file which is generated with the help of Rd file for the data-set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;namsespace-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;9) NAMSESPACE file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A file which will have all the functions that you created for your package.&lt;/li&gt;
&lt;li&gt;If a function is exported then it will be in this file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure2.PNG&#34; /&gt; NAMESPACE file and its components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rbuildignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;10) .Rbuildignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be used when building the package.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure3.PNG&#34; /&gt; .Rbuildignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gitignore-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;11) .gitignore file&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A document which includes what kind of files should not be pushed to the GitHub repository.&lt;/li&gt;
&lt;li&gt;Extensions of a file or partial or full name of the file can be added into this document.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/YourOwnPackage/otherfiles/figure4.PNG&#34; /&gt; .gitignore file of fitODBOD package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building the Package&lt;/h1&gt;
&lt;p&gt;All the files should be in their respective directories and names should not changed for files or folders manually if they are created automatically. After checking all of this we can proceed to building the package. This process has 9 steps and below is a diagram to show how it works.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:1000px;height:300px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;graph LR;\n           A(Document &lt;br&gt; Generation)--&gt;C(Clean and &lt;br&gt; Rebuild);\n           C--&gt;D(Spellcheck &lt;br&gt; Rd files);\n           D--&gt;B((Check for &lt;br&gt; issues));\n           B--&gt;Z((Make Necessary &lt;br&gt; Changes)); Z--&gt;A;\n           D--&gt;E(Test &lt;br&gt; the Package); E--&gt;B;\n           E--&gt;F(Check &lt;br&gt; for Errors); F--&gt;B;\n           F--&gt;G(Build &lt;br&gt; Source Package);\n           G--&gt;H(Build &lt;br&gt; Binary Package);\n           H--&gt;I(Generate &lt;br&gt; Manual pdf);\n           I--&gt;J(Check Errors &lt;br&gt; on Source Package); J--&gt;B&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;This process is explained through a small presentation &lt;a href=&#34;/YourOwnPackage/BuildYourOwnPackage.html&#34;&gt;here&lt;/a&gt;. This presentation also can be used to when you need to update your package version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributing-the-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Distributing the Package&lt;/h1&gt;
&lt;p&gt;There are several ways of distributing your package. They are mainly&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;tar.qz version.&lt;/li&gt;
&lt;li&gt;zip version.&lt;/li&gt;
&lt;li&gt;GitHub Repository.&lt;/li&gt;
&lt;li&gt;The project folder which includes the package.&lt;/li&gt;
&lt;li&gt;Submit to CRAN or Bioconductor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I have written two posts related to R packages as well. One is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/findrpackage/how-to-find-your-r-package/&#34;&gt;How to find your R package&lt;/a&gt; and Second is &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/newpackage/build-a-new-package-with-existing-r-packages/&#34;&gt;Using R packages to develop your own package&lt;/a&gt;. These two posts will be very useful.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 2: IMDB TV Shows Data</title>
      <link>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre&#34;&gt;Genre&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-season&#34;&gt;Genre and Season&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-year&#34;&gt;Genre and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genre-and-month&#34;&gt;Genre and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season&#34;&gt;Season&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-year&#34;&gt;Season and Year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#season-and-month&#34;&gt;Season and Month&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-3-genres&#34;&gt;Top 3 Genres&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#crime-drama-mystery&#34;&gt;Crime Drama Mystery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comedy-drama&#34;&gt;Comedy Drama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drama&#34;&gt;Drama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rating-over-the-years&#34;&gt;Rating over the years&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tv-series-with-more-than-14-seasons&#34;&gt;Tv Series with more than 14 Seasons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the packages
library(tidyverse)
library(summarytools)
library(magrittr)
library(readr)
library(lubridate)
library(gganimate)
library(stringr)

# load the dataset
Ratings &amp;lt;- read_csv(&amp;quot;IMDb_Economist_tv_ratings.csv&amp;quot;, 
                    col_types = cols(date = col_date(format = &amp;quot;%Y-%m-%d&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08&#34;&gt;Ratings&lt;/a&gt; data-set is from the IMDB site. I just found out that IMDb is active from 1990, that is a very long time and new information to me.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The code in GitHub: &lt;a href=&#34;https://t.co/ITQnSRH7Ot&#34;&gt;https://t.co/ITQnSRH7Ot&lt;/a&gt;  &lt;br&gt;.Week 2 of 2019.  Rating over the year and changes in sharing.  &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; &lt;a href=&#34;https://t.co/xP5F7PWDFV&#34;&gt;pic.twitter.com/xP5F7PWDFV&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1082693109304168448?ref_src=twsrc%5Etfw&#34;&gt;January 8, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
 &lt;a href=&#34;&amp;#39;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/week2&#34;&gt;GitHub code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TV shows from 1990 to 2018 with their ratings, genres, sharing and aired dates is in this data-set. I wanted a cool function to summarize the data-set at once, therefore scroll through the internet and found the package &lt;a href=&#34;https://github.com/dcomtois/summarytools&#34;&gt;summarytools&lt;/a&gt;. There are quite a few functions in the mix, yet I choose dfSummary.&lt;/p&gt;
&lt;p&gt;Below is the code of using that function on the Ratings data-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Basic summary of all variables
dfSummary(Ratings,style = &amp;#39;grid&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Frame Summary   
## Ratings     
## **Dimensions:** 2266 x 7     
## **Duplicates:** 1   
## 
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | No | Variable      | Stats / Values                 | Freqs (% of Valid)   | Text Graph                             | Valid  | Missing |
## +====+===============+================================+======================+========================================+========+=========+
## | 1  | titleId       | 1. tt0098844                   | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. tt0203259                   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. tt0118401                   | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. tt0108757                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. tt0247082                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. tt0413573                   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. tt0452046                   | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. tt0118375                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. tt0460681                   | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. tt0460627                  | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 866 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 2  | seasonNumber  | mean (sd) : 3.26 (3.44)        | 27 distinct values   | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 1 &amp;lt; 2 &amp;lt; 44                     |                      | :                                      |        |         |
## |    |               | IQR (CV) : 3 (1.05)            |                      | :                                      |        |         |
## |    |               |                                |                      | : .                                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 3  | title         | 1. Law &amp;amp; Order                 | 20 ( 0.9%)           |                                        | 2266   | 0       |
## |    | [character]   | 2. Law &amp;amp; Order: Special Vict   | 20 ( 0.9%)           |                                        | (100%) | (0%)    |
## |    |               | 3. Midsomer Murders            | 19 ( 0.8%)           |                                        |        |         |
## |    |               | 4. CSI: Crime Scene Investig   | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 5. ER                          | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 6. Grey&amp;#39;s Anatomy              | 15 ( 0.7%)           |                                        |        |         |
## |    |               | 7. Criminal Minds              | 14 ( 0.6%)           |                                        |        |         |
## |    |               | 8. King of the Hill            | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 9. Supernatural                | 13 ( 0.6%)           |                                        |        |         |
## |    |               | 10. Bones                      | 12 ( 0.5%)           |                                        |        |         |
## |    |               | [ 858 others ]                 | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII                     |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 4  | date          | min : 1990-01-03               | 1808 distinct val.   |                   :                    | 2266   | 0       |
## |    | [Date]        | med : 2012-12-07               |                      |                 . :                    | (100%) | (0%)    |
## |    |               | max : 2018-10-10               |                      |               . : :                    |        |         |
## |    |               | range : 28y 9m 7d              |                      |           . : : : :                    |        |         |
## |    |               |                                |                      | . . . . : : : : : :                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 5  | av_rating     | mean (sd) : 8.06 (0.67)        | 1997 distinct values |               :                        | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      |               : :                      | (100%) | (0%)    |
## |    |               | 2.7 &amp;lt; 8.11 &amp;lt; 9.68              |                      |               : :                      |        |         |
## |    |               | IQR (CV) : 0.76 (0.08)         |                      |             . : :                      |        |         |
## |    |               |                                |                      |             : : : .                    |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 6  | share         | mean (sd) : 1.28 (3.38)        | 454 distinct values  | :                                      | 2266   | 0       |
## |    | [numeric]     | min &amp;lt; med &amp;lt; max :              |                      | :                                      | (100%) | (0%)    |
## |    |               | 0 &amp;lt; 0.32 &amp;lt; 55.65               |                      | :                                      |        |         |
## |    |               | IQR (CV) : 0.99 (2.64)         |                      | :                                      |        |         |
## |    |               |                                |                      | :                                      |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+
## | 7  | genres        | 1. Crime,Drama,Mystery         | 369 (16.3%)          | III                                    | 2266   | 0       |
## |    | [character]   | 2. Comedy,Drama                | 174 ( 7.7%)          | I                                      | (100%) | (0%)    |
## |    |               | 3. Drama                       | 168 ( 7.4%)          | I                                      |        |         |
## |    |               | 4. Action,Crime,Drama          | 146 ( 6.4%)          | I                                      |        |         |
## |    |               | 5. Action,Adventure,Drama      | 112 ( 4.9%)          |                                        |        |         |
## |    |               | 6. Crime,Drama                 | 107 ( 4.7%)          |                                        |        |         |
## |    |               | 7. Drama,Romance               | 86 ( 3.8%)           |                                        |        |         |
## |    |               | 8. Comedy,Crime,Drama          | 80 ( 3.5%)           |                                        |        |         |
## |    |               | 9. Comedy,Drama,Romance        | 76 ( 3.4%)           |                                        |        |         |
## |    |               | 10. Crime,Drama,Thriller       | 63 ( 2.8%)           |                                        |        |         |
## |    |               | [ 87 others ]                  | 885 (39.1%)          | IIIIIII                                |        |         |
## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basic summary of seasons indicate 27 distinct values and obviously 1 is the minimum value, but the maximum value is 44. Another odd thing is median for being 2, further the average is 3.26. Which means most of the TV shows have only up-to few seasons.&lt;/p&gt;
&lt;p&gt;Summary of date indicates the earliest TV show from 1990 and latest from 2018. So the year difference is 28 years, but there are only 1808 distinct values. Even though the data-set contains 2266 observations. Some TV shows might have to start on the same day, that is only plausible conclusion.&lt;/p&gt;
&lt;p&gt;“av_rating” (I presume Audio/Video Rating) is in the scale from 1 to 10, where people had influence. The least rating value is 2.7 and the most rating value is 9.68, but the median is 8.11. Also the mean is 8.06. We can say most of these TV shows are excellent to watch. There is another numeric variable called share and the value ranges from 0 to 55.65 where the average is 1.28.&lt;/p&gt;
&lt;div id=&#34;genre&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Genre&lt;/h1&gt;
&lt;p&gt;Genres in this data-set has close to 100 distinct factors. The category “Crime,Drama,Mystery” holds the highest percentage of 16.3, while second place is to “Comedy,Drama” with 7.7% and third place is to “Drama” type with 7.4%. All the other types of genre is represented by less than 7%.&lt;/p&gt;
&lt;div id=&#34;genre-and-season&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Season&lt;/h2&gt;
&lt;p&gt;Genre and Season are two categorical variables which should be compared to find out if over time do people like the same genre type. If we look at the bar plot it is clear “Crime,Drama,Mystery” type has seasons from 1 to 20. Mostly in all genres there is clear sign of TV shows with seasons up-to three or four. Some of them make it to season ten or eleven, for example genres like “Drama”, “Drama,Thriller”, “Animation,Comedy,Drama” and “Adventure,Drama,Family”.&lt;/p&gt;
&lt;p&gt;Oddly in “Drama,Romance” and “Crime,Drama” there are TV shows which has seasons above 35 but very few. It becomes more weird where for the same genre types the seasons in-between 25 and 34 are missing. Clearly in the legend also until season 20 there is continuity, but this does not carry on for higher seasons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre and Seasons&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Season-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Year&lt;/h2&gt;
&lt;p&gt;Bar plot indicates that after 2014 around 90% of these genre type TV shows have been done. In the top ten category according to the counts of TV shows clearly all of these genres have been active since 1990 to now. Some of them were started in mid 1990s which include the genre types “Action,Crime,Drama”, “Crime,Drama” and “Comedy,Crime,Drama”.&lt;/p&gt;
&lt;p&gt;Types such as “Drama,Romance,Sport” and “Adventure,Drama,Romance” were in active in the mid 2000s but no longer. Genres such as “Drama,History”, “Drama,Horror,Thriller” and “Action,Drama” are a few of them which were popular after 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(year(date))))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Year&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Year&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genre-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Genre and Month&lt;/h2&gt;
&lt;p&gt;Month of airing might have an influence on the TV shows. Bar plot indicates that most of the TV shows are aired in the first quarter or last quarter of the year. Which means shows aired in the fall (September or October) or aired after winter break (January).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(month(date))))+
  geom_bar()+ coord_flip()+ labs(fill=&amp;quot;Month&amp;quot;)+
  xlab(&amp;quot;Genre&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Genre Over the Months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Genre%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;season&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Season&lt;/h1&gt;
&lt;p&gt;TV shows may run for few seasons or more most of the time. Some are limited seasons close to four or above, but definitely less than 10. It is very rare to see TV shows going beyond the 15 seasons mark.&lt;/p&gt;
&lt;div id=&#34;season-and-year&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Year&lt;/h2&gt;
&lt;p&gt;Clearly there is an increase in TV shows aired over the years. All time low occurs in 1992 but all time high occurs in 2017. From 1990 to 2010 the TV shows aired have increased from 25 to 100. By the end of year 2017 the number of shows aired has reached more than 250, but its drops to slightly above 175 the next year. The all time low of less than 12 seasons occurs in 1990.&lt;/p&gt;
&lt;p&gt;In the years 1990,1996,2005,2007,2010,2011 and 2015 there are TV shows which has season above 30, but it should be reminded that according to the legend after season 20 there is no continuity.&lt;/p&gt;
&lt;p&gt;If we focus closely until 2005 most of the TV shows have seasons up-to 10 , but after 2005 there are TV shows which aired season until 20. This shows the popularity of certain shows over three decades.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(year(date)),fill=factor(seasonNumber)))+
  geom_bar()+ coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Year&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the Years&amp;quot;)+
  scale_y_continuous(breaks=seq(0,230,10),labels=seq(0,230,10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Season%20vs%20Year-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;season-and-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Season and Month&lt;/h2&gt;
&lt;p&gt;Highest amount of more than 500 shows were aired in January and lowest amount of slightly less than 100 was aired in June. Second place goes to February with shows close to 200 being aired this is not even the half of what aired in the previous month. In the months of January, February and September most of the seasons were aired, while in the other months the seasons aired are from the range of 1 to 10. Where very few of them ever reached the double digits or above season 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ratings,aes(x=factor(month(date)),fill=factor(seasonNumber)))+
  geom_bar()+coord_flip()+labs(fill=&amp;quot;Season&amp;quot;)+
  xlab(&amp;quot;Month&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
  ggtitle(&amp;quot;Seasons over the months&amp;quot;)+
  scale_y_continuous(breaks=seq(0,550,25),labels=seq(0,550,25))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Season%20vs%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;top-3-genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top 3 Genres&lt;/h1&gt;
&lt;p&gt;Let focus on the most mentioned genre types which are “Crime,Drama,Mystery”, “Comedy,Drama” and “Drama”. The below section is to graphically represent the TV shows of a certain genre with its seasons and when they were aired.&lt;/p&gt;
&lt;p&gt;Therefore I will not be factual, mostly biased towards the shows I watched and special characteristics.&lt;/p&gt;
&lt;div id=&#34;crime-drama-mystery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crime Drama Mystery&lt;/h2&gt;
&lt;p&gt;Law and Order of 20 seasons has been over for more than 5 years and it began airing in 1990. Law and Order Special Victims Unit started airing in 1999 even now its still being aired. There are also odd shows like which has not aired continuously and skipped an year or two. Among them Columbo, Agatha Christie’s Marple and II commissario Montalbano are specially noted.&lt;/p&gt;
&lt;p&gt;There are lot of shows which only aired one or two seasons only and then stopped. They also can be noted from the bar plot. In the legend there are colors to indicate all the years from 1990 to 2018 through color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Crime,Drama,Mystery&amp;quot;) %&amp;gt;%
  ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Crime,Drama,Mystery&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:20,labels=0:20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Crime%20Drama%20Mystery-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comedy-drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comedy Drama&lt;/h2&gt;
&lt;p&gt;Similarly as above we can interpret the bar plot as well. But here the highest amount of seasons any TV show has reached is 9. Only the TV shows “Scrubs” and “Shameless” have reached that milestone and both of them begin in different decades. “Scrubs” began in early 2000s, but “Shameless” was aired after 2010.&lt;/p&gt;
&lt;p&gt;In the years 1998 and 1999 there were no TV shows aired under the genre “Comedy,Drama”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Comedy,Drama&amp;quot;) %&amp;gt;%
    ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Comedy,Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:9,labels=0:9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Comedy%20and%20Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drama&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drama&lt;/h2&gt;
&lt;p&gt;Most of the TV shows in this genre type are limited to one season an a few more with 2 seasons. Even though most of them were aired after 2015. TV shows “Mad Men”, “Skins” and “The West Wing” has aired for 7 seasons.&lt;/p&gt;
&lt;p&gt;Some of these shows were limited series like “The News Room”. The only very early TV show is “Rebel Highway” which was aired in 1994 and in year 2004 “Summerland” was aired, where both of them were limited to one season.&lt;/p&gt;
&lt;p&gt;According to the legend the years 1992,1993,1995 to 1998 were years free of “Drama” genre TV shows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(Ratings,genres==&amp;quot;Drama&amp;quot;) %&amp;gt;%
 ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+
    geom_bar()+ coord_flip()+labs(color=&amp;quot;Year&amp;quot;,fill=&amp;quot;Year&amp;quot;)+
    xlab(&amp;quot;TV shows&amp;quot;)+ylab(&amp;quot;Frequency&amp;quot;)+
    ggtitle(&amp;quot;&amp;#39;Drama&amp;#39; Genre type over the Years&amp;quot;)+
    scale_y_continuous(breaks=0:7,labels=0:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Drama-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rating-over-the-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rating over the years&lt;/h1&gt;
&lt;p&gt;Lets discuss regarding TV shows and their rating over the years with related to sharing. Clearly we can see the number of shows of increasing from 1990 to 2018. Oddly there was more sharing related to movies before 2000, this is into the same over the next years. To be honest sharing becomes more extinct.&lt;/p&gt;
&lt;p&gt;In perspective of rating the range is very much centered and small(between 7.5 - 9), but over the years this changes and expands to a wider range.(6.5 - 9.5). Only a handful of TV shows are rated below 5 in the scale over the year span of 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggplot(Ratings,aes(x=factor(year(date)),y=av_rating,color=genres,size=share))+
      geom_point(show.legend = FALSE)+ 
      labs(title = &amp;quot;Ratings and Sharing : {frame_time}&amp;quot;
           ,x=&amp;quot;Year&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_y_continuous(breaks=2:10,labels=2:10)+
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      theme(axis.text.x = element_text(angle = 90))+
      shadow_mark()

animate(p,fps= 5,duration =60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/Rating%20vs%20Years-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tv-series-with-more-than-14-seasons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tv Series with more than 14 Seasons&lt;/h1&gt;
&lt;p&gt;This is simply me trying to focus on the TV shows which has seasons more than 14 and their rating, sharing changes over the years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-subset(Ratings,title==&amp;quot;CSI: Crime Scene Investigation&amp;quot;|
          title==&amp;quot;ER&amp;quot;| title==&amp;quot;Grey&amp;#39;s Anatomy&amp;quot;| title==&amp;quot;Midsomer Murders&amp;quot;| 
          title==&amp;quot;Law &amp;amp; Order&amp;quot;|
          title==&amp;quot;Law &amp;amp; Order: Special Victims Unit&amp;quot;) %&amp;gt;%
ggplot(aes(x=seasonNumber,y=av_rating,color=title,size=share))+ 
      geom_point()+
      labs(title = &amp;#39;Season and Rating Year: {frame_time}&amp;#39;,
           x=&amp;quot;Season&amp;quot;,y=&amp;quot;Rating&amp;quot;)+
      scale_x_continuous(breaks=1:20,labels=1:20)+
      #scale_y_continuous(breaks=)
      transition_time(date)+ease_aes(&amp;#39;linear&amp;#39;)+
      shadow_mark()+theme(legend.position = &amp;quot;bottom&amp;quot;)

animate(p,fps=5,duration = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week2/2019-01-08-week-2-imdb-tv-shows-data_files/figure-html/More%20than%2014%20Seasons-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2019 Week 1 : #TidyTuesday Tweets </title>
      <link>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidytuesday-tweets&#34;&gt;#Tidytuesday Tweets&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earliest-tweet&#34;&gt;Earliest Tweet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#any-verified-profiles&#34;&gt;Any Verified Profiles ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#source-of-tweets&#34;&gt;Source of Tweets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tweets-per-month&#34;&gt;Tweets Per Month&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name&#34;&gt;Most Tweets By Screen Name&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-and-their-source&#34;&gt;Most Tweets By Screen Name and their Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-retweet-counts&#34;&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-tweets-by-screen-name-with-their-favorite-counts&#34;&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-favorite-counts-vs-retweet-counts&#34;&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relationship-between-followers-count-vs-friends-count&#34;&gt;Relationship between Followers Count vs Friends Count ?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-analysis&#34;&gt;Further Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the necessary packages
library(tidyverse)
library(lubridate)
library(kableExtra)
library(ggthemr)

#load the ggthemr
ggthemr(&amp;quot;flat dark&amp;quot;)

# load the data set
tidytuesday_tweets&amp;lt;-readRDS(&amp;quot;tidytuesday_tweets.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;tidytuesday-tweets&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;#Tidytuesday Tweets&lt;/h1&gt;
&lt;p&gt;Using plots and Tables to express the #TidyTuesday data-set. You can obtain the dataset from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Favorite count vs Retweet count. Code: &lt;a href=&#34;https://t.co/QJwXxzrkFG&#34;&gt;https://t.co/QJwXxzrkFG&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; &lt;a href=&#34;https://t.co/l14pHDS3kq&#34;&gt;pic.twitter.com/l14pHDS3kq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amalan Mahendran (@Amalan_Con_Stat) &lt;a href=&#34;https://twitter.com/Amalan_Con_Stat/status/1080163707601195010?ref_src=twsrc%5Etfw&#34;&gt;January 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Amalan-ConStat/TidyTuesday/tree/master/2019/Week1&#34;&gt;GitHub Code&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;earliest-tweet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Earliest Tweet&lt;/h2&gt;
&lt;p&gt;The first tweet is on April 2nd and it has 156 favorites and 64 retweets, where the tweet is from Thomas Mock and the next 3 tweets are also from him.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[order(tidytuesday_tweets$created_at),c(3,4,13,14,71)] %&amp;gt;%
  head(5) %&amp;gt;%
  kable()  %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
created_at
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:08
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 21:35:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-02 23:31:11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
thomas_mock
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thomas Mock
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-03 00:25:51
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
umairdurrani87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Umair Durrani
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;any-verified-profiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Any Verified Profiles ?&lt;/h2&gt;
&lt;p&gt;There are only 3 verified profiles where Hadley Wickham has the highest amount of followers of 76469, where that tweet has 61 favorites but no retweets. Other profiles are Civis Analytics and grspur, but both of them have friends above 600 counts, but Hadley Wickham friends close to 290.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(tidytuesday_tweets[,c(4,13,14,76,77,82)],verified==TRUE) %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
favorite_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
retweet_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
followers_count
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
friends_count
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
verified
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CivisAnalytics
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
658
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
hadleywickham
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76469
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
288
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
grspur
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
857
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
623
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;source-of-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Source of Tweets&lt;/h2&gt;
&lt;p&gt;Close to 1050 tweets are done by the web client and other clients such as Android and Iphone have tweet counts of respectively 106 and 233. Other sources include oddly Instagram, Facebook, WordPress and LinkedIn, which I am naming because of their popularity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,aes(fct_infreq(source)))+
  geom_bar()+coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.25)+
  ylab(&amp;quot;Frequency&amp;quot;)+xlab(&amp;quot;Types of Sources&amp;quot;)+
  ggtitle(&amp;quot;Source of Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Source%20of%20Tweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets-per-month&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tweets Per Month&lt;/h2&gt;
&lt;p&gt;Beginning of #TidyTuesday we have 293 tweets on the month of April. Even though over the next months the number of tweets are decreasing this is not the case in October. Lowest number of tweets are recorded in September with 115 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets,
       aes(x=month(tidytuesday_tweets$created_at)))+
  geom_bar()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),vjust=-0.15)+
  scale_x_continuous(breaks = seq(1,12),labels = seq(1,12))+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Months&amp;quot;)+
  ggtitle(&amp;quot;Tweet Counts By Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Tweets%20Per%20Month-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Most Tweets By Screen Name&lt;/h2&gt;
&lt;p&gt;There are 30 twitter users if we consider the accounts that have tweeted more than or equal to 10 tweets under the hashtag “TidyTuesday”. Thomas Mock has tweeted most which is 172 including retweets, and the second place goes to R4DScommunity with 92 tweets. All the other users have individually less than 40 tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name)))+
  geom_bar()+ coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=-0.15)+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20screen%20name-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;most-tweets-by-screen-name-and-their-source&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name and their Source&lt;/h3&gt;
&lt;p&gt;For the same plot if we consider the source for the tweets, it is clear that only seven sources were used. Mostly all of these users are using the web client, but some are using the iPhone as well. R4DS community does more tweeting through iPhone than TweetDeck. TweetDeck is a simple way of handling multiple twitter accounts at the same time. Tidyyourworld account only uses Android and WeAreRLadies uses only TweetDeck.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(aes(x=fct_infreq(screen_name),fill=source))+
  geom_bar(position = &amp;quot;stack&amp;quot;,stat=&amp;quot;count&amp;quot;)+ 
  coord_flip()+
  geom_text(stat = &amp;quot;count&amp;quot;,aes(label=..count..),hjust=1,
            position = position_stack())+
  ylab(&amp;quot;Frequency&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Source&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20and%20their%20Source-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-retweet-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Retweet Counts&lt;/h3&gt;
&lt;p&gt;Of the Top 30 users with most amount of tweets the highest amount of retweets is to a tweet from WeAreRLadies and it is 95. There are more outliers from Thomas Mock. and the highest range is to the user drob. Most from this top 30 users have the range between 0 and 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;retweet_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=retweet_count))+
  geom_boxplot()+ coord_flip()+
  scale_y_continuous(breaks = seq(0,100,5),labels = seq(0,100,5))+
  ylab(&amp;quot;Retweets&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Retweets Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Retweets-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-tweets-by-screen-name-with-their-favorite-counts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Tweets By Screen Name with their Favorite Counts&lt;/h3&gt;
&lt;p&gt;Similarly Thomas Mock has more outliers, and the highest range is to the user drob. Second place for outliers goes to R4DScommunity user. Close to 500 favorites are counted to a tweet by drob and second place is to a tweet by WeAreRladies with favorite counts slightly above 450.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidytuesday_tweets[,c(&amp;quot;screen_name&amp;quot;,&amp;quot;favorite_count&amp;quot;)] %&amp;gt;%
  group_by(screen_name) %&amp;gt;%
  filter(n() &amp;gt;= 10) %&amp;gt;%
ggplot(.,aes(x=fct_infreq(screen_name),y=favorite_count))+
  geom_boxplot()+ coord_flip()+
   scale_y_continuous(breaks = seq(0,500,25),labels = seq(0,500,25))+
  ylab(&amp;quot;Favourites Count&amp;quot;)+ xlab(&amp;quot;Screen Name&amp;quot;)+
  ggtitle(&amp;quot;Screen Name with Most Tweets and their Favourties Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Most%20Tweets%20By%20Screen%20name%20with%20Favorite%20counts-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-favorite-counts-vs-retweet-counts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Favorite Counts vs Retweet Counts ?&lt;/h2&gt;
&lt;p&gt;Very clear positive correlation. Y scale ranges from 0 to 500, where x scale range is from 0 to 100 and most of the data points are centered around the range of 0 to 12 retweets and 0 to 60 Favorite. a Few data data points are out of the above mentioned range.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidytuesday_tweets, 
       aes(x=retweet_count,y=favorite_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,100,2) ,labels =seq(0,100,2))+
  scale_y_continuous(breaks =seq(0,500,10),labels =seq(0,500,10))+
  xlab(&amp;quot;Retweets&amp;quot;)+ylab(&amp;quot;Likes&amp;quot;)+
  ggtitle(&amp;quot;Retweets Versus Likes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Scatter%20plot%20between%20favourite%20count%20vs%20retweet%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-followers-count-vs-friends-count&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between Followers Count vs Friends Count ?&lt;/h2&gt;
&lt;p&gt;Here I have considers only twitter profiles which has followers count less than 5000 with friends count also less than 5000. The reason is to explain the relationship more clearly. Clearly most of the twitter profiles are has followers less than 2000 with Followers also less than 2000. Clearly there are some profiles with Followers count above 1000 but friends count less than 1000. Even though there are few profiles with less than 1000 followers but more than 1000 Friends&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(subset(tidytuesday_tweets,
       followers_count &amp;lt; 5000 &amp;amp; friends_count &amp;lt; 5000), 
       aes(x=followers_count,y=friends_count))+
  geom_point()+geom_smooth()+
  scale_x_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  scale_y_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+
  xlab(&amp;quot;Followers Count&amp;quot;)+ylab(&amp;quot;Friends Count&amp;quot;)+
  ggtitle(&amp;quot;Followers Count Versus Friends Count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/TidyTuesday2019/Week1/2019-01-01-week-1-tidytuesday-tweets_files/figure-html/Scatter%20plot%20between%20Followers%20count%20vs%20Friends%20count-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My conclusion of the above plots and tables in point form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using tidyverse as usual is fun.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Box plots for several variables in the same plot is easy for the use of comparison.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Scatter plots are nice to understand the relationship among two continuous variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The geom_smooth function is also very useful in modelling the data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can focus on the text variable which could be used for a word cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further we can try to understand the hashtags with favorites and retweets.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the mle and mle2 function </title>
      <link>/post/mleand2/benchmarking-the-mle-and-mle2-function/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/mleand2/benchmarking-the-mle-and-mle2-function/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mle&#34;&gt;mle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mle2&#34;&gt;mle2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/stats4/versions/3.5.1/topics/mle&#34;&gt;mle&lt;/a&gt; and &lt;a href=&#34;https://www.rdocumentation.org/packages/bbmle/versions/1.0.20/topics/mle2&#34;&gt;mle2&lt;/a&gt; are my favorite functions, because they provide extensive amount of outputs for the optimization process. Even though there is no difference in analytical methods used in both of these functions. Further, these analytical methods are the same ones used by optim function. To be honest mle and mle2 functions are wrapper functions of optim. It means both mle and mle2 are using the optim function inside but with some additional inputs, which would generate extended outputs.&lt;/p&gt;
&lt;p&gt;Even if I do Benchmark the analytical methods for the mle function it would be very similar to optim function tables but with additional time taken, because of the extra outputs. This would similarly occur when we benchmark analytical methods from the mle2 function as well.&lt;/p&gt;
&lt;p&gt;Therefore, I figure why do we need to benchmark them at all. So this blog post is to simply reiterate the initial things which I said in my earlier post on the blog post &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/&#34;&gt;Benchmarking optimization functions in R&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;mle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle&lt;/h2&gt;
&lt;p&gt;mle function is from the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats4/html/00Index.html&#34;&gt;stats4&lt;/a&gt; package. If we intend to use this function for the estimation of shape parameters a and b of the Beta-Binomial distribution when Binomial Outcome Data, then we need to use the EstMLEBetaBin function from the fitODBOD package. This is not enough because for limitations in the mle we need to make changes in our EstMLEBetaBin function as mentioned below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats4)
library(fitODBOD)

#new function to facilitate mle criteria 
formle&amp;lt;-function(a,b)
{
  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)
}

# optimizing values for a,b using default analytial method
mle_answer&amp;lt;-mle(minuslogl = formle,start = list(a=0.1,b=0.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use the Alcohol Consumption data of week 1. In the above code chunk we are using the mle function for our task of finding the optimum shape parameter values for a and b while using the give Binomial Outcome data. Also If you wish you study about the mle function refer &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle-function&#34;&gt;this link&lt;/a&gt; from my previous post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mle2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mle2&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/bbmle/index.html&#34;&gt;bbmle&lt;/a&gt; package holds the mle2 function. It is simply an updated version for the mle function. Although there need to be no changes in the EstMLEBetaBin function to satisfy the mle2 function’s criteria. Now it will be possible to use it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bbmle)

# optimizing values for a,b using default analytical method
mle2_answer&amp;lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),
                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still if someone needs a brief introduction to mle2 function they can refer my previous brief through &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#mle2-function&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My personal opinion is to use the mle2 function, but moving towards what should be the analytical method. It would be wise to choose it based on your needs as these methods completely depend on the data, function that needs to be estimated, complexity of the function and finally the number estimators that needs to be estimated.&lt;/p&gt;
&lt;p&gt;This is the link to the article which is for &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/&#34;&gt;Benchmarking optim function&lt;/a&gt;. It might be useful while understanding the analytical methods.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build a New Package with Existing R packages</title>
      <link>/post/newpackage/build-a-new-package-with-existing-r-packages/</link>
      <pubDate>Sun, 23 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/newpackage/build-a-new-package-with-existing-r-packages/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-essential-packages&#34;&gt;Most Essential Packages&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#devtools&#34;&gt;devtools&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#pkgbuild&#34;&gt;pkgbuild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgload&#34;&gt;pkgload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rcmdcheck&#34;&gt;rcmdcheck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usethis&#34;&gt;usethis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roxygen2&#34;&gt;roxygen2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#knitr&#34;&gt;knitr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#markdown-rmarkdown-rmdformats&#34;&gt;markdown, rmarkdown, rmdformats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spelling&#34;&gt;spelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trackmd&#34;&gt;trackmd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testthat&#34;&gt;testthat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#essential-packages&#34;&gt;Essential Packages&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#git2r-and-gh&#34;&gt;git2r and gh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#desc&#34;&gt;desc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#covr&#34;&gt;covr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#badgecreatr-and-badger&#34;&gt;badgecreatr and badger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hexsticker&#34;&gt;hexSticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgdown&#34;&gt;pkgdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#still-i-have-not-used&#34;&gt;Still I have not Used&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packrat&#34;&gt;packrat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkgconfig&#34;&gt;pkgconfig&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pkginspector&#34;&gt;pkginspector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rvcheck&#34;&gt;rvcheck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rversions&#34;&gt;rversions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formatr&#34;&gt;formatR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#whoami&#34;&gt;whoami&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Package development is a sense of accomplishment for any statistical programmer who needs self satisfaction. I developed the R package &lt;a href=&#34;&#34;&gt;fitODBOD&lt;/a&gt; for the purpose of fitting Over dispersed Binomial Outcome Data using Binomial Mixture Distributions and Alternate Binomial Distributions. It was a an amazing journey learning how to develop an R package, which took me around 6 months while understanding the theoretical aspects of my research project and doing my 4th year courses.&lt;/p&gt;
&lt;p&gt;I am still learning new things related to R, which is helpful for this R package development. Making package version updates regularly is for the benefit of the user. I have learned new ways to express the theoretical concepts in the simplest form of functions, classes and methods. Currently, I am exploring the possibility of using Rshiny dashboard and GUI.&lt;/p&gt;
&lt;p&gt;In the beginning, R package developers have used manual techniques (which mean difficult techniques)&lt;br /&gt;
to develop R functions, documentation and examples for their packages. Over time it has changed rapidly, where currently we are using R packages to develop our own R package. In this post I shall briefly mention these packages which you can use. Using these packages it is possible to make package development stress free, time efficient and objective effective. Simultaneously we can make our R packages more attractive for the users, which would lead to lot of attention in the R community.&lt;/p&gt;
&lt;p&gt;There are three types of packages in my perspective, first “Most Essential Packages” which cannot be ignored, second “Essential packages” it is your choice to ignore and finally, “Still I have not Used” packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-essential-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Most Essential Packages&lt;/h1&gt;
&lt;p&gt;There are three packages in my main interest list and they are &lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;devtools&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=roxygen2&#34;&gt;roxygen2&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/package=testthat&#34;&gt;testhat&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;devtools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;devtools&lt;/h2&gt;
&lt;p&gt;Collection of packages which would significantly help the package development process. Functions such as dev_mode, check_failures, check_win and check_man.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=devtools&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;pkgbuild&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pkgbuild&lt;/h3&gt;
&lt;p&gt;Locates compilers needed to build R packages on various platforms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgbuild&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgload&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pkgload&lt;/h3&gt;
&lt;p&gt;Simulate the process of installing a package and then attacking it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgload&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rcmdcheck&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;rcmdcheck&lt;/h3&gt;
&lt;p&gt;Run “R CMD check” from R programmaticallly, and capture the results of the individual checks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rcmdcheck&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;usethis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;usethis&lt;/h3&gt;
&lt;p&gt;Automating few tasks related to package building.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=usethis&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;roxygen2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;roxygen2&lt;/h2&gt;
&lt;p&gt;Generate Rd documentation, and Namespace file with simplicity which would save time, when package update occurs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=roxygen2&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;knitr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;knitr&lt;/h3&gt;
&lt;p&gt;Useful to develop vignettes related to R package development.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=knitr&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markdown-rmarkdown-rmdformats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;markdown, rmarkdown, rmdformats&lt;/h3&gt;
&lt;p&gt;Html formats to vignettes in R package development and special template styles for the vignettes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=markdown&#34;&gt;markdown&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=rmarkdown&#34;&gt;rmarkdown&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=rmdformats&#34;&gt;rmdformats&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spelling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;spelling&lt;/h3&gt;
&lt;p&gt;Checking for spelling issues in Rd documentation files.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=spelling&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trackmd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;trackmd&lt;/h3&gt;
&lt;p&gt;Tracking changes in markdown files for vignette.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/trackmd&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testthat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;testthat&lt;/h2&gt;
&lt;p&gt;Checking if functions work properly by testing them in multiple ways for errors, outputs and inputs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=testthat&#34;&gt;Link for the package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;essential-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Essential Packages&lt;/h1&gt;
&lt;p&gt;We would not necessarily need these packages to develop our package, but it would make things more official if we choose to use them. Creating official badges, using GitHub for version control, checking for code coverage, distinct logo and having a website to explore the functions and vignettes of the package.&lt;/p&gt;
&lt;div id=&#34;git2r-and-gh&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;git2r and gh&lt;/h2&gt;
&lt;p&gt;Access to GitHub so that version control would occur smoothly with integration in Rstudio.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=git2r&#34;&gt;git2r&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=gh&#34;&gt;gh&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;desc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;desc&lt;/h2&gt;
&lt;p&gt;Editing the Description file using package rather than manually editing the file.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=desc&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;covr&lt;/h2&gt;
&lt;p&gt;Checking code coverage, which means does all functions have examples and are there tests for error messages, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=covr&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;badgecreatr-and-badger&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;badgecreatr and badger&lt;/h2&gt;
&lt;p&gt;Adding badges to GitHub repository, for example Download, CRAN status, code coverage, Release date, version and much more.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=badgecreatr&#34;&gt;badgecreatr&lt;/a&gt; &lt;a href=&#34;https://cran.r-project.org/package=badger&#34;&gt;badger&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hexsticker&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;hexSticker&lt;/h2&gt;
&lt;p&gt;Creating a hexagon sticker for your package. Mostly just for the fun, but in a while its like promoting a brand.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=hexSticker&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkgdown&lt;/h2&gt;
&lt;p&gt;Using man files, vignette of your package to develop a static website. Further, it is possible to promote this site to get more people interested in the package.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgdown&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;still-i-have-not-used&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Still I have not Used&lt;/h1&gt;
&lt;p&gt;In this list I will explore packages which can be used to make package development more simple and elegant. Mostly using functions for the tasks of proper code spaces, indent and necessary R versions of dependency packages.&lt;/p&gt;
&lt;div id=&#34;packrat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;packrat&lt;/h2&gt;
&lt;p&gt;Manage the R packages in an isolated, portable and reproducible way.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=packrat&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkgconfig&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkgconfig&lt;/h2&gt;
&lt;p&gt;Set configuration options on a per-package basis.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=pkgconfig&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pkginspector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;pkginspector&lt;/h2&gt;
&lt;p&gt;Understand internal structure of an R package.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/pkginspector&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rvcheck&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;rvcheck&lt;/h2&gt;
&lt;p&gt;Check latest release version of R and R packages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rvcheck&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rversions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;rversions&lt;/h2&gt;
&lt;p&gt;Focusing on R version ‘r-release’ and ‘r-oldrel’. Further all previous R versions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=rversions&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;formatr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;formatR&lt;/h2&gt;
&lt;p&gt;Spaces and Indent for the code automatically added&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=formatR&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whoami&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;whoami&lt;/h2&gt;
&lt;p&gt;Username and full-name of current user, also email address and GitHub username.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/package=whoami&#34;&gt;Link to package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Hopefully we would have more packages and awareness towards R package development with more simplicity in the coming years. Even though we have more than 15,000 packages in CRAN it would still rapidly increase in the coming years, not only in CRAN but also in GitHub as well.&lt;/p&gt;
&lt;p&gt;So far this post has only word content and links, therefore I am adding a screenshot of my fitODBOD package GitHub package ReadME.md file. This screen shot includes badges for downloads, R version, published date, package version and a hexagon logo sticker.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/screenshots/GitHub1.png&#34; /&gt;

&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/screenshots/GitHub2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;I hope this blog post is useful for anyone who has intentions to develop their R package or in the process of development or version controlling. I developed fitODBOD as a research project for my final year and it included a thesis report as well. Over time I wrote a Journal article as well and it is still under review. Also I have seen R package development as a PhD submission as well. Therefore it would be worthwhile developing an R package as a way to keep an active status regarding your field of interest in Statistics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How To Find Your R package ?</title>
      <link>/post/findrpackage/how-to-find-your-r-package/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/findrpackage/how-to-find-your-r-package/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#google&#34;&gt;1. Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cran&#34;&gt;2. CRAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bio---conductor&#34;&gt;3. Bio - Conductor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#github-pages&#34;&gt;4. GitHub pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rdocumentation&#34;&gt;5. Rdocumentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#crantastic&#34;&gt;6. Crantastic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rpackages&#34;&gt;7. rpackages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r---opensci&#34;&gt;8. R - Opensci&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rseek&#34;&gt;9. Rseek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-site-search&#34;&gt;10. R Site Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-forge&#34;&gt;11. R-forge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#awesomer&#34;&gt;12. AwesomeR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cran-task-view&#34;&gt;13. CRAN Task View&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rstudio---rpackages&#34;&gt;14. Rstudio - Rpackages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stack-overflow---r&#34;&gt;15. stack overflow - r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cranalerts&#34;&gt;16. CRANalerts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;How to find your R package is simply a blog post helping people to provide a list of websites where they can find R packages. These websites were useful for me while developing my own R package &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt;. So that I would be sure that fitODBOD is a unique package and what its functions should be able to do.&lt;/p&gt;
&lt;p&gt;This is a list with 16 items&lt;/p&gt;
&lt;div id=&#34;google&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Google&lt;/h1&gt;
&lt;p&gt;When you have no idea to find a package first thing is to “Google”.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Google.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. CRAN&lt;/h1&gt;
&lt;p&gt;Official website to find standard packages. The packages downloaded here will have documentation manuals, vignettes and sometimes journal articles which would simplify work for people who use them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/CRAN.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bio---conductor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Bio - Conductor&lt;/h1&gt;
&lt;p&gt;Another standard location to publish your R package, but only related to the field of Biology.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Bioconductor.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github-pages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. GitHub pages&lt;/h1&gt;
&lt;p&gt;If CRAN or Bio - Conductor is with high standards or too much work for your package you can still publish it and the ideal place for this is GitHub.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/GitHub.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rdocumentation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Rdocumentation&lt;/h1&gt;
&lt;p&gt;A place to find interactive documentation for the packages in CRAN, Bio - Conductor and GitHub. They simply include everything in the manual of a package but in html format.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rdocumentation.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;crantastic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Crantastic&lt;/h1&gt;
&lt;p&gt;All packages which are a part of CRAN is in this website. We can search packages based on Authors, package name, reviews and tags.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.crantastic.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/crantastic.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rpackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7. rpackages&lt;/h1&gt;
&lt;p&gt;Similar to crantastic this website also provides information to CRAN packages, but it is better because package related statistics is also shown here.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpackages.io/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rpackages.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r---opensci&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. R - Opensci&lt;/h1&gt;
&lt;p&gt;Search range for R packages in this website has more categories which is informative. I would say better than above mentioned ones.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/ropensci.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rseek&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;9. Rseek&lt;/h1&gt;
&lt;p&gt;This is like a google search engine for R packages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rseek.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rseek.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-site-search&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;10. R Site Search&lt;/h1&gt;
&lt;p&gt;Website dedicated to search R functions, package vignettes and task views.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://finzi.psych.upenn.edu/search.html&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rsitesearch.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-forge&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;11. R-forge&lt;/h1&gt;
&lt;p&gt;Projects related to R are mentioned in this website and how progress has been made on them is also here. Most of these projects will be published as packages later with significant importance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://r-forge.r-project.org/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rforge.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;awesomer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;12. AwesomeR&lt;/h1&gt;
&lt;p&gt;This is a website which has R packages based on topics related to statistics. Some of these topics are Machine Learning, Bayesian, Optimization, Bio statistics and much more.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://awesome-r.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/AwesomeR.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cran-task-view&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;13. CRAN Task View&lt;/h1&gt;
&lt;p&gt;Topic related R packages are bundled together in this website. Further, the topics give a brief explanation, but the webpages give an extensive amount of information about what is unique is these packages. Also you do not need internet to use this because it is part of Rstudio help.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/views/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Crantaskview.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio---rpackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;14. Rstudio - Rpackages&lt;/h1&gt;
&lt;p&gt;Several crucial packages which would be very useful are considered here. All of these packages are projects. Further they are very popular in the R community.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/products/rpackages/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/rstudio-rpackages.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stack-overflow---r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;15. stack overflow - r&lt;/h1&gt;
&lt;p&gt;If you cannot achieve something very specific related to R coding it is possible to use the website. It provides answers from other R users, sometimes even blogs related to the issues with solutions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/tagged/r&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/stackoverflow.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cranalerts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;16. CRANalerts&lt;/h1&gt;
&lt;p&gt;This is an email service which would alert us regarding specific packages accordance to our request. Whenever there is an update for a chosen package we would receive an email alert.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cranalerts.com/&#34;&gt;Link&lt;/a&gt; &lt;img src=&#34;/screenshots/Cranalerts.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is my list of places for reaching out to help with related to R packages and R programming. I use them constantly and they are very much helpful to me. Finally, I hope this post would be useful to anyone who wants to find or use R packages.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking the maxLik function</title>
      <link>/post/maxlik/benchmarking-maxlik-function/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/maxlik/benchmarking-maxlik-function/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-the-shape-parameters-of-beta-binomial-distribution&#34;&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#brief-of-maxlik-function&#34;&gt;Brief of maxLik Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nr-method&#34;&gt;NR method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bfgs-method&#34;&gt;BFGS method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bfgsr-method&#34;&gt;BFGSR method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bhhh-method&#34;&gt;BHHH method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sann-method&#34;&gt;SANN method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cg-method&#34;&gt;CG method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nm-method&#34;&gt;NM method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sumary-of-time-evalutation-for-different-analytical-methods-of-maxlik-function&#34;&gt;Sumary of Time evalutation for different Analytical methods of maxLik function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-results-after-using-the-maxlik-function-for-different-analytical-methods&#34;&gt;Summary of results after using the maxLik function for different analytical methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-conclusion&#34;&gt;Final Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;estimating-the-shape-parameters-of-beta-binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating the shape parameters of Beta-Binomial Distribution&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Beginning of this month I wrote a &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#maxlik-function&#34;&gt;small section&lt;/a&gt; regarding maxLik function by comparing it to other optimization functions. Here, we will further study the analytical methods which can be used in this function and compare them to find suitability. maxLik function is from the package &lt;a href=&#34;https://cran.r-project.org/web/packages/maxLik/index.html&#34;&gt;maxLik&lt;/a&gt;. Further, &lt;a href=&#34;https://www.rdocumentation.org/packages/maxLik/versions/1.3-4/topics/maxLik&#34;&gt;Documentation&lt;/a&gt; clearly indicates all things related to the function in detail.&lt;/p&gt;
&lt;p&gt;Focusing on the seven analytical methods is my intention from this blog post. So, we have the Beta-Binomial distribution and Binomial Outcome data, and need to estimate proper shape parameters which would Maximize the Log Likelihood value of the Beta-Binomial distribution for the above Binomial Outcome data. In this case Alcohol Consumption data from the &lt;a href=&#34;https://cran.r-project.org/package=fitODBOD&#34;&gt;fitODBOD&lt;/a&gt; package will be used.&lt;/p&gt;
&lt;p&gt;Further we will focus on the process time to optimization, estimated shape parameters, maximized Log Likelihood value, expected frequencies, p-value and Over-dispersion with tables.&lt;/p&gt;
&lt;p&gt;Below are the seven analytical methods in concern&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;NR&lt;/li&gt;
&lt;li&gt;BFGS&lt;/li&gt;
&lt;li&gt;BFGSR&lt;/li&gt;
&lt;li&gt;BHHH&lt;/li&gt;
&lt;li&gt;SANN&lt;/li&gt;
&lt;li&gt;CG&lt;/li&gt;
&lt;li&gt;NM&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fitODBOD)
kable(Alcohol_data,&amp;quot;html&amp;quot;,align=c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;),font_size = 14,full_width = F) %&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Days
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
week2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
49
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;brief-of-maxlik-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brief of maxLik Function&lt;/h2&gt;
&lt;p&gt;Small &lt;a href=&#34;https://amalan-con-stat.netlify.com/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/#maxlik-function&#34;&gt;section&lt;/a&gt; about the maxLik function will be very useful to understand this blog post.&lt;/p&gt;
&lt;p&gt;Reference : Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood estimation in R Computational Statistics 26, 443–458 Marquardt, D.W., (1963)&lt;/p&gt;
&lt;p&gt;An Algorithm for Least-Squares Estimation of Nonlinear Parameters, Journal of the Society for Industrial &amp;amp; Applied Mathematics 11, 2, 431–441&lt;/p&gt;
&lt;p&gt;So for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would maximize the Log Likelihood value of the Beta-Binomial distribution.&lt;/p&gt;
&lt;p&gt;First we are transforming the given EstMLEBetaBin function to satisfy the maxLik function conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new function to facilitate maxLik criteria
# only one input but has two elements
formaxLik&amp;lt;-function(a)
  {
  -EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the formaxLik function can be used as above and parameters are estimated for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (or a, b) for the Alcohol Consumption data week 1. Further the maxLik function can be scrutinized as below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;package : maxLik&lt;/li&gt;
&lt;li&gt;No of Inputs: 6&lt;/li&gt;
&lt;li&gt;Minimum required Inputs : 2&lt;/li&gt;
&lt;li&gt;Class of output : list or class of maxim or class of maxLik&lt;/li&gt;
&lt;li&gt;No of outputs: 11&lt;/li&gt;
&lt;li&gt;No of Analytical Methods : 7&lt;/li&gt;
&lt;li&gt;Default Method : Automatically chosen&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;nr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NR method&lt;/h2&gt;
&lt;p&gt;NR is an abbreviation for Unconstrained and equality-constrained maximization based on the quadratic approximation (Newton) method. The idea of the Newton method is to approximate the function at a given location by a multidimensional quadratic function, and use the estimated maximum as the start value for the next iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(maxLik)

# optimizing values for a,b using NR analytical method
NR_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;NR&amp;quot;)

# obtaining class of output
class(NR_answer)

# length of output
length(NR_answer)

# the outputs
NR_answer$estimate # estimated values for a, b
NR_answer$maximum # minimized function value 
NR_answer$iterations  # no of iterations to succeed
NR_answer$gradient # last gradient value which was calculated
NR_answer$message # additional information
NR_answer$hessian # hessian matrix
NR_answer$code # indicates successful completion
NR_answer$fixed # logical vector indicating which parameters are constants
NR_answer$type # type of maximization
NR_answer$last.step # list describing the last unsuccessful step
NR_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           NR_answer$estimate[1],NR_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgs-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BFGS method&lt;/h2&gt;
&lt;p&gt;BFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.&lt;/p&gt;
&lt;p&gt;Reference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using BFGS analytical method
BFGS_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;BFGS&amp;quot;)

# obtaining class of output
class(BFGS_answer)

# length of output
length(BFGS_answer)

# the outputs
BFGS_answer$estimate # estimated values for a, b
BFGS_answer$maximum # minimized function value 
BFGS_answer$iterations  # no of iterations to succeed
BFGS_answer$gradient # last gradient value which was calculated
BFGS_answer$message # additional information
BFGS_answer$hessian # hessian matrix
BFGS_answer$code # indicates successful completion
BFGS_answer$fixed # logical vector indicating which parameters are constants
BFGS_answer$type # type of maximization
BFGS_answer$last.step # list describing the last unsuccessful step
BFGS_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           BFGS_answer$estimate[1],BFGS_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgsr-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BFGSR method&lt;/h2&gt;
&lt;p&gt;Combination of two methods which are Newton-Raphson, BFGS (Broyden 1970, Fletcher 1970, Goldfarb 1970, Shanno 1970).&lt;/p&gt;
&lt;p&gt;Reference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using BFGSR analytical method
BFGSR_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;BFGSR&amp;quot;)

# obtaining class of output
class(BFGSR_answer)

# length of output
length(BFGSR_answer)

# the outputs
BFGSR_answer$estimate # estimated values for a, b
BFGSR_answer$maximum # minimized function value 
BFGSR_answer$iterations  # no of iterations to succeed
BFGSR_answer$gradient # last gradient value which was calculated
BFGSR_answer$message # additional information
BFGSR_answer$hessian # hessian matrix
BFGSR_answer$code # indicates successful completion
BFGSR_answer$fixed # logical vector indicating which parameters are constants
BFGSR_answer$type # type of maximization
BFGSR_answer$last.step # list describing the last unsuccessful step
BFGSR_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           BFGSR_answer$estimate[1],BFGSR_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bhhh-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;BHHH method&lt;/h2&gt;
&lt;p&gt;BHHH method (Berndt, Hall, Hall, Hausman 1974). The BHHH (information equality) approximation is only valid for log-likelihood functions. It requires the score (gradient) values by individual observations and hence those must be returned by individual observations by grad or fn. With the complexity of BHHH method I choose not to discuss it here, but a reference is mentioned to anyone who has interest in this analytical method.&lt;/p&gt;
&lt;p&gt;Reference : Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974): Estimation and Inference in Nonlinear Structural Models, Annals of Social Measurement 3, 653–665.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sann-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SANN method&lt;/h2&gt;
&lt;p&gt;Method SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.&lt;/p&gt;
&lt;p&gt;By default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to &lt;span class=&#34;math inline&#34;&gt;\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the current iteration step and temp and tmax are specifiable via control.&lt;/p&gt;
&lt;p&gt;Note that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.&lt;/p&gt;
&lt;p&gt;Reference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using SANN analytical method
SANN_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;SANN&amp;quot;)

# obtaining class of output
class(SANN_answer)

# length of output
length(SANN_answer)

# the outputs
SANN_answer$estimate # estimated values for a, b
SANN_answer$maximum # minimized function value 
SANN_answer$iterations  # no of iterations to succeed
SANN_answer$gradient # last gradient value which was calculated
SANN_answer$message # additional information
SANN_answer$hessian # hessian matrix
SANN_answer$code # indicates successful completion
SANN_answer$fixed # logical vector indicating which parameters are constants
SANN_answer$type # type of maximization
SANN_answer$last.step # list describing the last unsuccessful step
SANN_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           SANN_answer$estimate[1],SANN_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cg-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CG method&lt;/h2&gt;
&lt;p&gt;Method CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak-Ribiere or Beale-Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.&lt;/p&gt;
&lt;p&gt;Reference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using CG analytical method
CG_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;CG&amp;quot;)

# obtaining class of output
class(CG_answer)

# length of output
length(CG_answer)

# the outputs
CG_answer$estimate # estimated values for a, b
CG_answer$maximum # minimized function value 
CG_answer$iterations  # no of iterations to succeed
CG_answer$gradient # last gradient value which was calculated
CG_answer$message # additional information
CG_answer$hessian # hessian matrix
CG_answer$code # indicates successful completion
CG_answer$fixed # logical vector indicating which parameters are constants
CG_answer$type # type of maximization
CG_answer$last.step # list describing the last unsuccessful step
CG_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           CG_answer$estimate[1],CG_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nm-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NM method&lt;/h2&gt;
&lt;p&gt;NM is the abbreviation to Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.&lt;/p&gt;
&lt;p&gt;Reference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimizing values for a,b using NM analytical method
NM_answer&amp;lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = &amp;quot;NM&amp;quot;)

# obtaining class of output
class(NM_answer)

# length of output
length(NM_answer)

# the outputs
NM_answer$estimate # estimated values for a, b
NM_answer$maximum # minimized function value 
NM_answer$iterations  # no of iterations to succeed
NM_answer$gradient # last gradient value which was calculated
NM_answer$message # additional information
NM_answer$hessian # hessian matrix
NM_answer$code # indicates successful completion
NM_answer$fixed # logical vector indicating which parameters are constants
NM_answer$type # type of maximization
NM_answer$last.step # list describing the last unsuccessful step
NM_answer$control # see the documentation understand

# fitting the Beta-Binomial distribution with estimated shape parameter values
fitBetaBin(Alcohol_data$Days,Alcohol_data$week1,
           NM_answer$estimate[1],NM_answer$estimate[2])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sumary-of-time-evalutation-for-different-analytical-methods-of-maxlik-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sumary of Time evalutation for different Analytical methods of maxLik function&lt;/h1&gt;
&lt;p&gt;Below table will compare the system process time for different analytical methods. In order to do this time comparison it is possible to use the &lt;a href=&#34;https://www.rdocumentation.org/packages/rbenchmark/versions/1.0.0/topics/benchmark&#34;&gt;benchmark&lt;/a&gt; function of &lt;a href=&#34;https://cran.r-project.org/package=rbenchmark&#34;&gt;rbenchmark&lt;/a&gt; package. Below mentioned code chunk provides the output in a table form which includes the analytical methods and their respective time values. The estimation process of the parameters where each method has been replicated 1000 times to receive a more accurate table for time values.&lt;/p&gt;
&lt;p&gt;Table is in the ascending order for elapsed time column. It is evidently clear that SANN analytical method has taken the most time. Before that the analytical method BFGSR is in 5th place. While NM or Nelder Mead method has taken the least time. This time is calculated for 1000 replications of the function being repeated under same conditions. These times does not only reflect based on analytical method, rather on the Log Likelihood function that needs to be maximized, the data provided, the number of estimated that needs to be estimated, the complexity of the function and finally the computer’s processing power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rbenchmark)

Results1&amp;lt;-benchmark(
          &amp;quot;NR&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;NR&amp;quot;)},
          &amp;quot;BFGS&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;BFGS&amp;quot;)},
          &amp;quot;BFGSR&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;BFGSR&amp;quot;)},
          &amp;quot;SANN&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;SANN&amp;quot;)},
          &amp;quot;CG&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;CG&amp;quot;)},
          &amp;quot;NM&amp;quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = &amp;quot;NM&amp;quot;)},
          replications = 1000,
          columns = c(&amp;quot;test&amp;quot;,&amp;quot;replications&amp;quot;,&amp;quot;elapsed&amp;quot;,
                      &amp;quot;relative&amp;quot;,&amp;quot;user.self&amp;quot;,&amp;quot;sys.self&amp;quot;),
          order = &amp;#39;elapsed&amp;#39;
          )

kable(Results1,&amp;quot;html&amp;quot;,align = c(&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;c&amp;#39;)) %&amp;gt;%
  kable_styling(full_width=T,bootstrap_options=c(&amp;quot;striped&amp;quot;),font_size = 14)%&amp;gt;%
  row_spec(0,color = &amp;quot;blue&amp;quot;) %&amp;gt;%
  column_spec(1,color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 14px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: blue;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
test
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
replications
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
elapsed
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
relative
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
user.self
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
sys.self
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
25.50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20.21
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BFGS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.498
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
33.25
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NR
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.204
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
46.31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
CG
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
97.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.826
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
79.44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
BFGSR
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
842.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
33.020
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
716.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
SANN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2523.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
98.972
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2072.40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.44
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-results-after-using-the-maxlik-function-for-different-analytical-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of results after using the maxLik function for different analytical methods&lt;/h1&gt;
&lt;p&gt;Estimated the shape parameters a,b pair wise from the analytical methods NR, BFGS, BFGSR, SANN, CG and NM. These estimated parameters will be now used in the fitBetaBin function to find the expected frequencies, p-values and over-dispersion. The above measurements can be used to compared for each analytical method for any significance difference.&lt;/p&gt;
&lt;p&gt;Comparing p-values it is clear that all analytical methods generate the same value up-to third decimal point. This is not the case in Maximum Log Likelihood value where analytical methods NR, BFGS, CG and NM have obtained the value -813.4571, while BFGSR and SANN have shown -813.4576. Further, Over-dispersion values are similar until third decimal point, but after that there is a clear difference among all six methods.&lt;/p&gt;
&lt;p&gt;All of the analytical methods have produced distinct values for estimated shape parameters of a and b. For the shape parameter a, similarity is only until second decimal point, and for shape parameter b, similarity of value is only on first decimal point.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: 12px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BinomialRandomVariable
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
NR
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BFGS
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
BFGSR
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
SANN
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
CG
&lt;/th&gt;
&lt;th style=&#34;text-align:center;color: blue;&#34;&gt;
NM
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41.98
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.85
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.91
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
38.54
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40.07
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
41
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.93
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
43.97
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
53.09
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.98
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.76
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
87.77
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Total No of Observations
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
399
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
398.99
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
p-value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0903
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0901
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0902
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Estimated a and b
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229428 b=0.5808488
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7228919 b=0.5807283
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7209896 b=0.5790360
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7219066 b=0.5813484
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7229403 b=0.5808469
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a=0.7230707 b=0.5809894
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Maximum Log Likelihood
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4576
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4576
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-813.4571
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;color: red;&#34;&gt;
Over Dispersion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.434067
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340993
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4347778
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4341682
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340679
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.4340165
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;final-conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Conclusion&lt;/h1&gt;
&lt;p&gt;Now we can conclude the above findings using the tables provided, and it is clear that there is no strong change in expected frequencies, maximum Log Likelihood value or p-value if we use any one of the methods mentioned above. If time is crucial it is best to avoid BFGSR and SANN methods as they take considerable amount of time. I would recommend choose the analytical method from maxLik function based on your needs of output and research objective.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;THANK YOU&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
