[{"authors":null,"categories":["fitODBOD"],"content":"","date":1544745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544745600,"objectID":"96374b42aeff237101e45033d4c029ed","permalink":"/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/","publishdate":"2018-12-14T00:00:00Z","relpermalink":"/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/","section":"post","summary":"","tags":["optim","fitODBOD","R"],"title":"optim: Estimating the shape parameters of Beta-Binomial Distribution","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(ggthemr)\rlibrary(lubridate)\rlibrary(stringr)\rlibrary(kableExtra)\r#using theme\rggthemr(\u0026quot;fresh\u0026quot;)\r#load data\rNYC\u0026lt;-read_csv(\u0026quot;nyc_restaurants.csv\u0026quot;, col_types = cols(inspection_date = col_date(format = \u0026quot;%m/%d/%Y\u0026quot;)))\rattach(NYC)\rData set is completed with more than 300000 records and several important variables such as inspection date, violation code, critical flag, score and violation description. In this article I will mainly focus on Inspection Type, boro, Inspection year, cuisine type and Critical Flag.\nThat tweet.\nFirst see what type of inspections have occurred.\nInpsection Type\rThis ordered bar plot for inspection type clearly indicates that cycle inspection / initial inspection has the highest amount of counts with 171,657. while second place goes to cycle inspection/ re-inspection with 73207. Other types of inspection hold less than 21,000 counts, where there are more than 10 types of inspections which hold counts less than 1000.\n#summary.factor(inspection_type) %\u0026gt;%\r# sort()\r# Bar plot for Insepction type\rggplot(NYC,aes(x=fct_infreq(str_wrap(inspection_type,35))))+\rgeom_bar()+coord_flip()+\rscale_y_continuous(breaks = seq(0,200000,25000),labels = seq(0,200000,25000))+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),hjust=-0.005)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Type of Insepction\u0026quot;)+\rggtitle(\u0026quot;Types of Inspection\u0026quot;)\r\rCritical Flag\rCritical flag has three types of categories which are Critical, Not Critical and Not Applicable. It is represented as a pie chart where 54.9% are critical(164,623) and second place goes to Not Critical(129,348) with 43.1%. Finally, only 6029 inspections have lead to Not Applicable which is represented by 2.0%.\n#summary for critical flag\r#NYC$critical_flag %\u0026gt;%\r# summary.factor() %\u0026gt;%\r# sum()\rvalue=c(164623,6029,129348)\r# creating data frame for Critical flag\rCF\u0026lt;-data.frame(\rgroup=c(\u0026quot;Critical\u0026quot;,\u0026quot;Not Applicable\u0026quot;,\u0026quot;Not Critical\u0026quot;),\rvalue=c(164623,6029,129348),\rper=round(value*100/300000,4)\r)\r# pie chart for percentages\rP1\u0026lt;-ggplot(CF,aes(x=\u0026quot;\u0026quot;,y=per,fill=group))+\rgeom_col()+ theme_void()+\rgeom_text(aes(label=scales::percent(per/100)),position=position_stack(vjust=0.5))+\rlabs(title = \u0026quot;Critical Flag \\nDistribution\u0026quot;,fill=\u0026quot;Type\u0026quot;)+\rcoord_polar(theta = \u0026quot;y\u0026quot;,start = 0)\r# pie chart for counts\rP2\u0026lt;-ggplot(CF,aes(x=\u0026quot;\u0026quot;,y=value,fill=group))+\rgeom_col()+theme_void()+\rlabs(title = \u0026quot;Critical Flag\\n Distribution\u0026quot;,fill=\u0026quot;Type\u0026quot;)+ geom_text(aes(label = value), position = position_stack(vjust = 0.5)) +\rcoord_polar(theta = \u0026quot;y\u0026quot;,start = 0)\rgridExtra::grid.arrange(P1,P2,nrow=2)\r\rInspection Type and Critical Flag over the years\rNow facing the issue of inspection year, inspection type and critical flag, I have generated bar plots for identifying the behavior. Basically what we have is bar plots for years from 2012 to 2018 how the counts of Critical flag types have changed for different paired types of inspections.\nInspection types considered\n\rCycle Inspection / Initial Inspection and Cycle Inspection / Re-inspection.\rPre-permit (Operational) / Initial Inspection and Pre-permit (Operational) / Re-inspection.\rAdministrative Miscellaneous / Initial Inspection and Administrative Miscellaneous / Re-inspection.\r\rCycle Inspection\rInitial Inspection is always high for all the years while assigning Critical. The years 2012, 2013 and 2014 has very low amount of counts while the succeeding years have increasing counts. Initial inspection over the years from 2015 is increasing for the gap between Critical and Not Critical. In 2015 the above gap is close to 4000 but by 2018 it has increased to 10000. If we consider Re-inspection the gap for Critical and Not Critical is close 1000 in year 2015 but by year 2018 it is 4000. Not applicable is increasing over the years for both initial inspection and re-inspection, even though the amounts are in hundreds.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Cycle Inspection / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Cycle Inspection / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Cycle Inspection\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Cycle Inspection over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\rPre-permit Operational\rSecond most considered type of inspection is Pre-permit Operational, where it begins from 2014. Even though in year 2014 the amounts for initial inspection and re-inspection are less than 20 in all three critical flag categories. Considering the gap between Critical and Not Critical for initial inspection over the years there is a clear increase. Where as in year 2015 the gap is slightly less than 400, next year it is close to 1000, but by year 2018 this is gap is above than 2000.\nIn re-inspection for the year 2015 the gap is almost 150, yet with more inspections by 2018 the gap increases to 600. For, Not Applicable the counts do not have a clear increasing or decreasing pattern over the years.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Pre-permit (Operational) / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Pre-permit (Operational) / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Pre-permit (Operational)\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Pre-permit (Operational) over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\rAdministrative Miscellaneous\rVery odd bar plot here than previous two plots for inspection types. There is no Critical type restaurants in the Administrative Miscellaneous inspection type. The counts begin from year 2014 but the amounts are less than 10. Clearly for initial inspections over the years from 2015 to 2018 the count for Not Applicable are increasing, but this is not the case for Not Critical.\nIn year 2015 the amount for the type Not Critical for initial inspection was 528, but in years 2016 and 2018 the amounts increased to respectively to 1038 and 1486. Even though the amount decreased to 1015 in year 2017. The same pattern of increase and decrease behavior occurs for Re-inspection type as well.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Administrative Miscellaneous / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Administrative Miscellaneous / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Administrative Miscellaneous\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Administrative Miscellaneous over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\r\rMost Inspected Restaurants\rFirst look at the top 5 restaurants which were inspected and clearly Dunkin’ Donuts has the highest amount with 3136, while second place goes to Subway with 2419 and third place goes to McDonald’s with 1783.\n# Most 5 restaurants which were inspected\rkable(summary.factor(dba) %\u0026gt;%\rsort() %\u0026gt;%\rtail(5)\r,col.names = c(\u0026quot;Frequency\u0026quot;),align = \u0026#39;c\u0026#39;) \r\r\r\r\rFrequency\r\r\r\r\r\rKENNEDY FRIED CHICKEN\r\r1031\r\r\r\rSTARBUCKS\r\r1636\r\r\r\rMCDONALD’S\r\r1783\r\r\r\rSUBWAY\r\r2419\r\r\r\rDUNKIN’ DONUTS\r\r3136\r\r\r\r\rDunkin Donuts\rQueens has close to 1000 observations of Dunkin Donuts and lowest amount goes to Staten Island with 308. Other three boros have counts in between 550 and 710.\n# subsetting Dunkin Donuts with boro\rsubset(NYC, dba==\u0026quot;DUNKIN\u0026#39; DONUTS\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Dunkin Donuts in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rThere are only 5 cuisine types and prominent ones are American, Donuts and drinks(Cafe/Coffee/Tea). Year 2014 is very low in amounts even for cuisine Donuts, but this is not the case over the next few years and the scores are mostly centered between 5 to 15. Bagels/Pretzels and Jewish/ Kosher have least amount of data where none of scores are above 25. It is safe to to say we have more Critical type data and they are mostly close to the score of 10.\n# Dunkin Donuts and scores with critical flag\rsubset(NYC, dba==\u0026quot;DUNKIN\u0026#39; DONUTS\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Dunkin Donuts score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,60,5),labels =seq(0,60,5))+\rfacet_wrap(~cuisine_description) \r\rSubway\r912 points from Manhattan with the highest count and lowest count goes to Staten Island with 141 counts. Bronx and Brooklyn has counts in between 320 and 365, but Queens boro has an amount of 683.\n# subsetting Subway with boro\rsubset(NYC, dba==\u0026quot;SUBWAY\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Subways in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rSubway has only 5 cuisines which are American, Other, Sandwiches, Sandwiches/Salads/Mixed Buffet and Soups \u0026amp; Sandwiches. In these five categories only Sandwiches has significant amount of data points. Oddly, we can see the year 1900 in the x axis and which means error.\nIn Sandwiches category there are more points which are centered in between 5 to 15 in scores, while most of them are Not Critical. Oddly in 2018 there are Critical data points with scores above 55 in Sandwiches cuisine type.\nFor the Other category there are only 4 observations which are in 2018 and 75% of them are Not Critical. Considering American Cuisines there are points in all 4 years and most of them are less than 25% and Not Critical.\n# Subway and scores with critical flag\rsubset(NYC, dba==\u0026quot;SUBWAY\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Subway score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rMcDonalds\rClose to 550 data points are from Manhattan boro, but Staten Island boro has points close to 75. Second place of 487 counts goes to Queens boro. Other two boros, which are Bronx and Brooklyn have counts in the range of 320 and 360.\n# subsetting McDonalds with boro\rsubset(NYC, dba==\u0026quot;MCDONALD\u0026#39;S\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many McDonalds in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rOnly the cuisines Other and Sandwiches/Salads/Mixed Buffet has data points in the years 2017 and 2018 and these points have an amount less than 15 in both cases. Considering the other two cuisines which are American and Hamburgers, there are more points in the latter than in the former. It should be noted that Hamburgers cuisine has mostly points centered close to the score range of 5 to 10, and these points are mostly Not Critical.\nIn American cuisine for the year 2016 there are 5 points which have scores close to 70, even though in any other year this has not occurred. To be more precise, except those 5 points all the others have scores less than 45 for American Cuisine.\n# McDonalds and scores with critical flag\rsubset(NYC, dba==\u0026quot;MCDONALD\u0026#39;S\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;McDonalds score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rStarbucks\rThere are more than 1100 Starbucks in Manhattan only while other boros have less than 220 and lowest amount goes to Bronx with 31. Second lowest goes to Staten Island with 41.\n#subsetting Starbucks with boro\rsubset(NYC, dba==\u0026quot;STARBUCKS\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Starbucks in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rStarbucks is mainly focused on Cafe/Coffee/Tea rather than cuisines such as Pizza, Sandwiches, American and other. Clearly there are negligible amount of data points in those four categories, except American cuisine.\nIf we focus on Drinks(Coffee/Cafe/Tea), evidently most of them are Not Critical and they are centered around the score of 0 to 10. Even though scores higher than 15 do occur they are spread out widely. This is a common occurrence for all four years, which is from 2015 to 2018.\n# Starbucks and scores with critical flag\rsubset(NYC, dba==\u0026quot;STARBUCKS\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+\rlabs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Starbucks score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rKennedy Fried Chicken\rBronx the has most amount observations to Kennedy Fried Chicken’s with 626 while lowest count of 5 is from Staten Island. Other three boros have counts in the range of 75 to 190.\n#subsetting Kennedy Fried Chicken with boro\rsubset(NYC, dba==\u0026quot;KENNEDY FRIED CHICKEN\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Kennedy Fried Chicken\u0026#39;s in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rThere are only four cuisines which are related to Kennedy Fried Chicken, and they are American, Chicken, Hamburgers and other. The categories Hamburgers and Other have very less amount of counts and the year 1900 is also mentioned. For Hamburgers cuisine only the year 2017 has significant amount of data points, where the year 2015 has only one and year 2018 has only two points.\nFurther, the scores for these points are always less than 20 and mostly Critical. Cuisines of Chicken has more data points than American but in both types there is no clear centering of data around a certain score.\n# Kennedy Fried Chicken and scores with critical flag\rsubset(NYC, dba==\u0026quot;KENNEDY FRIED CHICKEN\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Kennedy Fried Chicken score changes with Critical Flag\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\r\rConclusion\rMy conclusion of the above plots and a table in point form\n\rfacet wrap is very useful.\n\rAdding colors makes it more useful for the plots.\n\rpie chart using bar chart looks good.\n\r\r\rFurther Analysis\r\rIt is possible to focus on violation codes.\n\rFurther looking at the cuisines we can divide it based on the boros as well.\n\r\r\r","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"c4f2bb078d1fbbb4e228fb1f2893aac2","permalink":"/post/week_37/week-37-nyc-restaurants/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/post/week_37/week-37-nyc-restaurants/","section":"post","summary":"# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(ggthemr)\rlibrary(lubridate)\rlibrary(stringr)\rlibrary(kableExtra)\r#using theme\rggthemr(\u0026quot;fresh\u0026quot;)\r#load data\rNYC\u0026lt;-read_csv(\u0026quot;nyc_restaurants.csv\u0026quot;, col_types = cols(inspection_date = col_date(format = \u0026quot;%m/%d/%Y\u0026quot;)))\rattach(NYC)\rData set is completed with more than 300000 records and several important variables such as inspection date, violation code, critical flag, score and violation description. In this article I will mainly focus on Inspection Type, boro, Inspection year, cuisine type and Critical Flag.\nThat tweet.","tags":["R","TidyTuesday"],"title":"Week 37: NYC Restaurants","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r#loading packages\r#load data\rlibrary(readr)\r#manipulate data\rlibrary(dplyr)\rlibrary(magrittr)\r# format table with expense\rlibrary(formattable)\r# knitting the document\rlibrary(knitr)\r# another type of table\rlibrary(kableExtra)\r# playing with strings\rlibrary(stringr)\r# combining two plots\rlibrary(grid)\rlibrary(gridExtra)\r# that theme you wanted\rlibrary(ggthemr)\r# text analysis\rlibrary(tm)\rlibrary(SnowballC)\rlibrary(RColorBrewer)\rlibrary(wordcloud)\r# adding theme called fresh for plots\rggthemr(\u0026quot;fresh\u0026quot;)\r#loading the data\rmedium \u0026lt;- read_csv(\u0026quot;medium_datasci.csv\u0026quot;)\rattach(medium)\rFocusing on Claps with Authors and publications, where does writing alot of posts will get you popularity and claps. The code will focus on Top 10 Authors with most of the posts and Claps they have received. Further, does having an image in the post matter ?. Finally, word clouds for Top 10 authors and Top 5 publications with their titles.\nThe Tweet\nClaps\rTable indicates that 25,729 posts have 0 claps, while 7,093 posts with only one clap, and finally 3044 posts with 2 claps. The only odd one is posts with 50 claps where the count is 970.\n# extracting the top 15 with most claps\rclaps_table\u0026lt;-table(claps) %\u0026gt;%\rsort() %\u0026gt;%\rtail(15)\r# table it up kable(t(claps_table),\u0026quot;html\u0026quot;) %\u0026gt;%\rkable_styling(bootstrap_options = \u0026quot;striped\u0026quot;,full_width = T) %\u0026gt;%\rrow_spec(0,bold = T,font_size = 13,color = \u0026quot;grey\u0026quot;)\r\r\r13\r\r12\r\r9\r\r8\r\r11\r\r7\r\r50\r\r10\r\r6\r\r5\r\r4\r\r3\r\r2\r\r1\r\r0\r\r\r\r\r\r602\r\r634\r\r661\r\r721\r\r759\r\r864\r\r970\r\r989\r\r1124\r\r1389\r\r1402\r\r2150\r\r3044\r\r7093\r\r25729\r\r\r\r\r\rTop 10 Authors and Claps for their posts\rThere are only two posts which do not have Images in their content. The highest number of claps is 60,000, a post written by Sophia Ciocca under the title “How Does Spotify Know You So Well?”. Second Place is for the article “Blockchain is not only crappy technology but a bad vision for the future” which was written by Kai Stinchombe with 53,000 claps. De Xun is the only author who has two articles which are in this list on the places 8 and 9 with claps respectively 37,000 and 36,000.\n# seperate medium with author, titles, claps and image\rclaps_A_I\u0026lt;-medium[,c(\u0026quot;author\u0026quot;,\u0026quot;title\u0026quot;,\u0026quot;subtitle\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)] %\u0026gt;%\rarrange(claps) %\u0026gt;%\rtail(10)\rnames(claps_A_I)\u0026lt;-c(\u0026quot;Author\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Subtitle\u0026quot;,\u0026quot;Claps\u0026quot;,\u0026quot;Image\u0026quot;)\r# table it\rformattable(claps_A_I[,-3],align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;r\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rClaps=color_tile(\u0026quot;lightblue\u0026quot;,\u0026quot;blue\u0026quot;),\rImage=color_tile(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;)\r))\r\r\rAuthor\r\rTitle\r\rClaps\r\rImage\r\r\r\r\r\rVishal Maini\r\rA Beginners Guide to AI/ML\r\r36000\r\r1\r\r\r\rDe Xun\r\rSWIPE Bi-Weekly Update, 16th-27th July\r\r36000\r\r1\r\r\r\rDe Xun\r\r[PARTNERSHIP] SWIPE-Bluzelle: Building SWIPEs decentralized database\r\r37000\r\r1\r\r\r\rAndrej Karpathy\r\rSoftware 2.0\r\r38000\r\r0\r\r\r\rRadu Raicea\r\rWant to know how Deep Learning works? Heres a quick guide for everyone.\r\r39000\r\r1\r\r\r\rAnything App\r\rFast-forward twenty years with Anything App.\r\r42000\r\r0\r\r\r\rMichael Jordan\r\rArtificial IntelligenceThe Revolution Hasnt Happened Yet\r\r46000\r\r1\r\r\r\rXiaohan Zeng\r\rI interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers\r\r49000\r\r1\r\r\r\rKai Stinchcombe\r\rBlockchain is not only crappy technology but a bad vision for the future\r\r53000\r\r1\r\r\r\rSophia Ciocca\r\rHow Does Spotify Know You So Well?\r\r60000\r\r1\r\r\r\r\r\rTop 10 Authors with most posts\rYves Mulkers has most amount of posts with 487 but only 3,779 claps. This is not the case for Corsairs publishing where for 156 posts the number of claps are 111,501. Even looking at other authors names we can see that writing alot of posts does not create claps.\n# finding who are the top 10 authors with claps\r# summary.factor(medium$author) %\u0026gt;%\r# sort() %\u0026gt;%\r# tail(11)\r# extracting posts only from the top 10 authors with most posts\rTop10_author\u0026lt;-subset(medium,\rauthor ==\u0026quot;C Gavilanes\u0026quot; | author == \u0026quot;Jae Duk Seo\u0026quot; |\rauthor == \u0026quot;Corsair\u0026#39;s Publishing\u0026quot; | author==\u0026quot;Alibaba Cloud\u0026quot; |\rauthor == \u0026quot;Ilexa Yardley\u0026quot; | author == \u0026quot;Peter Marshall\u0026quot; |\rauthor == \u0026quot;AI Hawk\u0026quot; | author == \u0026quot;DEEP AERO DRONES\u0026quot; |\rauthor == \u0026quot;Synced\u0026quot; | author == \u0026quot;Yves Mulkers\u0026quot;)\r# plotting the top 10 authors\rg1_Top10_a\u0026lt;-ggplot(Top10_author,aes(x=author))+\rcoord_flip()+ geom_bar()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Top 10 Authors and number of posts they have written\u0026quot;)+\rgeom_text(stat = \u0026#39;count\u0026#39;,aes(label=..count..),hjust=0.5)\r# plotting the top 10 authors and their claps\rg2_Top10_a\u0026lt;-Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;)] %\u0026gt;%\rgroup_by(author) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rggtitle(\u0026quot;Total number of Claps they got\u0026quot;)+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+\rcoord_flip()+geom_text(aes(label=claps),hjust =0.5 )\r# plotting two plots at same grid\rgrid.arrange(g1_Top10_a,g2_Top10_a,ncol=1)\r\rTop 10 Authors who have posts with Images\rExtracting the top 10 authors with posts which have images it clear most of the posts do have Images and they do generate claps. This is true for Corsairs’s Publishing. With 154 posts it generates 109,906 claps. There are authors who have written more posts than totally received claps . It should be noted that two Authors did not add any Images for their post and they are Peter Marshall and C Gavilanes.\n# plotting top 10 authors with Image\rI1_g1_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;image\u0026quot;)],image==1) %\u0026gt;%\rggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 10 Authors and posts which has Images\u0026quot;)+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(label=..count..),hjust =0.5 )\r# plotting the claps for top 10 authors with Image I1_g2_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)],image==1) %\u0026gt;%\rgroup_by(author,image) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rcoord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+\rggtitle( \u0026quot;Total number of Claps they got\u0026quot;)+\rgeom_text(aes(label=claps),hjust =0.5 ) # top plots one grid\rgrid.arrange(I1_g1_Top10_a,I1_g2_Top10_a,ncol=1)\r\rTop 10 Authors who have posts without Images\rPosts without images have very low amount of total claps. To be specific 14 posts by Jae Duk Seo have 1833 claps but 2 posts by Corsair’s publishing has 1595 claps. That is very Impressive. Further there are even posts which have claps less than 10 where the number of posts is less than 5.\n# plotting top 10 authors with No Image\rI0_g1_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;image\u0026quot;)],image==0) %\u0026gt;%\rggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 10 Authors and posts without Images\u0026quot;)+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(label=..count..),hjust =0.5 )\r# plotting the claps for top 10 authors with No Image I0_g2_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)],image==0) %\u0026gt;%\rgroup_by(author,image) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rcoord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+ ggtitle(\u0026quot;Total number of claps they got\u0026quot;)+\rgeom_text(aes(label=claps),hjust =0.5 )\r# top plots one grid\rgrid.arrange(I0_g1_Top10_a,I0_g2_Top10_a,ncol=1)\r\rTop 10 Authors and Reading time for their posts\rReading in minutes, does it has anything to do with number of posts?. Looking at the plot it is clear that posts from Synced has more total reading time than Yves Mulkers with highest number posts. The difference between posts is close 150, while difference between reading times is above 150 for Yves Mulkers and Synced.\n# plotting top 10 authors with reading times\rRT_g1_Top10_a\u0026lt;-Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;reading_time\u0026quot;)] %\u0026gt;%\rgroup_by(author) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=reading_time))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rggtitle(\u0026quot;Reading Time\u0026quot;)+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Reading Time in minutes\u0026quot;)+ coord_flip()+geom_text(aes(label=reading_time),hjust =0.5 )\r# printing the above plot with number of posts grid.arrange(g1_Top10_a,RT_g1_Top10_a,ncol=1)\r\rTop 5 Publications with most posts\rPublications with most number of posts has the highest number of claps and order achieved\nin the “Top 5 publications and number of posts they have written” plot is maintained in the “Total number of Claps they got” plot as well. This simply refers, when you write alot of posts under a publication you will receive alot of claps because of the foundation this specific publications holds in Medium.\n# finding who are the top 5 publications with claps\r# summary.factor(medium$publication) %\u0026gt;%\r# sort() %\u0026gt;%\r# tail(11)\r# extracting posts only from the top 5 publications with most posts\rTop5_pub\u0026lt;-subset(medium,\rpublication ==\u0026quot;Towards Data Science\u0026quot; | publication == \u0026quot;Hacker Noon\u0026quot; |\rpublication == \u0026quot;Becoming Human: Artificial Intelligence Magazine\u0026quot; |\rpublication ==\u0026quot;Chatbots Life\u0026quot; |\rpublication == \u0026quot;Data Driven Investor\u0026quot; )\r# plotting the top 5 publications\rg1_Top5_p\u0026lt;-ggplot(Top5_pub,aes(x=str_wrap(publication,15)))+\rcoord_flip()+ geom_bar()+\rxlab(\u0026quot;Publication\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 5 Publications and number of posts they have written\u0026quot;)+\rgeom_text(stat = \u0026#39;count\u0026#39;,aes(label=..count..),hjust=0.5)\r# plotting the top 5 publications and their claps\rg2_Top5_p\u0026lt;-Top5_pub[,c(\u0026quot;publication\u0026quot;,\u0026quot;claps\u0026quot;)] %\u0026gt;%\rgroup_by(publication) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=str_wrap(publication,15),y=claps))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rxlab(\u0026quot;Publication\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+ ggtitle(\u0026quot;Total number of Claps they got\u0026quot;)+\rcoord_flip()+geom_text(aes(label=claps),hjust =0.5 )\r# plotting two plots at same grid\rgrid.arrange(g1_Top5_p,g2_Top5_p,ncol=1)\r\rWord Cloud for the Titles from Top 10 Authors\rWord cloud from the titles of the posts by Top 10 authors of most number of posts is below. The words thing, read, data, drone and new are with highest mentions. Where words such as big, telecom and tech are next in the line with higher amount of posts. In restrictions I have considered that this word cloud will have 1500 words and only if a word atleast holds the frequency of 10.\nWell, I could clearly see that there cannot be 1500 words here.\n#convert into data frame\rTop10_author\u0026lt;-data.frame(Top10_author)\r# Calculate Corpus\rTop10_author.Corpus\u0026lt;-Corpus(VectorSource(Top10_author$title))\r# clean the data\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Corpus,PlainTextDocument)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Corpus,tolower)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removeNumbers)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removeWords,stopwords(\u0026quot;english\u0026quot;))\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removePunctuation)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,stripWhitespace)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,stemDocument)\r# save as png\r#png(filename = \u0026quot;wordcloud1.png\u0026quot;,width = 1024,height = 768)\r# plot the word cloud\rwordcloud(Top10_author.Clean,max.words = 1500,min.freq = 10,\rcolors = brewer.pal(11,\u0026quot;Spectral\u0026quot;),random.color = FALSE,\rrandom.order = TRUE)\r\rWord Cloud for the Titles from Top 5 publications\rThis word cloud also has similar restrictions for number of words and minimum frequency for a word. Words such as data, learn, use, machin, network, deep, scienc and artifici have most amount of frequency. Further, words such as neural, intellig, chatbot, part and python are also with significant amount of frequency. Here we can see clearly see there can be more than 1000 words.\n#convert into data frame\rTop5_pub\u0026lt;-data.frame(Top5_pub)\r# Calculate Corpus\rTop5_pub.Corpus\u0026lt;-Corpus(VectorSource(Top5_pub$title))\r#clean the data\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Corpus,PlainTextDocument)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Corpus,tolower)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removeNumbers)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removeWords,stopwords(\u0026quot;english\u0026quot;))\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removePunctuation)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,stripWhitespace)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,stemDocument)\r# save as png\r#png(filename = \u0026quot;wordcloud2.png\u0026quot;,width = 1024,height = 768)\r# plot the word cloud\rwordcloud(Top5_pub.Clean,max.words = 1500,min.freq = 10,\rcolors = brewer.pal(11,\u0026quot;Spectral\u0026quot;),random.color = FALSE,\rrandom.order = TRUE)\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rUsing dplyr to manipulate the datset was useful when there is complication.\n\rgrid and gridExtra packages provide a safe way of combining multiple plots into one plot.\n\rformattable and kableExtra were crucial in generating tables which are informative.\n\rWord cloud or analyzing text is very useful and flexible when we use the above packages.\n\r\r\rFurther Analysis\r\rSimilarly we can do the above analysis for Top 5 publications and other variables.\n\rWord clouds for subtitles also will be interesting to see, specially focusing on authors and publications.\n\r\rPlease see that\nThis is my sixth post on the internet so my mistakes in grammar and spellings should be very less than previous posts. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1544227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544227200,"objectID":"f4ffa12d8ba15a17d69ab850903ae950","permalink":"/post/week_36/week-36-medium-posts/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/post/week_36/week-36-medium-posts/","section":"post","summary":"#loading packages\r#load data\rlibrary(readr)\r#manipulate data\rlibrary(dplyr)\rlibrary(magrittr)\r# format table with expense\rlibrary(formattable)\r# knitting the document\rlibrary(knitr)\r# another type of table\rlibrary(kableExtra)\r# playing with strings\rlibrary(stringr)\r# combining two plots\rlibrary(grid)\rlibrary(gridExtra)\r# that theme you wanted\rlibrary(ggthemr)\r# text analysis\rlibrary(tm)\rlibrary(SnowballC)\rlibrary(RColorBrewer)\rlibrary(wordcloud)\r# adding theme called fresh for plots\rggthemr(\u0026quot;fresh\u0026quot;)\r#loading the data\rmedium \u0026lt;- read_csv(\u0026quot;medium_datasci.csv\u0026quot;)\rattach(medium)\rFocusing on Claps with Authors and publications, where does writing alot of posts will get you popularity and claps.","tags":["R package","TidyTuesday","Medium"],"title":"Week 36: Medium Posts","type":"post"},{"authors":null,"categories":["fitODBOD"],"content":"\rIntroduction\rWhen we need to estimate parameters from a discrete distribution or continuous distribution or a function we can use the below mentioned R functions. We will be using the technique of maximizing the Log Likelihood function or minimizing the Negative Log Likelihood function. Based on this technique we will compare these R functions because it might benefit people who are struggling to which one to choose. We have 7 functions in total by my knowledge when I was writing this post.\nUsing the fitODBOD package, I will use the Alcohol Consumption data to try and model it for the Beta Binomial Distribution, which has two shape parameters to estimate. The process is that we find values for shape parameters a and b (or \\(\\alpha\\) and \\(\\beta\\)) which will maximize the Log Likelihood function of Beta Binomial Distribution or in our case minimize the Negative Log Likelihood function of Beta Binomial Distribution.\nAbove mentioned Beta-Binomial distributions Probability Mass function is denoted as\n\\[P_{BetaBin}(x)= {n \\choose x} \\frac{B(\\alpha+x,n+\\beta-x)}{B(\\alpha,\\beta)} \\]\nwhere \\(n=0,1,2,...\\), \\(x=0,1,2,...,n\\) and \\(\\alpha,\\beta \u0026gt; 0\\). Further \\(x\\) is the Binomial Random variable, a,b(or \\(\\alpha\\), \\(\\beta\\)) are shape parameters and \\(n\\) is the binomial trial value. Also B(\\(\\alpha\\),\\(\\beta\\)) is the beta function. In this distribution we have to estimate the values for a and b.\nFurther, using the PMF we can construct the Likelihood function for \\(\\Omega_{BB}=(\\alpha,\\beta)^T\\) as given below:\n\\[L(\\Omega_{BB}|x)=\\prod_{i=1}^{N} \\binom{n}{x_i} \\frac{B(\\alpha+x_i,n+\\beta-x_i)}{B(\\alpha,\\beta)}\\]\nwhere N is the Number of observations. Then Negative Log Likelihood function is given as\n\\[l(\\Omega_{BB}|x)=-\\sum_{i=1}^{N} log\\binom{n}{x_i} + \\sum_{i=1}^{N} log(B(\\alpha+x_i,n+\\beta-x_i)) - Nlog(B(\\alpha,\\beta))\\]\nIn the package fitODBOD we have the function EstMLEBetaBin which is constructed based on the above Negative Log Likelihood function and we will use it.\nWe take Log to transform the Likelihood function values into small values, which will simplify the computation process and save time. The estimation functions we need to compare will use specific mathematical methods to find the most appropriate shape parameter values.\nThe functions in question are\noptim\rnlm\rnlminb\rucminf\rmaxLik\rmle\rmle2\r\rFurther, I will focus on the attributes of the above functions considering from which package, Inputs, Outputs, Time to complete estimation and Analytical Methods used with the assistance of tables. Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.\nlibrary(fitODBOD)\rkable(Alcohol_data,\u0026quot;html\u0026quot;,align=c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 14,full_width = F) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\rDays\r\rweek1\r\rweek2\r\r\r\r\r\r0\r\r47\r\r42\r\r\r\r1\r\r54\r\r47\r\r\r\r2\r\r43\r\r54\r\r\r\r3\r\r40\r\r40\r\r\r\r4\r\r40\r\r49\r\r\r\r5\r\r41\r\r40\r\r\r\r6\r\r39\r\r43\r\r\r\r7\r\r95\r\r84\r\r\r\r\roptim Function\roptim is the first function in concern. Documentation of the optim function is useful and it indicates that this function is only used on one input situations only. This means our EstMLEBetaBin function has to be modified. Reason for this is that only the parameters that should be estimated need to be input values but our EstMLEBetaBin function has four parameters which are a,b,x(Binomial Random Variable) and freq(corresponding frequency values).\nWhile using optim function first index refers to \\(\\alpha\\) or a and second index refers to \\(\\beta\\) or b. Further, we have to input the observations or in our case the Binomial random variable values and their respective frequencies. I think it is inconvenient to modify the EstMLEBetaBin function, because if we want to estimate parameters for different data-sets it would become extra work. After modification we have a new function which is foroptim and I can use it for demonstration and comparison.\nBelow is the code to estimation and going through the outputs. It should be noted that we have to provide initial parameter values as an input to the optim function and it is best to provide values in the domain of\nvalues a and b.\nHere the shape parameters a and b are in the region of greater than zero but less than positive infinity. So for the initial parameters of a=0.1 and b=0.2 we are finding parameters which would minimize the Negative Log Likelihood function of Beta-Binomial distribution.\n# new function to facilitate optim criteria\r# only one input but has two elements\rforoptim\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\r}\r# optimizing values for a,b using default inputs\roptim_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim)\r# obtaining class of output\rclass(optim_answer)\r#length of output\rlength(optim_answer)\r# the outputs\roptim_answer$par # estimated values for a, b\roptim_answer$value # minimized function value optim_answer$counts # see the documentation to understand\roptim_answer$convergence # indicates successful completion\roptim_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\roptim_answer$par[1],optim_answer$par[2])\rSo the foroptim function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further we have scrutinized the outputs.\n\rpackage : stats\rNo of Inputs: 7\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 5\rNo of Analytical Methods : 6\rDefault Method : Nelder-Mead\r\r\rnlm Function\rnlm function is also similar to optim but only one analytical method will be used, which is a Newton-type Algorithm. Here also there need to be changes made to our EstMLEBetaBin function as previously. After making those changes we have called the new Negative Log-likelihood R function as fornlm. Then we can use the nlm function and estimate a and b which are the initial shape parameter values of 0.1 and 0.2 respectively. Documentation of nlm function is very useful to understand how it works.\nBelow is the code for using nlm function appropriately and fiddling with the results.\n#new function to facilitate nlm criteria\rfornlm\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\r}\r#optimizing values for a,b using default input\rnlm_answer\u0026lt;-nlm(f=fornlm,p=c(0.1,0.2))\r#obtaining class of output\rclass(nlm_answer)\r#length of output\rlength(nlm_answer)\r# the outputs\rnlm_answer$estimate # estimated values for a, b\rnlm_answer$minimum # minimized function value nlm_answer$gradient # gradient at the estimated minimum of given funciton\rnlm_answer$code # indicates successful completion\rnlm_answer$iterations # number of iterations performed\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rnlm_answer$estimate[1],nlm_answer$estimate[2])\rSimilarly for the fornlm function we have estimated values for a and b which would fit the Alcohol consumption data. of week 1.\n\rpackage : stats\rNo of Inputs: 12\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 5\rNo of Analytical Methods : 1\rDefault Method : Newton-type Algorithm\r\r\rnlminb Function\rnlminb is also similar and requires that EstMLEBetaBin function to be restructured as previous two situations. After this task we now have a function called fornlminb. nlminb function is based on analytical method of unconstrained and box-constrained optimization using PORT routines.\nAfter choosing initial parameter values for a and b, which are respectively 0.1 and 0.2 the estimation was done using nlminb function.\n# new function to facilitate nlminb criteria\rfornlminb\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default inputs\rnlminb_answer\u0026lt;-nlminb(start=c(0.1,0.2), objective=fornlminb)\r# obtaining class of output\rclass(nlminb_answer)\r# length of output\rlength(nlminb_answer)\r# the outputs\rnlminb_answer$par # estimated values for a, b\rnlminb_answer$objective # minimized function value nlminb_answer$evaluations # see the documentation to understand\rnlminb_answer$convergence # indicates successful completion\rnlminb_answer$message # additional information\rnlminb_answer$iterations # number of iterations performed\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rnlminb_answer$par[1],nlminb_answer$par[2])\rDocumentation includes the information related to the function therefore referring it will be useful. After estimating values for a and b using nlminb function these were noticed as before in input and output criteria\n\rpackage : stats\rNo of Inputs: 8\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 6\rNo of Analytical Methods : 1\rDefault Method : Unconstrained and box-constrained optimization using PORT routines\r\r\rucminf Function\rPackage uncminf produces the ucminf function, as previously mentioned functions here also we have to change the EstMLEBetaBin function. After making the changes we will be using the forucminf function to the estimation process of shape parameters a and b.\nWhen initial parameter values are set to a=0.1 and b=0.2 we will obtain results from ucminf function, which will minimize the Negative Log-likelihood function. Below is the code for estimation and using the results to understand the function ucminf.\nlibrary(ucminf)\r# new function to facilitate ucminf criteria\rforucminf\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default methods\rucminf_answer\u0026lt;-ucminf(par=c(0.1,0.2),fn=forucminf)\r#obtaining class\rclass(ucminf_answer)\r# length of output\rlength(ucminf_answer)\r# the outputs\rucminf_answer$par # estimated values for a, b\rucminf_answer$value # minimized function value ucminf_answer$invhessian.lt # see the documentation understand\rucminf_answer$convergence # indicates successful completion\rucminf_answer$message # additional information\rucminf_answer$info\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rucminf_answer$par[1],ucminf_answer$par[2])\rWith the help of R Documentation the estimation was done for shape parameter values a and b using the ucminf function. Below are the initial understandings of the inputs and outputs of ucminf function\n\rpackage : ucminf\rNo of Inputs: 5\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 6\rNo of Analytical Methods : 1\rDefault Method : Quasi-Newton Algorithm type with BFGS updating\r\r\rmaxLik Function\rmaxLik function is the cause of maxLik package, which only maximizes the Log Likelihood function. Therefore we have to restructure EstMLEBetaBin as previously mentioned but as an addition a negative sign is added for the output. This new function will be called as formaxLik.\nFor the initial parameter values where a=0.1 and b=0.2 the maxLik function will be used and results will be evaluated as below.\nlibrary(maxLik)\r# new function to facilitate maxLik criteria\rformaxLik\u0026lt;-function(a)\r{\r-EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default methods\rmaxLik_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2))\r# obtaining class of output\rclass(maxLik_answer)\r# length of output\rlength(maxLik_answer)\r# the outputs\rmaxLik_answer$estimate # estimated values for a, b\rmaxLik_answer$maximum # minimized function value maxLik_answer$iterations # no of iterations to succeed\rmaxLik_answer$gradient # last gradient value which was calculated\rmaxLik_answer$message # additional information\rmaxLik_answer$hessian # hessian matrix\rmaxLik_answer$code # indicates successful completion\rmaxLik_answer$fixed # logical vector indicating which parameters are constants\rmaxLik_answer$type # type of maximization\rmaxLik_answer$last.step # list describing the last unsuccessful step\rmaxLik_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmaxLik_answer$estimate[1],maxLik_answer$estimate[2])\rUsing the Documentation of maxLik and further using the Documentation of maxNR function. According to the code above below are the findings in for maxLik function in point form\n\rpackage : maxLik\rNo of Inputs: 6\rMinimum required Inputs : 2\rClass of output : list or class of maxim or class of maxLik\rNo of outputs: 11\rNo of Analytical Methods : 7\rDefault Method : Automatically chosen\r\r\rmle Function\rstats4 package is installed so that mle function can be operated for the purpose of estimating a and b parameters. Here also as previously we need to make some changes as below and create a new estimating function called formle.\nFor the initial parameter values of a=0.1 and b=0.2 the Negative Log Likelihood value has been minimized using mle function. Below is the code for estimation and investigation from the outputs of mle after estimation.\nlibrary(stats4)\r# new function to facilitate mle criteria\rformle\u0026lt;-function(a,b)\r{\rEstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)\r}\r# optimizing values for a,b using default methods\rmle_answer\u0026lt;-mle(minuslogl=formle,start = list(a=0.1,b=0.2))\r# obtaining class\rclass(mle_answer)\r# length of output\rlength(mle_answer)\r# the outputs\rmle_answer@call # inputs i have used mle_answer@coef # estimated values for a,b\rmle_answer@fullcoef # all values, even the fixed values we did not want to estimate\rmle_answer@vcov # variance covariance matrix for a,b\rmle_answer@min # minimized function value\rmle_answer@details # details after estimation process\rmle_answer@nobs # number of observations to be used for computing only if given mle_answer@method # optimization methods used\r# Methods used\rconfint(mle_answer) # confidence intervals for estimated values\rlogLik(mle_answer) # Negative loglikelihood value for estimated values profile(mle_answer) # Likelihood profile generation.\rnobs(mle_answer) # number of observations to be used for computing only if given\rshow(mle_answer) # display object briefly\rsummary(mle_answer) # generate a summary\r#update() # updating if we have new data and need to estimate new values\rvcov(mle_answer) # variance covariance matrix\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmle_answer@coef[1],mle_answer@coef[2])\rIt should be noted that in all 5 previous function we only saw lists of output, but through mle we are seeing a class of mle output.The Documention explains the inputs and outputs of mle. Documentation for mle-class explains further about the methods that can be used. Further, below is a list of findings based on the outputs from mle function.\n\rpackage : stats4\rNo of Inputs: 5\rMinimum required Inputs : 2\rClass of output : class of mle\rNo of outputs: 9\rMethods for output: 8\rNo of Analytical Methods :6\rDefault Method : Nelder and Mead\r\r\rmle2 Function\rmle2 function is advanced than mle function and it is from the package bbmle. Here, there is no need to modify the EstMLEBetabin function from the fitODBOD package to estimation as all previous situations .\nFor the initial parameter values of a=0.1 and b=0.2 Negative Log Likelihood function will be minimized where outputs will be investigated and methods related to output in class of mle2 will be used.\nBelow is the code for using mle2 function and scrutinizing the output and methods relate to it.\nlibrary(bbmle)\r# optimizing values for a,b using default methods\rmle2_answer\u0026lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),\rdata = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))\r# obtaining class\rclass(mle2_answer)\r# length of output\rlength(mle2_answer)\r# the outputs\rmle2_answer@call # inputs generally considered mle2_answer@call.orig # inputs i have given\rmle2_answer@coef # estimated values for a,b\rmle2_answer@fullcoef # all values, even the fixed values we did not want to estimate\rmle2_answer@vcov # variance covariance matrix for a,b\rmle2_answer@min # minimized function value\rmle2_answer@details # details after estimation process\rmle2_answer@method # optimization methods used\rmle2_answer@data # data used for estimation mle2_answer@formula # if a formula was specified in the input mle2_answer@optimizer # function used for optimizing\r# Methods used\rcoef(mle2_answer) # extrat the estimated values\rconfint(mle2_answer) # confidence intervals for estimated values\rshow(mle2_answer) # display object briefly\rsummary(mle2_answer) # generate a summary\r#update() #updating if we have new data and need to estimate new values\rvcov(mle2_answer) # variance covariance matrix\r#formula(mle2_answer) # if a formula was specified in the input #plot(mle2_answer) # plot the profile\rlogLik(mle2_answer) # Negative loglikelihood value for estimated values profile(mle2_answer) # profile of estimated values\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmle2_answer@coef[1],mle2_answer@coef[2])\rAfter estimation, fiddling with the output, with the Documention of mle2 function and mle2-class Documentation few findings are mentioned below in point form.\n\rpackage : bbmle\rNo of Inputs: 22\rMinimum required Inputs : 3\rClass of output : class of mle2\rNo of outputs: 12\rMethods for output: 9\rNo of Analytical Methods : 6\rDefault Method : Nelder and Mead\r\r\r\rSummary of package Information of the R functions\rAfter completing a brief fact finding of the R functions optim, nlm, nlminb, ucminf maxLik, mle and mle2 we can record the facts and results in a table. Using this table we can choose the best suitable function for our needs of estimating parameters.\nI would prefer mle2 function than others because it provides special methods to handle the mle2 outputs and the output themselves are very thorough than other R functions. It can be seen that even though estimated a and b shape parameter values seem different only from the fourth decimal point in between all outputs. But this does not effect the minimized Negative Log Likelihood function which is same for all seven function outputs.\n\r\rFunction\r\roptim\r\rnlm\r\rnlminb\r\rucminf\r\rmaxLik\r\rmle\r\rmle2\r\r\r\r\r\rPackage\r\rstats\r\rstats\r\rstats\r\rucminf\r\rmaxLik\r\rstats4\r\rbbmle\r\r\r\rNo of Inputs\r\r7\r\r12\r\r8\r\r5\r\r6\r\r5\r\r22\r\r\r\rMinimum No of Inputs\r\r2\r\r2\r\r2\r\r2\r\r2\r\r2\r\r3\r\r\r\rAnalytical Methods\r\r6\r\r1\r\r1\r\r1\r\r7\r\r6\r\r6\r\r\r\rOutput Type\r\rList\r\rList\r\rList\r\rList\r\rList class maxLik class maxim\r\rclass mle\r\rclass mle2\r\r\r\rNo of Outputs\r\r5\r\r5\r\r6\r\r6\r\r11\r\r9\r\r12\r\r\r\rNo of Methods\r\rNone\r\rNone\r\rNone\r\rNone\r\rNone\r\r8\r\r9\r\r\r\rInitial value(a,b)\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\r\r\rEstimated value(a,b)\r\ra=0.7230707 b=0.5809894\r\ra=0.7229384 b=0.5808448\r\ra=0.7229404 b=0.5808469\r\ra=0.7229390 b=0.5808458\r\ra=0.7229428 b=0.5808488\r\ra=0.7228930 b=0.5807279\r\ra=0.7228930 b=0.5807279\r\r\r\rNegative Log Likelihood value\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r\r\r\r\rSummary of Time evaluation for the R functions\rFurther at the beginning I have considered to evaluate the time of system process for the functions to produce results and compare them. In order to do this time comparison it is possible to use the benchmark function of rbenchmark package and below mentioned code chunk provides the output in a table form which includes the functions and their respective time values. The estimation process of the parameters where each function has been replicated 1000 times to receive a more accurate and strong table of time values.\nThe table is order according to the elapsed time value column in ascending order. According to this we can see that least time takes to the nlminb function and most time is taken to the mle2 function. These times complete depends on the Negative Log Likelihood function you need to minimize, the data you provided, the number of estimators that needs to be estimated and finally the complexity of the function.\nTherefore based on the needs of your output and the function which needs estimation choose the best function. Because as we say in the earlier table the estimated values are slightly different but does not make any strong influence on the results.\nlibrary(rbenchmark)\rResults2\u0026lt;-benchmark(\r\u0026quot;optim\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim)},\r\u0026quot;nlm\u0026quot;={nlm(f = fornlm, p = c(0.1,0.2))},\r\u0026quot;nlminb\u0026quot;={nlminb(start = c(0.1,0.2), objective = fornlminb)},\r\u0026quot;ucminf\u0026quot;={ucminf(par = c(0.1,0.2), fn = forucminf)},\r\u0026quot;maxLik\u0026quot;={maxLik(logLik = formaxLik, start = c(0.1,0.2))},\r\u0026quot;mle\u0026quot;={mle(minuslogl = formle, start = list(a=0.1,b=0.2))},\r\u0026quot;mle2\u0026quot;={mle2(minuslogl = EstMLEBetaBin, start = list(a=0.1, b=0.2),\rdata = list(x=Alcohol_data$Days, freq=Alcohol_data$week1))},\rreplications = 1000,\rcolumns = c(\u0026quot;test\u0026quot;,\u0026quot;replications\u0026quot;,\u0026quot;elapsed\u0026quot;,\r\u0026quot;relative\u0026quot;,\u0026quot;user.self\u0026quot;,\u0026quot;sys.self\u0026quot;),\rorder = \u0026#39;elapsed\u0026#39;\r)\rkable(Results2,\u0026quot;html\u0026quot;,align = c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(full_width = F,bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 10) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\r\rtest\r\rreplications\r\relapsed\r\rrelative\r\ruser.self\r\rsys.self\r\r\r\r\r\r3\r\rnlminb\r\r1000\r\r6.97\r\r1.000\r\r6.95\r\r0.00\r\r\r\r4\r\rucminf\r\r1000\r\r8.42\r\r1.208\r\r8.41\r\r0.00\r\r\r\r2\r\rnlm\r\r1000\r\r9.68\r\r1.389\r\r9.55\r\r0.02\r\r\r\r1\r\roptim\r\r1000\r\r11.28\r\r1.618\r\r11.22\r\r0.00\r\r\r\r6\r\rmle\r\r1000\r\r25.63\r\r3.677\r\r25.34\r\r0.03\r\r\r\r5\r\rmaxLik\r\r1000\r\r40.71\r\r5.841\r\r40.34\r\r0.02\r\r\r\r7\r\rmle2\r\r1000\r\r45.76\r\r6.565\r\r45.28\r\r0.06\r\r\r\r\r\rSummary of Results after estimating parameters using the R functions\rAfter using the functions optim, nlm, nlminb, ucminf, maxLik, mle and mle2 to estimate the shape parameters a, b we can use the estimated parameters in the function fitBetaBin. Using this function we can find expected frequencies for each of these functions and compare p-values and over-dispersion and understand if using different estimation functions had any effect on them.\nAccording to the below table there is no significant changes between the expected frequencies, p-values or the over dispersion values. This is a clear indication of it does not matter what function we use the estimation will occur effectively but only efficiency will be affected.\n\r\rBinomialRandomVariable\r\rFrequency\r\roptim\r\rnlm\r\rnlminb\r\rucminf\r\rmaxLik\r\rmle\r\rmle2\r\r\r\r\r\r0\r\r47\r\r54.61\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r\r\r1\r\r54\r\r42\r\r42\r\r42\r\r42\r\r42\r\r42\r\r42\r\r\r\r2\r\r43\r\r38.91\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r\r\r3\r\r40\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r\r\r4\r\r40\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r\r\r5\r\r41\r\r44\r\r44\r\r44\r\r44\r\r44\r\r43.99\r\r43.99\r\r\r\r6\r\r39\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r\r\r7\r\r95\r\r87.77\r\r87.78\r\r87.78\r\r87.78\r\r87.78\r\r87.8\r\r87.8\r\r\r\rTotal No of Observations\r\r399\r\r398.99\r\r399\r\r399\r\r399.01\r\r399\r\r399.01\r\r399.01\r\r\r\rp-value\r\r\r0.0902\r\r0.0901\r\r0.0901\r\r0.0901\r\r0.0901\r\r0.0903\r\r0.0903\r\r\r\rOver Dispersion\r\r\r0.4340165\r\r0.4340686\r\r0.4340679\r\r0.4340683\r\r0.434067\r\r0.4340992\r\r0.4340992\r\r\r\r\r\rFinal conclusion\rWe had 7 functions to compare but choosing one over the other is completely harmless to the final result of estimation as seen by our tables. The only issue is time, therefore I would recommend chose the best function based on your needs of output and research objective.\nThank You\n\r","date":1543881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543881600,"objectID":"4d5c6f53598f1fd0c54e06c65bec9ee8","permalink":"/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/","publishdate":"2018-12-04T00:00:00Z","relpermalink":"/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/","section":"post","summary":"Introduction\rWhen we need to estimate parameters from a discrete distribution or continuous distribution or a function we can use the below mentioned R functions. We will be using the technique of maximizing the Log Likelihood function or minimizing the Negative Log Likelihood function. Based on this technique we will compare these R functions because it might benefit people who are struggling to which one to choose. We have 7 functions in total by my knowledge when I was writing this post.","tags":["bbmle","fitODBOD","maxLik","mle","mle2","nlm","nlminb","optim","ucminf"],"title":"Benchmarking Maximum Likelihood Estimation functions from R","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"# loading the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(stringr)\rlibrary(ggthemr)\rlibrary(gganimate)\rlibrary(formattable)\r# load the theme flat dark\rggthemr(\u0026quot;flat dark\u0026quot;)\r# reading the data\rbridges \u0026lt;- read_csv(\u0026quot;baltimore_bridges.csv\u0026quot;)\r#View(bridges)\r# naming the columns\rnames(bridges)\u0026lt;-c(\u0026quot;lat\u0026quot;,\u0026quot;long\u0026quot;,\u0026quot;County\u0026quot;,\u0026quot;Carries\u0026quot;,\u0026quot;Year Built\u0026quot;,\r\u0026quot;Condition\u0026quot;,\u0026quot;Average Daily Traffic\u0026quot;,\u0026quot;Total Improvement\u0026quot;,\r\u0026quot;Month\u0026quot;,\u0026quot;Year\u0026quot;,\u0026quot;Owner\u0026quot;,\u0026quot;Responsibility\u0026quot;,\u0026quot;Vehicles\u0026quot;)\rattach(bridges)\rBridge Data and Baltimore\rData for the analysis and description about the Baltimore bridges are hyper-linked. Further , my tweet is also in this hyperlink.\nData on bridges is of week 35 from TidyTuesday. Trying to explain the data using maps is obvious, yet I will use animated jitter plots. There are 13 variables and 2079 observations. Brave choice of limiting my self to less than 10 variables, where latitude, longitude and Vehicles will not be taken into account.\nSo with the help of packages tidyverse, ggthemr, gganimate,formattable and readr I will complete this analysis. Most of the bridges are owned by several agencies, but I will only focus on the top three ownership holders.\nCounties which have bridges owned by State Highway Agency\rClose to 1000 bridges are owned by State Highway Agency, where most of them are in Baltimore County. High amount of bridges are in good condition, further more bridges are in Fair condition and only around 10 bridges in Poor condition.\nConsidering the Average Daily Traffic only one bridge in Poor condition has the amount of close to 110,000, while all the other poor condition bridges have Average Daily Traffic less than 30,000. While counties Anne Arundel and Hartford have no Poor condition bridges at all.\nMost of the bridges are from Baltimore County and around 20 bridges have count of more than 150,000 Average Daily Traffic for both Fair and Good conditions. Hartford and Carroll Counties have their Average Daily Traffic which does not exceed 80,000 at any condition of the bridge.\n# jitter plot to State Highway Agency\rggplot(subset(bridges,Owner==\u0026quot;State Highway Agency\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+\rggtitle(\u0026quot;Condition of Bridges owned by State Highway Agency \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,230000,10000) ,breaks = seq(0,230000,10000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+geom_jitter()\r\rCounties which have bridges owned by County Highway Agency\rCounty Highway Agency owns the second most amount of bridges in this data-set. Therefore using jitter plots we are going to check how the condition of the bridges and counties are explained in the simplest manner.\nLess amount of poor condition bridges in all counties except Anne Roundel County. All bridges owned by County Highway Agency have a limited Average Daily Traffic less than 50,000. Clearly we have more Fair bridges than Good ones. In the Poor condition category only two have Average Daily Traffic more than 20,000, while other two have more than 10 bridges.\nMost of these bridges are in Baltimore County even it is in any one of three conditions. There are few bridges which have values more than 40,000 Average Daily Traffic and they are also in Baltimore County.\nThere are bridges which have Zero Average Daily Traffic. In all three Conditions only Hartford County has bridges which has Average Daily Traffic less than 10,000.\n# jitter plot to County Highway Agency\rggplot(subset(bridges,Owner==\u0026quot;County Highway Agency\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+ geom_jitter()+\rggtitle(\u0026quot;Condition of Bridges owned by County Highway Agency \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,40000,5000) ,breaks = seq(0,40000,5000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rCounties which have bridges owned by State Toll Authority\rThere is only bridge which is in Poor condition and it is in Baltimore County, while counties Howard and Anne Arundel have no Good condition bridges. Further there is only 3 Fair condition bridges in Howard County while they have values for Average Daily Traffic less than 10,000.\nThe highest Average Daily Traffic is close to 170,000 which are only 4 and are in Good and Fair conditions. Further, Anne Arundel County has only one Good bridge and in Hartford it is six bridges. Only few of the bridges have Average Daily Traffic close to zero.\n# jitter plot to State tolll authority\rggplot(subset(bridges,Owner==\u0026quot;State Toll Authority\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+geom_jitter()+\rggtitle(\u0026quot;Condition of Bridges owned by State Toll Authority \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,170000,10000) ,breaks = seq(0,170000,10000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rMost amount of bridges Built based on Year\rYears 1957, 1970, 1975, 1991, 1963 and 1961 have the top 6 spots for building more than 50 bridges in those years. If we consider the conditions of Fair and Good only the year 1991 is suitable to be mentioned, while all other years has at-least one Poor condition bridge. Further There are more Poor condition bridges in 1961 than in 1957. While all Poor condition bridges has Average Daily Traffic less than 50,000.\nFinally, there are only few bridges which have Average Daily Traffic above 100,000 and only 3 are in Good condition. There are Bridges which can have Average Daily Traffic close to zero in all 6 years and all conditions.\n# jitter plot to years based on Most amount of bridges built\rggplot(subset(bridges,`Year Built`==\u0026quot;1957\u0026quot; | `Year Built`==\u0026quot;1970\u0026quot; | `Year Built`==\u0026quot;1975\u0026quot; | `Year Built`==\u0026quot;1991\u0026quot; |\r`Year Built`==\u0026quot;1963\u0026quot; | `Year Built`==\u0026quot;1961\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=factor(`Year Built`)))+\rxlab(\u0026quot;Year Built\u0026quot;)+ylab(\u0026quot;Average Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Most amount of Bridges built based on Years \\nand their Conditions\u0026quot;)+\rgeom_jitter()+legend_bottom()+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rAverage Traffic Less than or equal to 100,000 for Counties with Bridge Condition\rWhile obtaining summary for county variable there is one issue because there are two observations which say “Baltimore city” than “Baltimore City” and I don’t want to change them.\nIf we focus on Average Daily Traffic less than or equal to 100,000 based on County and Condition. It is clear that Poor condition bridges are part of this criteria and mostly Average Daily Traffic is less than 5000 for Counties Howard, Hartford and Carroll. While Baltimore County has highest amount up-to 75,000, but Baltimore County has highest amount close to 40,000 for Average Daily Traffic. Finally Anne Arundel County has only one Poor condition bridge which has Average Daily Traffic close to zero.\nWe can see that there are more Fair Condition bridges than Good ones. In Baltimore County most of the Fair condition bridges have Average Daily Traffic less than 15000. Similarly Carroll county and Hartford county also behave under such criteria. But for Good condition bridges this is not the case where there is no certain strong dense region as similar to Fair condition bridges.\nPreviously when we looked into county we did not see Baltimore City often as a factor, but here that is not the case.\n# jitter plot to average daily Traffic less than or equal 1000000\rggplot(subset(bridges,`Average Daily Traffic`\u0026lt;=100000 \u0026amp; County!=\u0026quot;Baltimore city\u0026quot;),\raes(x=County,y=`Average Daily Traffic`,color=Condition))+\rxlab(\u0026quot;County\u0026quot;)+ylab(\u0026quot;Averag Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Average Daily Traffic Less than 100,000 \\nFor Counties\u0026quot;)+\rscale_y_continuous(labels = seq(0,100000,5000),breaks = seq(0,100000,5000))+\rtheme(axis.text.x = element_text(angle = -90))+coord_flip()+ geom_jitter()+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rAverage Traffic More than 100,000 for Counties with Bridge Condition\rThis Jitter plot is completely different than previous one, because there are no clear dense regions for any counties and conditions of the bridge. There is only one Poor condition bridge in Baltimore County where the Average Daily Traffic is close to 115,000. In Fair condition bridges also Baltimore County holds the most, while they are slightly dense in the region of 175,000 to 190,000. while for Howard County similar density occurs between 190,000 to 205,000. Bridges in Good condition have more higher values in Baltimore County than Anne Arundel County.\n# jitter plot to average daily Traffic more than 1000000\rggplot(subset(bridges, `Average Daily Traffic` \u0026gt; 100000 \u0026amp; County != \u0026quot;Baltimore city\u0026quot;),\raes(x=County,y=`Average Daily Traffic`,color=Condition))+\rxlab(\u0026quot;County\u0026quot;)+ylab(\u0026quot;Average Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Average Daily Traffic More than 100,000 \\nFor Counties\u0026quot;)+\rscale_y_continuous(labels=seq(100000,230000,5000),breaks=seq(100000,230000,5000))+\rcoord_flip()+ theme(axis.text.x = element_text(angle = -90))+\rgeom_jitter()+transition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rImprovement and Bridge Conditions with Counties\rIn the variable of Total Improvement there are 1438 missing values, 42 values are zero and the rest are actual values. I am going to look at Total Improvement in two tables. First table will include where Bridges have Total Improvement higher than 9,999,000 and less than 30,000,000. Second table is for Bridges which have Total Improvement higher than or equal to 30,000,000.\nFurther to make these tables interesting I will be using the package formattable, and colors and tiles for numerical values. In the first table there are 7 bridges while only Anne Arundel County holds 3 and Baltimore City holds 4. One bridge is from 1953, and others are from the period of 1977 to 1983. Conditions of these bridges are mostly Fair and two bridges are in Good condition. Lowest Average Daily Traffic is 11760, while highest is 124193, where both bridges are in Fair Condition, and the amount spent on them for Total Improvement are respectively 18,163,000 and 16,264,000. The bridge with Highest amount of Average Daily traffic is built in 1953.\n# removing unnecessary columns and setting restriction to # Total Improvement\rTop10\u0026lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` \u0026gt; 9999 \u0026amp; `Total Improvement` \u0026lt; 30000)\r# setting colours\rcustomRed0 = \u0026quot;#FF8080\u0026quot;\rcustomRed = \u0026quot;#7F0000\u0026quot;\rcustomyellow0 = \u0026quot;#FFFF80\u0026quot;\rcustomyellow = \u0026quot;#BFBF00\u0026quot;\rcustomblue0 = \u0026quot;#6060BF\u0026quot;\rcustomblue = \u0026quot;#00007F\u0026quot;\r# creating the table for above data set\rformattable(Top10,align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rCounty =formatter(\u0026quot;span\u0026quot;,style= ~style(color=\u0026quot;grey\u0026quot;)),\r`Total Improvement`=color_tile(customblue0,customblue),\r`Average Daily Traffic`=color_tile(customyellow0,customyellow),\r`Year Built`=color_tile(customRed0,customRed)\r))\r\r\rCounty\r\rCarries\r\rYear Built\r\rCondition\r\rAverage Daily Traffic\r\rTotal Improvement\r\r\r\r\r\rAnne Arundel County\r\rMD 2\r\r1983\r\rFair\r\r53221\r\r13504\r\r\r\rAnne Arundel County\r\rUS 50\r\r1953\r\rFair\r\r124193\r\r16264\r\r\r\rAnne Arundel County\r\rUPPER LEVEL ROADWA\r\r1977\r\rFair\r\r11760\r\r18163\r\r\r\rBaltimore City \r\rIS 95 SB\r\r1977\r\rFair\r\r94765\r\r15785\r\r\r\rBaltimore City \r\rIS 95 VIADUCT SB\r\r1980\r\rGood\r\r63650\r\r16051\r\r\r\rBaltimore City \r\rIS 95 VIADUCT NB\r\r1980\r\rFair\r\r52850\r\r20484\r\r\r\rBaltimore City \r\rIS 95 SB\r\r1980\r\rGood\r\r55621\r\r20484\r\r\r\r\rWhen I did try to plot the top ten bridges with most Total improvement values there was one issue, which is the distance between first two values and the next 8 values. Therefore I divided the table into two.\nIn this second table We can see there are two bridges which are from Baltimore City and are built in 1980 and 1971, but the amount spent on Total Improvement is 300,000,000 each. But their Average Daily Traffic is respectively 56280 and 30600.\nWhile we have another bridge from Baltimore City and built in 1907, but Total Improvement amount is 35,026,000. Here, the Average Daily Traffic is 3900.\n# removing unnecessary columns and setting restriction to # Total Improvement\rTop3\u0026lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` \u0026gt;= 30000)\r# setting colours\rcustomRed0 = \u0026quot;#FF8080\u0026quot;\rcustomRed = \u0026quot;#7F0000\u0026quot;\rcustomyellow0 = \u0026quot;#FFFF80\u0026quot;\rcustomyellow = \u0026quot;#BFBF00\u0026quot;\rcustomblue0 = \u0026quot;#6060BF\u0026quot;\rcustomblue = \u0026quot;#00007F\u0026quot;\r# creating the table for above data set\rformattable(Top3,align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rCounty =formatter(\u0026quot;span\u0026quot;,style= ~style(color=\u0026quot;black\u0026quot;)),\r`Total Improvement`=color_tile(customblue0,customblue),\r`Average Daily Traffic`=color_tile(customyellow0,customyellow),\r`Year Built`=color_tile(customRed0,customRed)\r))\r\r\rCounty\r\rCarries\r\rYear Built\r\rCondition\r\rAverage Daily Traffic\r\rTotal Improvement\r\r\r\r\r\rBaltimore City\r\rUS 40 EDMONDSON AV\r\r1907\r\rPoor\r\r3900\r\r35026\r\r\r\rBaltimore City\r\rIS 95 VIADUCT SB\r\r1980\r\rFair\r\r56280\r\r300000\r\r\r\rBaltimore City\r\rEASTERN AVENUE\r\r1971\r\rFair\r\r30600\r\r300000\r\r\r\r\r\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rJitter plots and animation are useful in explaining one continuous variable with multiple categorical variables.\n\rSub-setting the data set and applying formattable package is useful to explain different continuous values with in a table.\n\r\r\rFurther Analysis\r\rSimilarly we can use mapping to point out the locations of the bridges and use animation to make it more clear.\r\rPlease see that\nThis is my fifth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1543363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543363200,"objectID":"badc88246b79ceb83ebc0429f4a042e2","permalink":"/post/week_35/week-35-baltimore-bridges/","publishdate":"2018-11-28T00:00:00Z","relpermalink":"/post/week_35/week-35-baltimore-bridges/","section":"post","summary":"# loading the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(stringr)\rlibrary(ggthemr)\rlibrary(gganimate)\rlibrary(formattable)\r# load the theme flat dark\rggthemr(\u0026quot;flat dark\u0026quot;)\r# reading the data\rbridges \u0026lt;- read_csv(\u0026quot;baltimore_bridges.csv\u0026quot;)\r#View(bridges)\r# naming the columns\rnames(bridges)\u0026lt;-c(\u0026quot;lat\u0026quot;,\u0026quot;long\u0026quot;,\u0026quot;County\u0026quot;,\u0026quot;Carries\u0026quot;,\u0026quot;Year Built\u0026quot;,\r\u0026quot;Condition\u0026quot;,\u0026quot;Average Daily Traffic\u0026quot;,\u0026quot;Total Improvement\u0026quot;,\r\u0026quot;Month\u0026quot;,\u0026quot;Year\u0026quot;,\u0026quot;Owner\u0026quot;,\u0026quot;Responsibility\u0026quot;,\u0026quot;Vehicles\u0026quot;)\rattach(bridges)\rBridge Data and Baltimore\rData for the analysis and description about the Baltimore bridges are hyper-linked. Further , my tweet is also in this hyperlink.\nData on bridges is of week 35 from TidyTuesday.","tags":["TidyTuesday","R"],"title":"Week 35: Baltimore Bridges","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r# Load the packges\rlibrary(ggplot2)\rlibrary(ggthemr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(tidyverse)\rlibrary(tweenr)\rlibrary(gganimate)\rlibrary(kableExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(readr)\r#load the data\rThanksgiving\u0026lt;-read_csv(\u0026quot;thanksgiving_meals.csv\u0026quot;)\r# apply the theme grape\rggthemr(\u0026quot;grape\u0026quot;)\r#subset the people who said yes for celebrating thanksgiving\rThanksgiving_Yes\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;Yes\u0026quot;)\r#subset the people who said no for celebrating thanksgiving\rThanksgiving_No\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;No\u0026quot;)\rData set was provided on week 34 for TidyTuesday analysis. As it is Thanksgiving week this is understandable. You can receive the data set here. There are more than 65 variables and 1058 observations. The data was acquired buy a survey conducted online and information about them are here.\nThis is my tweet on week 34\nPeople who do and do not celebrate Thanksgiving\rIn this ThanksGiving data set 980 are celebrating, and 78 are not celebrating Thanksgiving. I will use plots to understand their composition and tables to explain them further.\nVariables in consideration for this task is none other than Age, Gender, Family Income and US regions. Finally my aim is to create animated plots and interactive tables for the above variables through the help of packages gganimate and kable.\nAge Distribution\rFirst the age distribution has only 4 groups, where people who celebrate Thanksgiving in the age category of 18-29 is the very least. Highest count goes to the age category of 45-59 with 269. There are 33 missing observations and they were removed.\nConsidering the people who do not celebrate Thanksgiving the least count of 6 goes to category of 60+, but here the category of 18-29 has the highest count of 31. No missing observations were recorded here.\nBelow is an animated bar plot where the counts change for their respective 4 categories. As 90% of respondents have answered Yes for celebrating Thanksgiving and rest have answered No we can clearly see the count differences\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate dont_age\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$age))\r# people who do celebrate\rdo_age\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$age)))\r# people who do celebrate\rdata_do_age\u0026lt;-data.frame(group=c(\u0026quot;18-29\u0026quot;,\u0026quot;30-44\u0026quot;,\u0026quot;45-59\u0026quot;,\u0026quot;60+\u0026quot;),\rvalues=do_age$`summary.factor(na.omit(Thanksgiving_Yes$age))`,\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,4))\r# people who do not celebrate\rdata_dont_age\u0026lt;-data.frame(group=c(\u0026quot;18-29\u0026quot;,\u0026quot;30-44\u0026quot;,\u0026quot;45-59\u0026quot;,\u0026quot;60+\u0026quot;),\rvalues=dont_age$`summary.factor(Thanksgiving_No$age)`,\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,4))\r# combining both\rdata_age\u0026lt;-rbind(data_do_age,data_dont_age)\r# animated bar plot for people who do celebrate and who do not celebrate ggplot(data_age,aes(x=factor(group),values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Age Group\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Age\u0026quot;)+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;cubic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rAge with Other Factors\rFirst table is Age vs Gender for people who celebrate Thanksgiving. All age categories have a percentage range in between 19 and 29. Highest percentage of 28.4055 is for Age category 45 - 59. Female have a higher percentage of 54.3823.\nFemale who are 60+ have the highest percentage of 15.2059, while lowest percentage of 8.7645 is for male in the age category of 18-29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,gender))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,gender)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\rcolumn_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rFemale\r\rMale\r\rSum\r\r\r\r\r\r18 - 29\r\r10.7709\r\r8.7645\r\r19.5354\r\r\r\r30 - 44\r\r13.4108\r\r11.4044\r\r24.8152\r\r\r\r45 - 59\r\r14.9947\r\r13.4108\r\r28.4055\r\r\r\r60+\r\r15.2059\r\r12.0380\r\r27.2439\r\r\r\rSum\r\r54.3823\r\r45.6177\r\r100.0000\r\r\r\r\rdetach(Thanksgiving_Yes)\rWhen considering the people who do not celebrate Thanksgiving, highest percentage of 62.8205 is for Male, while age category of 18-29 have the highest percentage of 39.7436.\nMale who are in between 18 and 29 have the highest percentage of 25.6410, while Female who are above 60 have the lowest percentage of 2.5641.\nattach(Thanksgiving_No)\r#kable(addmargins(table(age,gender))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,gender)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\rcolumn_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rFemale\r\rMale\r\rSum\r\r\r\r\r\r18 - 29\r\r14.1026\r\r25.6410\r\r39.7436\r\r\r\r30 - 44\r\r11.5385\r\r19.2308\r\r30.7693\r\r\r\r45 - 59\r\r8.9744\r\r12.8205\r\r21.7949\r\r\r\r60+\r\r2.5641\r\r5.1282\r\r7.6923\r\r\r\rSum\r\r37.1796\r\r62.8205\r\r100.0001\r\r\r\r\rdetach(Thanksgiving_No)\rWith relative to people who celebrate Thanksgiving in the Family Income category highest percentage goes to USD 25,000 to 49,999.\nPeople who have Family Income USD 25,000 to 49,999 and age above 60 have the highest percentage of 4.96, while lowest percentage of 0.11 is for people who have Family Income in between USD 175,000 to 199,999 of age category of 18-29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,family_income)),\u0026quot;html\u0026quot;) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\r18 - 29\r\r3.4847\r\r2.1119\r\r0.9504\r\r0.2112\r\r0.5280\r\r0.1056\r\r0.6336\r\r3.6959\r\r2.1119\r\r2.0063\r\r3.6959\r\r19.5354\r\r\r\r30 - 44\r\r1.3728\r\r1.5839\r\r2.5343\r\r0.8448\r\r0.6336\r\r0.3168\r\r1.2672\r\r4.8574\r\r4.0127\r\r4.2239\r\r3.1679\r\r24.8153\r\r\r\r45 - 59\r\r0.3168\r\r1.3728\r\r4.1183\r\r2.5343\r\r2.0063\r\r1.1616\r\r3.1679\r\r4.0127\r\r3.1679\r\r3.6959\r\r2.8511\r\r28.4056\r\r\r\r60+\r\r0.3168\r\r1.2672\r\r3.9071\r\r1.4784\r\r0.8448\r\r1.1616\r\r2.9567\r\r4.9630\r\r4.1183\r\r3.4847\r\r2.7455\r\r27.2441\r\r\r\rSum\r\r5.4911\r\r6.3358\r\r11.5101\r\r5.0687\r\r4.0127\r\r2.7456\r\r8.0254\r\r17.5290\r\r13.4108\r\r13.4108\r\r12.4604\r\r100.0004\r\r\r\r\rdetach(Thanksgiving_Yes)\rOf people who do not celebrate Thanksgiving the Family Income category has the highest percentage which goes to People who prefer not to answer.\n15 cells in this table are zero which is the lowest percentage that can occur, while highest percentage goes to people who are in the age category 18 -29 while Family Income is USD 0 to 9,999 and prefer not to answer.\nattach(Thanksgiving_No) #kable(addmargins(table(age,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;) \r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\r18 - 29\r\r12.8205\r\r2.5641\r\r1.2821\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.5641\r\r5.1282\r\r2.5641\r\r12.8205\r\r39.7436\r\r\r\r30 - 44\r\r2.5641\r\r3.8462\r\r1.2821\r\r0.0000\r\r1.2821\r\r0.0000\r\r2.5641\r\r8.9744\r\r3.8462\r\r1.2821\r\r5.1282\r\r30.7695\r\r\r\r45 - 59\r\r2.5641\r\r2.5641\r\r0.0000\r\r1.2821\r\r1.2821\r\r1.2821\r\r2.5641\r\r5.1282\r\r0.0000\r\r3.8462\r\r1.2821\r\r21.7951\r\r\r\r60+\r\r0.0000\r\r1.2821\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.2821\r\r1.2821\r\r0.0000\r\r3.8462\r\r7.6925\r\r\r\rSum\r\r17.9487\r\r10.2565\r\r2.5642\r\r1.2821\r\r2.5642\r\r1.2821\r\r5.1282\r\r17.9488\r\r10.2565\r\r7.6924\r\r23.0770\r\r100.0007\r\r\r\r\rdetach(Thanksgiving_No)\rFor the people who celebrate Thanksgiving highest percentage of 21.80 goes to US region of South Atlantic. While lowest percentage goes to Mountain with 4.41.\nPeople who are from South Atlantic in the age categories of 45-59 and 60+ have the highest percentage of 6.55. While the lowest percentage of 0.64 goes to people who are in the age category of 18-29 and from East South Central.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,us_region)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r18 - 29\r\r2.79\r\r0.64\r\r2.26\r\r0.75\r\r0.97\r\r3.11\r\r3.76\r\r2.26\r\r2.47\r\r19.01\r\r\r\r30 - 44\r\r3.44\r\r1.18\r\r4.51\r\r0.97\r\r1.61\r\r3.87\r\r4.94\r\r1.72\r\r2.47\r\r24.71\r\r\r\r45 - 59\r\r4.40\r\r2.15\r\r4.94\r\r1.40\r\r1.72\r\r3.22\r\r6.55\r\r1.72\r\r2.69\r\r28.79\r\r\r\r60+\r\r4.94\r\r2.04\r\r3.87\r\r1.29\r\r1.61\r\r3.76\r\r6.55\r\r1.93\r\r1.50\r\r27.49\r\r\r\rSum\r\r15.57\r\r6.01\r\r15.58\r\r4.41\r\r5.91\r\r13.96\r\r21.80\r\r7.63\r\r9.13\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\rOf people who do not celebrate Thanksgiving 23.5294% are from Pacific, while lowest percentage is for people who are from New England and West North Central with 4.4118.\n10 cells have zero values which is the lowest percentage value. While highest percentage of 11.7647 occurs to people from Pacific and in the age category 30-44.\nattach(Thanksgiving_No)\r#kable(addmargins(table(age,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,us_region)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r18 - 29\r\r1.4706\r\r2.9412\r\r8.8235\r\r4.4118\r\r0.0000\r\r7.3529\r\r4.4118\r\r2.9412\r\r5.8824\r\r38.2354\r\r\r\r30 - 44\r\r2.9412\r\r0.0000\r\r5.8824\r\r1.4706\r\r1.4706\r\r11.7647\r\r5.8824\r\r0.0000\r\r2.9412\r\r32.3531\r\r\r\r45 - 59\r\r2.9412\r\r2.9412\r\r4.4118\r\r2.9412\r\r2.9412\r\r1.4706\r\r4.4118\r\r0.0000\r\r0.0000\r\r22.0590\r\r\r\r60+\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r2.9412\r\r1.4706\r\r1.4706\r\r0.0000\r\r7.3530\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5883\r\r8.8236\r\r4.4118\r\r23.5294\r\r16.1766\r\r4.4118\r\r8.8236\r\r100.0005\r\r\r\r\rdetach(Thanksgiving_No)\r\rGender Distribution\rWe have two types of gender categories in this data set which are male and female. According to the people who celebrate Thanksgiving 515 are Female, while only 432 are male. Here also there are 33 missing observations and they have been removed.\nBut this is not the case for those who do not celebrate Thanksgiving. Female have a count of only 29, where males have a count of 49. There were no missing observations.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_sex\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$gender))\r# people who do celebrate\rdo_sex\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$gender)))\r# people who do celebrate\rdata_do_sex\u0026lt;-data.frame(group=c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;),\rvalues=do_sex$`summary.factor(na.omit(Thanksgiving_Yes$gender))`,\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,2))\r# people who do not celebrate\rdata_dont_sex\u0026lt;-data.frame(group=c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;),\rvalues=dont_sex$`summary.factor(Thanksgiving_No$gender)`,\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,2))\r# combining both data_sex\u0026lt;-rbind(data_do_sex,data_dont_sex)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_sex,aes(group,values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Gender\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Gender\u0026quot;)+\rscale_y_continuous(labels= seq(0,520,10),breaks = seq(0,520,10))+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;elastic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rGender with Other Factors\rOf people who do celebrate Thanksgiving highest percentage of 10.14 goes to Females where Family Income is USD 25,000 to 49,999. While lowest percentage of 1.27 goes to Males of Family Income category USD 175,000 to 199,999.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(gender,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(gender,family_income)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\rFemale\r\r2.64\r\r3.80\r\r5.39\r\r2.11\r\r2.11\r\r1.48\r\r4.44\r\r10.14\r\r7.92\r\r6.97\r\r7.39\r\r54.39\r\r\r\rMale\r\r2.85\r\r2.53\r\r6.12\r\r2.96\r\r1.90\r\r1.27\r\r3.59\r\r7.39\r\r5.49\r\r6.44\r\r5.07\r\r45.61\r\r\r\rSum\r\r5.49\r\r6.33\r\r11.51\r\r5.07\r\r4.01\r\r2.75\r\r8.03\r\r17.53\r\r13.41\r\r13.41\r\r12.46\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\r3 cells in the below table are zero values, which is the lowest percentage value. Highest percentage of 14.1026 goes to Males who chose not to answer regarding Family Income where they do not celebrate Thanksgiving.\nattach(Thanksgiving_No)\r#kable(addmargins(table(gender,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(gender,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\rFemale\r\r6.4103\r\r3.8462\r\r1.2821\r\r1.2821\r\r0.0000\r\r0.0000\r\r1.2821\r\r5.1282\r\r3.8462\r\r5.1282\r\r8.9744\r\r37.1798\r\r\r\rMale\r\r11.5385\r\r6.4103\r\r1.2821\r\r0.0000\r\r2.5641\r\r1.2821\r\r3.8462\r\r12.8205\r\r6.4103\r\r2.5641\r\r14.1026\r\r62.8208\r\r\r\rSum\r\r17.9488\r\r10.2565\r\r2.5642\r\r1.2821\r\r2.5641\r\r1.2821\r\r5.1283\r\r17.9487\r\r10.2565\r\r7.6923\r\r23.0770\r\r100.0006\r\r\r\r\rdetach(Thanksgiving_No)\rFemale from South Atlantic who celebrate Thanksgiving have a highest percentage of 12.14. Where respondents from Mountain region and Males have the lowest percentage of 1.29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(gender,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(gender,us_region)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\rFemale\r\r8.16\r\r3.33\r\r8.59\r\r3.11\r\r3.33\r\r7.30\r\r12.14\r\r4.19\r\r4.40\r\r54.55\r\r\r\rMale\r\r7.41\r\r2.69\r\r6.98\r\r1.29\r\r2.58\r\r6.66\r\r9.67\r\r3.44\r\r4.73\r\r45.45\r\r\r\rSum\r\r15.57\r\r6.02\r\r15.57\r\r4.40\r\r5.91\r\r13.96\r\r21.81\r\r7.63\r\r9.13\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\rMale respondents who do not celebrate Thanksgiving where they are from Middle Atlantic have a highest percentage of 16.1765. Even though Females of West South Central have the lowest percentage of 1.4706 and Males from West North Central also have the same percentage value.\nattach(Thanksgiving_No)\r#kable(addmargins(table(gender,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(gender,us_region)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\rFemale\r\r4.4118\r\r2.9412\r\r4.4118\r\r4.4118\r\r1.4706\r\r8.8235\r\r7.3529\r\r2.9412\r\r1.4706\r\r38.2354\r\r\r\rMale\r\r2.9412\r\r2.9412\r\r16.1765\r\r4.4118\r\r2.9412\r\r14.7059\r\r8.8235\r\r1.4706\r\r7.3529\r\r61.7648\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5883\r\r8.8236\r\r4.4118\r\r23.5294\r\r16.1764\r\r4.4118\r\r8.8235\r\r100.0002\r\r\r\r\rdetach(Thanksgiving_No)\r\rFamily Income Distribution\rThere are 11 categories when it comes to Family Income. The option of Prefer Not to answer is given and has been chosen by people who celebrate and people who do not celebrate Thanksgiving.\nConsidering the people the who celebrate Thanksgiving, highest count of 166 goes to the category of 25,000 to 49,999 USD. While least count goes to 175,000 to 199,999 USD and the count is 26. Further, 118 people have chosen not to answer this question. 33 Missing observations were removed.\nWhere as in people who do not celebrate Thanksgiving, second highest count goes to the categories of 0 to 9,999 USD and 25,000 to 49,999 USD, where the count is 14. Similarly, for the least count of 1 also there are two Family Income categories, which are 125,000 to 149,999 USD and 175,000 to 199,999 USD. Prefer not to answer is the choice of 18 respondents who participated in this survey. No missing observations were recorded.\nAs before here also an animated bar plot is used to explain this.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_FI\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$family_income))\r# people who do celebrate\rdo_FI\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$family_income)))\r# people who do celebrate\rdata_do_FI\u0026lt;-data.frame(group=c(\u0026quot;0-9,999\u0026quot;,\u0026quot;10,000-24,999\u0026quot;,\u0026quot;25,000-49,999\u0026quot;,\r\u0026quot;50,000-74,999\u0026quot;,\u0026quot;75,000-99,999\u0026quot;,\u0026quot;100,000-124,999\u0026quot;,\r\u0026quot;125,000-149,999\u0026quot;,\u0026quot;150,000-174,999\u0026quot;,\r\u0026quot;175,000-199,999\u0026quot;,\u0026quot;200,000 and up\u0026quot;,\u0026quot;Not to answer\u0026quot;),\rvalues=c(52,60,166,127,127,109,48,38,26,76,118),\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,11))\r# people who do not celebrate\rdata_dont_FI\u0026lt;-data.frame(group=c(\u0026quot;0-9,999\u0026quot;,\u0026quot;10,000-24,999\u0026quot;,\u0026quot;25,000-49,999\u0026quot;,\r\u0026quot;50,000-74,999\u0026quot;,\u0026quot;75,000-99,999\u0026quot;,\u0026quot;100,000-124,999\u0026quot;,\r\u0026quot;125,000-149,999\u0026quot;,\u0026quot;150,000-174,999\u0026quot;,\u0026quot;175,000-199,999\u0026quot;,\r\u0026quot;200,000 and up\u0026quot;,\u0026quot;Not to answer\u0026quot;),\rvalues=c(14,8,14,8,6,2,1,2,1,4,18),\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,11))\r# combine the dataset\rdata_FI\u0026lt;-rbind(data_do_FI,data_dont_FI)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_FI,aes(group,values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Family Income in dollars\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Family Income\u0026quot;)+\rscale_y_continuous(labels= seq(0,170,10),breaks = seq(0,170,10))+\rgeom_text(aes(label=values), vjust=1)+coord_flip()+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;bounce-in\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rFamily Income with Other Factors\rOf people who do celebrate Thanksgiving the lowest percentage of zero is for people from West North Central with Family Income USD 175,000 to 199,999, people from West South Central with Family Income USD 175,000 to 199,999, people from Mountain with Family Income USD 150,000 to 174,999 and people from Mountain with Family Income USD 175,000 to 199,999. Highest percentage of 4.9409 goes to people from South Atlantic with Family Income USD 25,000 to 49,999.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(family_income,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(12,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\u0026quot;html\u0026quot;)%\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(12,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r$0 to $9,999\r\r0.6445\r\r0.2148\r\r0.6445\r\r0.1074\r\r0.2148\r\r1.0741\r\r0.8593\r\r0.2148\r\r0.9667\r\r4.9409\r\r\r\r$10,000 to $24,999\r\r0.8593\r\r0.6445\r\r0.5371\r\r0.1074\r\r0.3222\r\r1.5038\r\r1.3963\r\r0.5371\r\r0.3222\r\r6.2299\r\r\r\r$100,000 to $124,999\r\r2.7927\r\r0.8593\r\r1.3963\r\r0.4296\r\r0.5371\r\r1.6112\r\r2.2556\r\r0.3222\r\r1.5038\r\r11.7078\r\r\r\r$125,000 to $149,999\r\r0.5371\r\r0.1074\r\r0.6445\r\r0.3222\r\r0.3222\r\r0.6445\r\r1.8260\r\r0.2148\r\r0.5371\r\r5.1558\r\r\r\r$150,000 to $174,999\r\r0.2148\r\r0.2148\r\r0.5371\r\r0.0000\r\r0.2148\r\r0.9667\r\r0.8593\r\r0.3222\r\r0.7519\r\r4.0816\r\r\r\r$175,000 to $199,999\r\r0.6445\r\r0.2148\r\r1.0741\r\r0.0000\r\r0.2148\r\r0.3222\r\r0.3222\r\r0.0000\r\r0.0000\r\r2.7926\r\r\r\r$200,000 and up\r\r0.7519\r\r0.4296\r\r2.0408\r\r0.6445\r\r0.7519\r\r1.0741\r\r1.1815\r\r0.5371\r\r0.6445\r\r8.0559\r\r\r\r$25,000 to $49,999\r\r2.2556\r\r1.1815\r\r2.5779\r\r0.6445\r\r1.0741\r\r2.1482\r\r4.9409\r\r1.5038\r\r1.0741\r\r17.4006\r\r\r\r$50,000 to $74,999\r\r2.5779\r\r0.9667\r\r2.0408\r\r0.5371\r\r0.5371\r\r1.8260\r\r2.7927\r\r1.3963\r\r0.8593\r\r13.5339\r\r\r\r$75,000 to $99,999\r\r2.7927\r\r0.8593\r\r1.9334\r\r0.8593\r\r0.5371\r\r1.2889\r\r2.4705\r\r1.2889\r\r1.5038\r\r13.5339\r\r\r\rPrefer not to answer\r\r1.5038\r\r0.3222\r\r2.1482\r\r0.7519\r\r1.1815\r\r1.5038\r\r2.9001\r\r1.2889\r\r0.9667\r\r12.5671\r\r\r\rSum\r\r15.5748\r\r6.0149\r\r15.5747\r\r4.4039\r\r5.9076\r\r13.9635\r\r21.8044\r\r7.6261\r\r9.1301\r\r100.0000\r\r\r\r\rdetach(Thanksgiving_Yes)\rThere are a lot of cell values which have zero therefore I am not going to state them. Further, Highest percentage value of 5.8824 is from people of Middle Atlantic and Family Income categories of USD 0 to 9,999 and USD 25,000 to 49,999.\nattach(Thanksgiving_No)\r#kable(addmargins(table(family_income,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(12,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\u0026quot;html\u0026quot;)%\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(12,bold = T,color = \u0026quot;red\u0026quot;)%\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r$0 to $9,999\r\r0.0000\r\r0.0000\r\r5.8824\r\r4.4118\r\r0.0000\r\r2.9412\r\r2.9412\r\r0.0000\r\r1.4706\r\r17.6472\r\r\r\r$10,000 to $24,999\r\r0.0000\r\r1.4706\r\r0.0000\r\r1.4706\r\r0.0000\r\r1.4706\r\r1.4706\r\r1.4706\r\r1.4706\r\r8.8236\r\r\r\r$100,000 to $124,999\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.9412\r\r\r\r$125,000 to $149,999\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r\r\r$150,000 to $174,999\r\r0.0000\r\r0.0000\r\r2.9412\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.9412\r\r\r\r$175,000 to $199,999\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r\r\r$200,000 and up\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r2.9412\r\r0.0000\r\r0.0000\r\r5.8824\r\r\r\r$25,000 to $49,999\r\r2.9412\r\r0.0000\r\r5.8824\r\r1.4706\r\r1.4706\r\r4.4118\r\r2.9412\r\r0.0000\r\r0.0000\r\r19.1178\r\r\r\r$50,000 to $74,999\r\r1.4706\r\r1.4706\r\r1.4706\r\r0.0000\r\r0.0000\r\r4.4118\r\r0.0000\r\r1.4706\r\r1.4706\r\r11.7648\r\r\r\r$75,000 to $99,999\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r1.4706\r\r4.4118\r\r1.4706\r\r0.0000\r\r0.0000\r\r8.8236\r\r\r\rPrefer not to answer\r\r0.0000\r\r1.4706\r\r2.9412\r\r0.0000\r\r0.0000\r\r4.4118\r\r4.4118\r\r1.4706\r\r4.4118\r\r19.1178\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5884\r\r8.8236\r\r4.4118\r\r23.5296\r\r16.1766\r\r4.4118\r\r8.8236\r\r100.0008\r\r\r\r\rdetach(Thanksgiving_No)\r\rUS Region Distribution\rThere are 9 regions in both sides, and also both sides have missing values. People who do celebrate Thanksgiving have 49 missing values, while only 10 are missing values for people who do not celebrate Thanksgiving.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_USR\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_No$us_region)))\r# people who do celebrate\rdo_USR\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$us_region)))\r# people who do celebrate\rdata_do_USR\u0026lt;-data.frame(group=c(\u0026quot;East North Central\u0026quot;, \u0026quot;East South Central\u0026quot;,\r\u0026quot;West South Central\u0026quot;, \u0026quot;West North Central\u0026quot;,\r\u0026quot;Middle Atlantic\u0026quot;,\u0026quot;South Atlantic\u0026quot;, \u0026quot;Mountain\u0026quot;, \u0026quot;New England\u0026quot;, \u0026quot;Pacific\u0026quot;),\rvalues=c(145,56,85,71,145,203,41,55,130),\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,9))\r# people who do not celebrate\rdata_dont_USR\u0026lt;-data.frame(group=c(\u0026quot;East North Central\u0026quot;, \u0026quot;East South Central\u0026quot;,\r\u0026quot;West South Central\u0026quot;, \u0026quot;West North Central\u0026quot;,\r\u0026quot;Middle Atlantic\u0026quot;,\u0026quot;South Atlantic\u0026quot;, \u0026quot;Mountain\u0026quot;, \u0026quot;New England\u0026quot;, \u0026quot;Pacific\u0026quot;),\rvalues=c(5,4,6,3,14,11,6,3,16),\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,9))\r# combine both datasets\rdata_USR\u0026lt;-rbind(data_do_USR,data_dont_USR)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_USR,aes(x=str_wrap(group,7),values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;US Regions\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to US Regions\u0026quot;)+\rscale_y_continuous(labels= seq(0,210,10),breaks = seq(0,210,10))+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;cubic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\r\rConclusion\rI shall conclude my findings in point form\n\rWe can use gganimate to make bar plots interesting and useful.\n\rkable is very useful because of styling options.\n\r\r\rFurther Analysis\r\rThere are more than 50 variables therefore much more can be done than describing the data-set.\n\rWe can use advanced methods such as clustering and model fitting.\n\r\rPlease see that\nThis is my fourth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1543190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543190400,"objectID":"34115d82fb4a9eb856586040bfd50ff8","permalink":"/post/week_34/week-34-thanksgiving/","publishdate":"2018-11-26T00:00:00Z","relpermalink":"/post/week_34/week-34-thanksgiving/","section":"post","summary":"# Load the packges\rlibrary(ggplot2)\rlibrary(ggthemr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(tidyverse)\rlibrary(tweenr)\rlibrary(gganimate)\rlibrary(kableExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(readr)\r#load the data\rThanksgiving\u0026lt;-read_csv(\u0026quot;thanksgiving_meals.csv\u0026quot;)\r# apply the theme grape\rggthemr(\u0026quot;grape\u0026quot;)\r#subset the people who said yes for celebrating thanksgiving\rThanksgiving_Yes\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;Yes\u0026quot;)\r#subset the people who said no for celebrating thanksgiving\rThanksgiving_No\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;No\u0026quot;)\rData set was provided on week 34 for TidyTuesday analysis. As it is Thanksgiving week this is understandable. You can receive the data set here.","tags":["TidyTuesday","R","R package"],"title":"Week 34 : Thanksgiving ","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"# load the packages\rlibrary(ggplot2)\rlibrary(ggrepel)\rlibrary(ggthemr)\rlibrary(magrittr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(readr)\rlibrary(gganimate)\r# load the theme flat\rggthemr(\u0026quot;flat\u0026quot;)\r#load the data sets\rmalaria_deaths\u0026lt;-read_csv(\u0026quot;malaria_deaths.csv\u0026quot;)\rmalaria_deaths_age\u0026lt;-read_csv(\u0026quot;malaria_deaths_age.csv\u0026quot;)\rattach(malaria_deaths)\rattach(malaria_deaths_age)\r# disseminating data\rMalaria_deaths_Code_missing\u0026lt;-malaria_deaths[!complete.cases(Code),]\rMD_CM_SDI\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;High-middle SDI\u0026quot; | Entity == \u0026quot;High SDI\u0026quot; | Entity == \u0026quot;Low SDI\u0026quot; | Entity ==\u0026quot;Low-middle SDI\u0026quot; | Entity ==\u0026quot;Middle SDI\u0026quot;)\rMD_CM_Sahara\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Central Sub-Saharan Africa\u0026quot; |\rEntity == \u0026quot;Western Sub-Saharan Africa\u0026quot; | Entity == \u0026quot;Southern Sub-Saharan Africa\u0026quot; |\rEntity ==\u0026quot;Eastern Sub-Saharan Africa\u0026quot; | Entity ==\u0026quot;Sub-Saharan Africa\u0026quot;)\rMD_CM_EU\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Western Europe\u0026quot; | Entity == \u0026quot;Eastern Europe\u0026quot; | Entity == \u0026quot;Central Europe\u0026quot;)\rMD_CM_GB\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;England\u0026quot; | Entity == \u0026quot;Northern Ireland\u0026quot; | Entity == \u0026quot;Scotland\u0026quot; | Entity ==\u0026quot;Wales\u0026quot;)\rMD_CM_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Andean Latin America\u0026quot; | Entity == \u0026quot;North America\u0026quot; | Entity == \u0026quot;Southern Latin America\u0026quot; | Entity ==\u0026quot;Tropical Latin America\u0026quot; | Entity ==\u0026quot;Central Latin America\u0026quot; )\rMD_CM_A\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;North Africa and Middle East\u0026quot; | Entity == \u0026quot;Southeast Asia\u0026quot; | Entity == \u0026quot;Australasia\u0026quot; | Entity ==\u0026quot;East Asia\u0026quot; | Entity ==\u0026quot;Central Asia\u0026quot; | Entity ==\u0026quot;Oceania\u0026quot;| Entity ==\u0026quot;High-income Asia Pacific\u0026quot;| Entity ==\u0026quot;South Asia\u0026quot; )\rMD_CM_C_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Caribbean\u0026quot; | Entity == \u0026quot;Latin America and Caribbean\u0026quot;)\r# disseminating data\r#Malaria_deaths_age_Code_missing\u0026lt;-malaria_deaths_age[!complete.cases(code),]\r#MD_age_CM_SDI\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;High-middle SDI\u0026quot; | # entity == \u0026quot;High SDI\u0026quot; | entity == \u0026quot;Low SDI\u0026quot; |\r# entity ==\u0026quot;Low-middle SDI\u0026quot; | entity ==\u0026quot;Middle SDI\u0026quot;)\r#MD_age_CM_Sahara\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Central Sub-Saharan Africa\u0026quot; | # entity == \u0026quot;Western Sub-Saharan Africa\u0026quot; | entity == \u0026quot;Southern Sub-Saharan Africa\u0026quot; |\r# entity ==\u0026quot;Eastern Sub-Saharan Africa\u0026quot; | entity ==\u0026quot;Sub-Saharan Africa\u0026quot;)\r#MD_age_CM_EU\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Western Europe\u0026quot; | # entity == \u0026quot;Eastern Europe\u0026quot; | entity == \u0026quot;Central Europe\u0026quot;)\r#MD_age_CM_GB\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;England\u0026quot; | # entity == \u0026quot;Northern Ireland\u0026quot; | entity == \u0026quot;Scotland\u0026quot; |\r# entity ==\u0026quot;Wales\u0026quot; )\r#MD_age_CM_LA\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Andean Latin America\u0026quot; | # entity == \u0026quot;North America\u0026quot; | entity == \u0026quot;Southern Latin America\u0026quot; |\r# entity ==\u0026quot;Tropical Latin America\u0026quot; | entity ==\u0026quot;Central Latin America\u0026quot;)\r#MD_age_CM_A\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;North Africa and Middle East\u0026quot;| # entity == \u0026quot;Southeast Asia\u0026quot; | entity == \u0026quot;Australasia\u0026quot; | entity ==\u0026quot;East Asia\u0026quot;| # entity ==\u0026quot;Central Asia\u0026quot; | entity ==\u0026quot;Oceania\u0026quot;| # entity ==\u0026quot;High-income Asia Pacific\u0026quot;| entity ==\u0026quot;South Asia\u0026quot; )\r#MD_age_CM_C_LA\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Caribbean\u0026quot; | # entity == \u0026quot;Latin America and Caribbean\u0026quot;)\rWhat I posted in #TidyTuesday on 13th November 2018.\rBelow is the code and simple analysis I posted in relative to the Malaria data. I only focused on Sri Lanka and Malaria deaths over years. I could not find anything else at that time, but over the days I realized it should be worth to see how death rate counts change with different regions which do not have Code variable assigned for the data sets malaria deaths and malaria deaths of age.\nThe Tweet itself.\n\rPackages : ggplot2, ggrepel, ggthemr\rTidyTuesday : week 33\rData: malaria_deaths\rMalaria_deaths_plot: Plot shows the death rate per 100,000 people in Sri Lanka decreasing rapidly from 1996 to 2003 with a drop of 0.89 to 0.24, and by 2016 it reaches 0. While in 2015 this rate is 0.13. Highest death rate was in 1990 with 0.91.\r\r#data subset has been used\r#scales of x and y have been more scrutinized\r#labels have been added\r#x axis have been modified to accomodate the years Malaria_deaths_plot\u0026lt;-ggplot(subset.data.frame(malaria_deaths,Code==\u0026quot;LKA\u0026quot;),\raes(x=Year,\ry= `Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rgeom_point()+geom_line()+geom_text_repel()+\rggtitle(\u0026quot;Malaria Deaths for Both genders in a rate over the years in Sri Lanka\u0026quot;)+\rylab(\u0026quot;Deaths - Malaria - Sex: Both -\\n Age: Age-standardized (Rate) (per 100,000 people)\u0026quot;)+\rscale_y_continuous(breaks=seq(0.1,1,by=0.1) ,labels=seq(0.1,1,by=0.1))+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rtheme(axis.text.x = element_text(angle = 90))\r# print the plot\rprint(Malaria_deaths_plot)\r# save the plot\rggsave(Malaria_deaths_plot,width = 10,height = 10,dpi=300,\rfilename = \u0026quot;Malaria_Deaths_Sri Lanka.png\u0026quot;)\r\rData: malaria_deaths_age\rMalaria_deaths_age_plot: There are five categories in concern,where age category 15-49 has the most counts of in the range of 45-50. Second category is Under 5 close to 35 counts, while third category is 50-69 in-between 20-25. It should be noted that this order is for the year 1990. At the end of year 2015 this is not the case, where the categories and counts are 15-49 (close to 10), 50-69 (less than 10), 70 or order(close to 5), under 5(less than 5) and finally 5-14 (close to 0).\r\r#data subset has been used\r#according to age group colors are assigned\r#scales of x and y have been more scrutinized\r#labels have been added\r#x axis have been modified to accomodate the years Malaria_deaths_age_plot\u0026lt;-ggplot(subset.data.frame(malaria_deaths_age,code==\u0026quot;LKA\u0026quot;),\raes(x=year,y=deaths,color=factor(age_group)))+\rgeom_point()+geom_line()+\rggtitle(\u0026quot;Malaria Deaths by age category in Sri Lanka over the years\u0026quot;)+\rylab(\u0026quot;Deaths Count\u0026quot;)+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rscale_y_continuous(breaks=seq(0,60,by=5) ,labels=seq(0,60,by=5))+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_color_discrete(name=\u0026quot;Age Category\u0026quot;)\r# print the plot\rprint(Malaria_deaths_age_plot)\r# save the plot\rggsave(Malaria_deaths_age_plot,width=10,height = 10,dpi = 300,\rfilename = \u0026quot;Malaria Deaths Sri Lanka by Age.png\u0026quot;)\rMalaria Death count rate can be observed by countries or specific regions given in the data set. There are 9 regions in concern and each of these regions have sub parts or collections of countries. Variables included in this data-set are\n\rEntity - Full name of a country or region it is referred.\rCode - 3 letter ISO code for countries, but for regions there are no values.\rYear - Year range from 1990 to 2016.\rDeaths - Malaria - Sex : Both - Age : Age - Standardized (Rate) (per 100,000 people) - Number of people dead for both sexes per 100,000 people in a standardized age because of Malaria.\r\rThe Description of the data-set. I will focus on each region separately, because even inside the same region sub parts can behave differently. The 9 regions are\nSDI Countries\rSaharan Region\rEuropean Countries\rGreat Britain or United Kingdom\rAmerican Region\rAsian Countries\rNorth Africa and Middle East\rOceania Region\rCaribbean and Latin American Countries\r\r\rSDI Countries with Malaria Death Count Rate\rInitially there are 5 sub parts in SDI countries, but we can factor them into three based on their death rate behavior. Some regions are having higher death rate close to 75, while others have lower death rate close to 0.0002.\nFirst group is the sub regions which are having higher death rates than other SDI regions, which are Low SDI and Low-middle SDI countries. Low SDI countries are behaving poorly from 1990 with a 73.78 death rate which reaches it highest peek of 75.88 in 2001 after this death rate gradually declines until 2016 and reaches 37.87.\nThis is not the case for Low middle SDI countries where in 1990 death rate is 19.11 and reaches its peek of 22.01 in 2003 with small fluctuations. After this it gradually and slowly declines until 2016 while finishing in a rate of 15.7.\nattach(MD_CM_SDI)\r# scatter plot for Low and Low middle SDI\rggplot(subset(MD_CM_SDI,Entity == \u0026quot;Low SDI\u0026quot; | Entity == \u0026quot;Low-middle SDI\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Low-middle and Low SDI Countries\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ legend_bottom()+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(15,80,5),breaks =seq(15,80,5))\rSDI countries Middle and High-middle are performing better than the above two regions from 1990 itself. While Middle SDI countries have a very low death rate of close to 1 and periodically it decreases to 0.6 at 2016. To be exact in 1990 the death rate is 0.9274 and decrement occurs with small but unaffected fluctuations and reaches 0.6018 in 2016, which is the lowest point.\nFor High middle SDI countries the death rate in 1990 is 0.1168 and in 1996 it reaches the highest point of 0.1281. After 1996 there is clear decrease of death rate which reaches its minimum value of 0.0454 in 2013.\n# scatter plot for High middle and Middle\rggplot(subset(MD_CM_SDI,Entity == \u0026quot;Middle SDI\u0026quot;| Entity == \u0026quot;High-middle SDI\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Middle and High-middle SDI Countries\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,1,0.05),breaks =seq(0,1,0.05))\rHigh SDI countries are showing very strong decrease from 1990 it self. While in 1990 the death rate is 0.001387 and by polynomial decreasing by 2016 this reaches the least value of 0.000194.\n# scatter plot for High SDI\rggplot(subset(MD_CM_SDI, Entity ==\u0026quot;High SDI\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,6)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;High SDI Countries\u0026quot;)+ geom_text_repel()+ legend_bottom()+ geom_point()+ geom_line()+ scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.0014,0.0002),breaks =seq(0,0.0014,0.0002))\rdetach(MD_CM_SDI)\rIf we do plot all these 5 sub parts of SDI countries on one plot we would not have seen these differences and even if we do use facet grid this is not possible. Therefore, first I did plot all 5 regions together and then sub setted them based on their death rate change in values.\nIt is clear that High SDI values have the lowest death rate and Low, Low middle SDI have the highest death rates over the years 1990 to 2016.\n\rSaharan Region with Malaria Death Count Rate\rThere are 5 sub parts for Saharan region countries, which are Central Sub-Saharan, Eastern Sub-Saharan, Southern Sub-Saharan, Western Sub-Saharan and Sub-Saharan Africa countries. Here though it does not look like we need to separate these regions and plot them as groups. While plotting them as a whole they are clear and possible to interpret the differences in this line plot.\nCentral Sub-Saharan Africa and Western Sub-Saharan Africa countries behave similarly where they start with death rates respectively 108.67, 118.94. Further this death rate increases and reaches its highest peaks of 138.23 and 121.08 respectively for Western Sub-Saharan Africa and Central Sub-Saharan Africa countries for the year 2003. Finally they decrease into their lowest of 64.64 death rate for Central Sub-Saharan Africa and 87.54 death rate for Western Sub-Saharan Africa.\nSub-Saharan Africa Regions has a higher death rate performance through out the time line when it begins with 89.33 in 1990, and reaches its highest value of 96.28 in 2003. Further death rate decreases slowly up-to 51.93 in 2016, which is the least minimum value. Eastern Sub-Saharan Africa starts with 77.83 in 1990 and reaches its peak of 78.49 in 1991. In the next few years until 2000 there is small fluctuations. Finally there is a steep slope and reaches its lowest death rate value of 23.15 in 2016.\nDeath rate of Southern Sub-Saharan Africa is completely different than above four sub regions of Saharan Africa. In 1990 the death rate is 1.69 and it fluctuates until 2016, but it reaches the highest death rate of 2.57 in 2006. Further with this fluctuation in 2016 the death rate is 2.29 which is higher than the previous 7 years from 2016.\nattach(MD_CM_Sahara)\r# scatter plot for Sahara Region\rggplot(MD_CM_Sahara,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Saharan Africa Region\u0026quot;)+geom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ geom_text_repel()+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\rdetach(MD_CM_Sahara)\rConsidering 1990 and 2016 the biggest decrease occurs for the region of Eastern Sub-Saharan Africa, while the lowest decrease is for Western Sub-Saharan Africa. While in the year gap of 1990 and 2016 only Southern Sub-Saharan Africa has an increase in death rate from 1.69 to 2.29.\n\rEuropean Countries with Malaria Death Count Rate\rI believe that European countries did not have any malaria related deaths even before 1990, because of their weather patterns. That is simply assured by here in this plot, where there is no deaths for the sub regions of Europe. Which are Central Europe, Eastern Europe and Western Europe.\nattach(MD_CM_EU)\r# scatter plot for European Region\rggplot(MD_CM_EU,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;European Region\u0026quot;)+ geom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+geom_text_repel()+\rscale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\rdetach(MD_CM_EU)\r\rGreat Britain or United Kingdom with Malaria Death Count Rate\rWe consider the collection of these countries as Great Britain or United Kingdom, which are England, Northern Ireland, Scotland and Wales. Here also weather pattern does have a high probability in causing a situation of no malaria related deaths. Further, these four countries are close to the European regions geographically therefore this assures us more that over the years from 1990 to 2016 the death rate is zero.\nattach(MD_CM_GB)\r# scatter plot for Great Britain or United Kingdom\rggplot(MD_CM_GB,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Great Britain or United Kingdom\u0026quot;)+ legend_bottom()+\rgeom_point()+geom_line()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,1,0.5),breaks =seq(0,1,0.5))\rdetach(MD_CM_GB)\r\rAmerican Region with Malaria Death Count Rate\rThere are 5 sub regions for American countries, yet we can divide them into two groups. One group will include Andean, Central and Tropical Latin American countries, and other group includes North America and Southern Latin American countries.\nIn the first group, Andean and Tropical Latin American countries begin with respectively 0.354, 0.351 death rates in 1990. This gradually decreases until 2016 for both regions, but Tropical Latin countries has a better decrement than Andean Latin American countries because they achieve death rate of respectively 0.038 and 0.048.\nIn this same time period of 1990 to 2016, Central Latin American begins with a death rate of 0.296 and reaches its lowest point of 0.053. In year 1994, Tropical Latin American region and Central Latin American region have the same death rate of 0.222. Further in the years 2004 and 2005 the death rates of Andean Latin American countries are 0.089, 0.071, but the same years Central Latin American region has death rates of 0.088, 0.080. Those are the two crucial changes which occur.\nattach(MD_CM_LA)\r# scatter plot for Andean, Central and Tropical Latin America\rggplot(subset(MD_CM_LA,Entity == \u0026quot;Andean Latin America\u0026quot;| Entity == \u0026quot;Central Latin America\u0026quot; | Entity == \u0026quot;Tropical Latin America\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,3)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Andean, Central and Tropical Latin American Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.40,0.05),breaks =seq(0,0.40,0.05))\rThe second group has North American region and Southern Latin American region. Clearly North America with its cold weather condition and developed status does not hold any deaths for malaria from 1990 to 2016. It is even possible to consider before this time range also there was no deaths for malaria. Yet Southern Latin American has death rate of 0.0208 in 1990 but decreases gradually and reaches its least minimum point of 0.0035 in 2016.\n# scatter plot for North and Southern Latin America\rggplot(subset(MD_CM_LA,Entity== \u0026quot;North America\u0026quot;| Entity ==\u0026quot;Southern Latin America\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;North America and Southern Latin American Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.021,0.001),breaks =seq(0,0.021,0.001))\rdetach(MD_CM_LA)\rComparing the American region only North American region is malaria free, while least amount progress has occurred in Central Latin American region. Tropical region has out performed Andean Latin American region. Next to North America, Southern Latin American has done more progress than other three regions.\n\rAsian Countries with Malaria Death Count Rate\rThere are 6 regions in the Asian region, which are South Asia, Southeast Asia, East Asia, High Income Asia Pacific, Australasia and Central Asia. Asia is a Large continent with wide variety of countries therefore we have more sub regions than any other continent here.\nSouth Asia and Southeast Asia are two regions which behave similarly where it begins in a higher rate in year 1990 and gradually decreasing until year 2016. South Asia has a death rate of 6.21 in 1990, but in 2016 it reaches to 3.61. For Southeast Asian region the death rate is 5.29 in year 1990 and reaches the death rate of 2.56 in year 2016.\nattach(MD_CM_A)\r# scatter plot for South Asia and Southeast Asia\rggplot(subset(MD_CM_A,Entity == \u0026quot;South Asia\u0026quot; | Entity == \u0026quot;Southeast Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;South Asia and Southeast Asia Region\u0026quot;)+ geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(2.4,6.4,0.2),breaks =seq(2.4,6.4,0.2))\rEast Asia region begins with a death rate of 0.0269 in year 1990 and it decreases over the coming years which will reach the lowest death rate of 0.0121 in year 2015. Further in year 2016 it slightly increases to 0.0122.\n# scatter plot for East Asia\rggplot(subset(MD_CM_A, Entity == \u0026quot;East Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;East Asia Region\u0026quot;)+ geom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.01,0.03,0.002),breaks =seq(0.01,0.03,0.002))\rCentral Asia region begins with a a death rate of 0.0304 in year 1990 and increases very slowly until year 1998 towards the death rate of 0.0314. After this the death rate decreases gradually until year 2013 where it becomes 0.0057. Until year 2016 this becomes the standard death rate for malaria in Central Asia Region.\n# scatter plot for Central Asia\rggplot(subset(MD_CM_A,Entity == \u0026quot;Central Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Central Asia Region\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.05,0.005),breaks =seq(0,0.05,0.005))\rHigh Income Asia Pacific countries have a lower death rate than previous Asian region countries where in 1990 the death rate is 0.0073 but over the years it decreases gradually and reaches the lowest death rate value of 0.0008 in year 2016.\nAustralasia countries have no death rate over the year range of 1990 to 2016, which implicate that there is a possibility that before 1990 also there could have not been any deaths related to malaria\n# scatter plot for High Income Pacific and Australasia\rggplot(subset(MD_CM_A, Entity == \u0026quot;Australasia\u0026quot; | Entity == \u0026quot;High-income Asia Pacific\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;High Income Asia Pacific and Australasia Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.001,0.008,0.0005),breaks =seq(0.001,0.008,0.0005))\rdetach(MD_CM_A)\rAustralasia is the only region which does not have malaria over the years given in the data set. The lowest death rate occurs to High Income Asian Pacific region than other regions of Asia.\n\rNorth Africa and Middle East Region with Death Count Rate\rNorth Africa and Middle East region is the only odd region of all in this data set for malaria death rate. In year 1990 the death rate is 0.7968 but in the coming years it increases until 2005 and reaches 1.3444. Despite this increment after 2005 the death rate suddenly and rapidly drops until year 2011 and reaches its lowest point of 0.731. Again there is a steady but slow increase in death rate and reaches 0.8332 in year 2016.\nattach(MD_CM_A)\r# scatter plot to North Africa and Middle East\rggplot(subset(MD_CM_A, Entity == \u0026quot;North Africa and Middle East\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rggtitle(\u0026quot;North Africa and Middle East Region\u0026quot;)+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.5,1.5,0.05),breaks =seq(0.5,1.5,0.05))\rdetach(MD_CM_A)\rComparing all the other regions in this data set only North Africa and Middle East Region was very special with its odd fluctuations. At the end comparing the death rates of year 1990 and 2016 there is an increase close to 0.04, but it is far less than what was in year 2005 and 1990.\n\rOceania Region with Death Count Rate\rOceania region begins the malaria death rate of 14.78 in 1990, even though it oscillates over the next few years and reaches a death rate of 12.5 in 1998 and in year 2000 it reaches a staggering highest peak of 53.18 death rate. After reaching the highest peak it decreases slowly in the next few years and reaches its lowest point of 8.71 in 2016.\nattach(MD_CM_A)\r# Scatter plot to Oceania\rggplot(subset(MD_CM_A,Entity == \u0026quot;Oceania\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Oceania Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+scale_y_continuous(labels = seq(6,54,2),breaks =seq(6,54,2))\rdetach(MD_CM_A)\rOceania region behaves very differently than other regions. By the year 2015 lowest death rate of 8.64 occurs for Oceania region.\n\rCaribbean and Latin America and Caribbean Countries with Death Count Rate\rCaribbean region countries in the 1990 has a malaria death rate of 0.1265, but next year it is 0.1519. Further it decreases over the years with some fluctuations. By year 2016 the death rate reaches 0.058.\nLatin American and Caribbean region countries begin with the death rate of 0.3043 in 1990, but it exponentially decreases until 2016 and reaches 0.0468. In the course it out performs Caribbean countries after year 1999 where Latin American and Caribbean countries occupy a death rate of 0.1282, Caribbean countries occupy a death rate of 0.1274.\nattach(MD_CM_C_LA)\r# Scatter plot to Caribbean and Latin America\rggplot(MD_CM_C_LA,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Caribbean and Latin American and Caribbean Region\u0026quot;)+\rgeom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.0,0.35,0.05),breaks =seq(0.0,0.35,0.05))\rdetach(MD_CM_C_LA)\rClearly Latin American and Caribbean countries perform better than Caribbean countries after year 1999 until year 2016. At the beginning of 1990 the death rate gap between both these regions are clearly high.\n\rConclusion\rI shall conclude my findings in point form\n\rDeath rate for malaria decreases over the years with small fluctuations over the years and reaches zero in 2016 for Sri Lanka. By 2016 all age categories reach a death count of zero, but in 1990 they are at different parts of the scale.\n\rHigh SDI countries have a low death rate. While Low and Low middle SDI countries have the highest death rates over the years from 1990 to 2016.\n\rIn the Saharan region lowest death rate is for Southern Sub-Saharan region and highest death rate is for Western Sub-Saharan region on the year 2016. This order is same for year 1990 as well in these regions. Highest death rate of all years and all sub regions occurs in year 2003 of 138.23 for Western Sub-Saharan Africa. While lowest death rate of all years and all sub regions is in year 1994 for Southern Sub-Saharan Africa with 1.55.\n\rEuropean and Great Britain related regions have no malaria related incidents over the years of 1990 to 2016, this could be because of their cold climate pattern and their developed status in the world.\n\rNorth America shows figures of no malaria related deaths over the years of 1990 to 2016. Southern Latin America has the lowest death rate of 0.0035 in 2016 and highest death rate is for Central Latin America. But in year 1990 highest death rate is for Andean Latin American region which is 0.354 and least death rate is for Southern Latin America with 0.0208.\n\rAustralasia has no death rate over the years of 1990 to 2016. Where in year 2016 lowest death rate occurs to High Income Asia Pacific of 0.0008 and highest in that year for Oceania region with a value of 8.71. In the year 1990 highest death rate is for Oceania region with 14.78 but lowest is for High Income Asia Pacific region with 0.0073. A special occurrence where comparing all the regions and years the highest death rate is for Oceania Region in year 2000.\n\rHighest death rate occurs in year 2005 for North Africa and Middle East region, but in 1990 the malaria death rate is 0.7968. Further in 2011 it hits the lowest value of 0.731 and finally moves up-to 0.8332 in year 2016.\n\rIn year 1990 Caribbean region has a lower death rate than Latin American and Caribbean region. But this gap gradually decreases over time and by year 2016 Caribbean region reaches a death rate of 0.058 while Latin America and Caribbean region occupies the rate of 0.0468.\n\r\r\rFurther Analysis\r\rThis article only focuses on the death rate but we can expand it to countries and compare them over continents.\n\rIt is possible to consider the age category and compare the death counts of different regions to understand further of these regions.\n\r\rPlease see that\nThis is my third post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1542758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542758400,"objectID":"3b0d1659d69ae99e197e303ac0509f59","permalink":"/post/week_33/week-33-malaria-deaths/","publishdate":"2018-11-21T00:00:00Z","relpermalink":"/post/week_33/week-33-malaria-deaths/","section":"post","summary":"# load the packages\rlibrary(ggplot2)\rlibrary(ggrepel)\rlibrary(ggthemr)\rlibrary(magrittr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(readr)\rlibrary(gganimate)\r# load the theme flat\rggthemr(\u0026quot;flat\u0026quot;)\r#load the data sets\rmalaria_deaths\u0026lt;-read_csv(\u0026quot;malaria_deaths.csv\u0026quot;)\rmalaria_deaths_age\u0026lt;-read_csv(\u0026quot;malaria_deaths_age.csv\u0026quot;)\rattach(malaria_deaths)\rattach(malaria_deaths_age)\r# disseminating data\rMalaria_deaths_Code_missing\u0026lt;-malaria_deaths[!complete.cases(Code),]\rMD_CM_SDI\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;High-middle SDI\u0026quot; | Entity == \u0026quot;High SDI\u0026quot; | Entity == \u0026quot;Low SDI\u0026quot; | Entity ==\u0026quot;Low-middle SDI\u0026quot; | Entity ==\u0026quot;Middle SDI\u0026quot;)\rMD_CM_Sahara\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Central Sub-Saharan Africa\u0026quot; |\rEntity == \u0026quot;Western Sub-Saharan Africa\u0026quot; | Entity == \u0026quot;Southern Sub-Saharan Africa\u0026quot; |\rEntity ==\u0026quot;Eastern Sub-Saharan Africa\u0026quot; | Entity ==\u0026quot;Sub-Saharan Africa\u0026quot;)\rMD_CM_EU\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Western Europe\u0026quot; | Entity == \u0026quot;Eastern Europe\u0026quot; | Entity == \u0026quot;Central Europe\u0026quot;)\rMD_CM_GB\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;England\u0026quot; | Entity == \u0026quot;Northern Ireland\u0026quot; | Entity == \u0026quot;Scotland\u0026quot; | Entity ==\u0026quot;Wales\u0026quot;)\rMD_CM_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Andean Latin America\u0026quot; | Entity == \u0026quot;North America\u0026quot; | Entity == \u0026quot;Southern Latin America\u0026quot; | Entity ==\u0026quot;Tropical Latin America\u0026quot; | Entity ==\u0026quot;Central Latin America\u0026quot; )\rMD_CM_A\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;North Africa and Middle East\u0026quot; | Entity == \u0026quot;Southeast Asia\u0026quot; | Entity == \u0026quot;Australasia\u0026quot; | Entity ==\u0026quot;East Asia\u0026quot; | Entity ==\u0026quot;Central Asia\u0026quot; | Entity ==\u0026quot;Oceania\u0026quot;| Entity ==\u0026quot;High-income Asia Pacific\u0026quot;| Entity ==\u0026quot;South Asia\u0026quot; )\rMD_CM_C_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Caribbean\u0026quot; | Entity == \u0026quot;Latin America and Caribbean\u0026quot;)\r# disseminating data\r#Malaria_deaths_age_Code_missing\u0026lt;-malaria_deaths_age[!","tags":["R","R package","TidyTuesday"],"title":"Week 33 : Malaria Deaths","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r# load the packages\rlibrary(readr)\rlibrary(ggplot2)\rlibrary(lubridate)\rlibrary(ggthemr)\rlibrary(gridExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(kableExtra)\rlibrary(readr)\r# load the data\rr_downloads_year \u0026lt;- read_csv(\u0026quot;r_downloads_year.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rr_downloads \u0026lt;- read_csv(\u0026quot;r-downloads.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rIntroduction\rTidy Tuesday is a very good move to improve R programming for anyone who is interested in statistics. Data-sets are uploaded every Tuesday, and plots are published under the #tidytuesday. This is just me presenting a few accumulated plots for the data-set r-downloads.csv and r_downloads_year.csv.\nI shall be focusing on the data set provided on 2018 October 30th, Which is R and Package download stats. My main objective is to understand how Sri Lankan users have behaved in this data-set.\nThe packages used in R are readr,ggplot2, lubridate, ggthemr, gridExtra, magrittr, knitr and kableExtra.\nThere are 701 downloads occurred in between the given time limit of 2017 October 20th to 2018 October 20th in Sri Lanka. Similarly, if we look at the downloads on the day of 2018 October 23rd, which is 3 observations. There are 7 variables to be concerned, which are\n\rdate - date of download (y-m-d)\rtime - time of download (in UTC)\rsize - size in bytes\rversion - R release version\ros - Operating System\rcountry - Two letter ISO country code\rip_id - Anonymized daily ip code(unique identifier)\r\r# extracting the observations only if the country is Sri Lanka\rr_downloads_year_LK\u0026lt;-subset.data.frame(r_downloads_year,country==\u0026quot;LK\u0026quot;)\rr_downloads_LK\u0026lt;-subset.data.frame(r_downloads,country==\u0026quot;LK\u0026quot;)\r# number of observations #dim(r_downloads_year_LK)\r#dim(r_downloads_LK)\r\rOperating Systems\rWindows is not a favorable operating system for open source programming was my myth. Well, No longer I shall believe that if it is considering Sri Lankans and R programming.\n#checking what type of operating systems are in use\rggthemr(\u0026quot;flat dark\u0026quot;)\rggplot(r_downloads_year_LK,aes(x=os))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;, aes(label=..count..), vjust=-0.5)+\rxlab(\u0026quot;Operating System\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks=seq(0,675,by=25))+\rggtitle(\u0026quot;Operating system preference of Sri lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\r# frequency table for Operating system\rtab1\u0026lt;-round(prop.table(table(r_downloads_year_LK$os)),4)\rtab1\u0026lt;-as.data.frame(tab1)\rnames(tab1)\u0026lt;-c(\u0026quot;Operating System\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab1) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to Operating system\u0026quot;=2))\r\r\rFrequency table to Operating system\r\r\r\r\rOperating System\r\rFrequency\r\r\r\r\r\rosx\r\r0.0371\r\r\r\rsrc\r\r0.0271\r\r\r\rwin\r\r0.9358\r\r\r\r\rMajority of users have used Windows which is 93.58%, while Mac users are represented with 3.71% and finally 2.71% from the source file. Next, focusing on the R versions downloaded.\n\rR versions\rVersions are updated regularly for R and a grand update occurred on 2018 April for the version 3.5.0. Further, versions 3.4.3 and 3.4.4 were updated in the time gap considered. There are versions from 3.0.0 and higher for Sri Lankan users. It is crucial to study this where we can understand how far does the user have knowledge about R and updating the software version.\n#checking what type of R versions were downloaded\rggplot(r_downloads_year_LK,aes(x=version))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;, aes(label=..count..), vjust=-0.5)+\rxlab(\u0026quot;R versions\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = seq(0,275,by=25))+\rggtitle(\u0026quot;R versions downloaded of Sri Lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rTable shows that version 3.5.1 represents a 36.52% followed by version 3.4.3 of 27.96% and in the third place version 3.5.0 with 15.98%. Further, all downloads are occurred for versions 3.0.0 or higher than it. People believed in 3.4.3 than 3.5.0, which could only mean that 3.4.3 was more stable for user and package requirements.\n# frequency table to R versions\rtab2\u0026lt;-sort(round(prop.table(table(r_downloads_year_LK$version)),4))\rtab2\u0026lt;-as.data.frame(tab2)\rnames(tab2)\u0026lt;-c(\u0026quot;R Version\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab2) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to R versions\u0026quot;=2))\r\r\rFrequency table to R versions\r\r\r\r\rR Version\r\rFrequency\r\r\r\r\r\r3.0.0\r\r0.0014\r\r\r\r3.2.2\r\r0.0014\r\r\r\r3.4.0\r\r0.0014\r\r\r\r3.3.3\r\r0.0029\r\r\r\r3.4.1\r\r0.0029\r\r\r\r3.4.4\r\r0.0899\r\r\r\r3.4.2\r\r0.0956\r\r\r\r3.5.0\r\r0.1598\r\r\r\r3.4.3\r\r0.2796\r\r\r\r3.5.1\r\r0.3652\r\r\r\r\rIf we further divide the operating systems bar plot with respective to R version it is clearly seen that only versions 3.5.1, 3.5.0, 3.4.4, 3.4.3 and 3.4.2 have maintained importance for the windows operating system.\n# Checking what type of operating system is used with R version\r#setting 10 colors becuase flat dark theme only has four originally\rset_swatch(c(\u0026quot;white\u0026quot;,\u0026quot;firebrick1\u0026quot;,\u0026quot;gold\u0026quot;,\u0026quot;darkorange\u0026quot;,\u0026quot;dodgerblue\u0026quot;,\u0026quot;darkblue\u0026quot;,\r\u0026quot;forestgreen\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;grey\u0026quot;,\u0026quot;grey44\u0026quot;,\u0026quot;black\u0026quot;))\rggplot(r_downloads_year_LK,aes(x=os,fill=version))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(y=..count..,label=..count..),position=\u0026quot;stack\u0026quot;,vjust=1)+\rxlab(\u0026quot;Operating System\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks=seq(0,675,by=25))+\rggtitle(\u0026quot;Operating system preference of Sri lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\r#Contingency table to R version versus operating System\rkable(round(prop.table(table(r_downloads_year_LK$os,\rr_downloads_year_LK$version)),4)) %\u0026gt;%\rkable_styling(bootstrap_options = \u0026quot;striped\u0026quot;,full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Contingency table for R version vs Operating System\u0026quot;=11))\r\r\rContingency table for R version vs Operating System\r\r\r\r\r\r3.0.0\r\r3.2.2\r\r3.3.3\r\r3.4.0\r\r3.4.1\r\r3.4.2\r\r3.4.3\r\r3.4.4\r\r3.5.0\r\r3.5.1\r\r\r\r\r\rosx\r\r0.0000\r\r0.0000\r\r0.0029\r\r0.0000\r\r0.0000\r\r0.0100\r\r0.0086\r\r0.0000\r\r0.0071\r\r0.0086\r\r\r\rsrc\r\r0.0014\r\r0.0014\r\r0.0000\r\r0.0000\r\r0.0029\r\r0.0043\r\r0.0057\r\r0.0000\r\r0.0014\r\r0.0100\r\r\r\rwin\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0014\r\r0.0000\r\r0.0813\r\r0.2653\r\r0.0899\r\r0.1512\r\r0.3466\r\r\r\r\rWindows operating system percentages indicate that 34.66% of users have chosen version 3.5.1, 26.53% have chosen version 3.4.3 and finally version 3.5.0 with 15.12%. Close to 9% is represented by version 3.4.2 and 3.4.4.\n\rDate versus Operating System\rDate is a difficult variable in statistics therefore I have disseminated the date into 4 types, which are month(January to December), day(1-31), hour(0-23) and minutes(0-59). Further, I have tried to understand what type of operating systems were used in those time types.\n#checking which months the downloads occured inrespecitive to operating system\rggthemr(\u0026quot;flat dark\u0026quot;)\rggplot(r_downloads_year_LK,aes(x=month(date),fill=os))+\rgeom_bar()+ xlab(\u0026quot;Months\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks =seq(0,140,10))+\rscale_x_continuous(breaks=1:12) +\rggtitle(\u0026quot;Operating systems used in the months of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rSo, the first sub part of time is months. Here, we are considering 12 months of an year and months August and April reflects no Operating System is better than Windows property. While, August holding the least amount of downloads with slightly above frequency 20. Highest frequency occurs to October with count higher than 130 and significantly osx and src types of files also have higher amount than any-other month. Except August only the month of December has counts higher than 100.\n#checking which days the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=day(date),fill=os))+\rgeom_bar()+xlab(\u0026quot;Days\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks =seq(0,80,10))+\rscale_x_continuous(breaks=1:31)+\rggtitle(\u0026quot;Operating systems used in the days of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rNext, focusing on the days it is clear that 10th and 11th have most downloads respectively reaching more than 60 and 70 in counts, while in other days it is mostly less than 30. Further, clearly on the 31st it includes least frequency of 10 because 31st is not a common day of all 12 months. It would be very tiring to focus on operating systems individually, but to be fair there is clear sign of few days with only the use of windows, and a few days with combination of other operating systems with windows.\n#checking which hour the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=hour(time),fill=os))+\rgeom_bar()+xlab(\u0026quot;Hour\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = seq(0,100,5))+\rscale_x_continuous(breaks=0:23)+\rggtitle(\u0026quot;Operating systems used in the Hours of the day of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rI have been curious at this part because of wanting to know at which hour of the day did our Sri Lankan users download R and packages. Yet it should be noted that the hour of time could be local time of Sri Lanka or otherwise. Still according to the bar chart the hours 4th and 5th have most downloads with counts of above 80 and above 90 respectively. Where in the 21st hour it reaches the least amount of less than 5 counts. Most of the frequencies are in the range of 10 and 35.\n#checking which minute the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=minute(time),fill=os))+\rgeom_bar()+xlab(\u0026quot;Minute\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = 0:25)+\rscale_x_continuous(breaks=0:59)+\rggtitle(\u0026quot;Operating systems used in the minutes of the day of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)+\rcoord_flip()\rLooking at the minutes it is very spread out. Focusing on special occasions only four minutes which are 59th, 46th, 12th and 0th have counts more than 20. While 51st minute has a count of 2. Rather than this nothing more significant occurs here. I think considering these counts in perspective of specific operating systems is tedious amount of work and waste of time.\n\rDownload Size and IP ID\rPackages were downloaded but none of their names were given in this data-set. Therefore we cannot know which package were downloaded. Yet we can identify the package sizes which were downloaded most. According to the table an R package with 82375220 bytes has most downloads of 50, while second place goes to to a size of 82375219 bytes and finally in third place is for 82375216 bytes with 39 counts.\n# table of frequency for sizes of download\rtab3\u0026lt;-as.data.frame(sort(table(r_downloads_year_LK$size))%\u0026gt;% tail(5))\rnames(tab3)\u0026lt;-c(\u0026quot;Size\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab3) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to Download size\u0026quot;=2))\r\r\rFrequency table to Download size\r\r\r\r\rSize\r\rFrequency\r\r\r\r\r\r82877772\r\r18\r\r\r\r78171328\r\r22\r\r\r\r82375216\r\r39\r\r\r\r82375219\r\r48\r\r\r\r82375220\r\r50\r\r\r\r\rLooking at the IP ID it is clear that 334 has the highest downloads of 55, while second place goes to 1060 with 46 downloads. Finally, ID number 1286 has 16 downloads with third place.\n# table of frequency to IP ID\rtab4\u0026lt;-as.data.frame(sort(table(r_downloads_year_LK$ip_id))%\u0026gt;% tail(5))\rnames(tab4)\u0026lt;-c(\u0026quot;IP ID\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab4) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to IP ID\u0026quot;=2))\r\r\rFrequency table to IP ID\r\r\r\r\rIP ID\r\rFrequency\r\r\r\r\r\r623\r\r9\r\r\r\r157\r\r12\r\r\r\r1286\r\r16\r\r\r\r1060\r\r46\r\r\r\r334\r\r55\r\r\r\r\r\rConclusion\rI shall conclude my findings in point form\n\rMost of Sri lankans (93.58%) use windows as a OS for R downloads\n\rTop three R versions are 3.5.1, 3.4.3 and 3.5.0 with percentages respectively 36.52% 27.96% and 15.98%.\n\rWindows users use versions 3.5.1, 3.4.3 and 3.5.0 with percentages 34.56%, 26.53% and 3.5.0.\n\rMost of the downloads occur in the months October and December, while days are 10th and 11th, while hours are 3rd and 4th and minutes of 59th, 46th, 12th and 0th.\n\rDownload size of 82375220 bytes happens with the highest count of 50, while the IP ID of 334 has most downloads of 55.\n\r\r\rFurther Analysis\r\rWe can do similar analysis for other countries and compare them.\n\rUsing Size it should be possible to understand what is being downloaded.\n\r\rPlease see that\nThis is my Second post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"71b808f1f5d9f2f822b72b73b18bd71c","permalink":"/post/week_31/week-31-r-and-package-downloads/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/post/week_31/week-31-r-and-package-downloads/","section":"post","summary":"# load the packages\rlibrary(readr)\rlibrary(ggplot2)\rlibrary(lubridate)\rlibrary(ggthemr)\rlibrary(gridExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(kableExtra)\rlibrary(readr)\r# load the data\rr_downloads_year \u0026lt;- read_csv(\u0026quot;r_downloads_year.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rr_downloads \u0026lt;- read_csv(\u0026quot;r-downloads.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rIntroduction\rTidy Tuesday is a very good move to improve R programming for anyone who is interested in statistics.","tags":["TidyTuesday","R downloads","R","R package"],"title":"Week 31 : R and Package Downloads","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rMovie Profit, Not So Profit\rThis is my first post on Tidy Tuesday and the data-set in question is Movie profit data-set. Even though the title of data says Movie profit I am going to focus on the movies which did not generate any revenue domestic and suggest on gross in worldwide.\nThe packages that I have used here are magrittr, tidyverse, scales, ggthemr, knitr, kableExtra, ggthemr and lubridate. The theme I am using for plots is “flat dark”.\n\rUnderstand Genre and Mpaa Rating on Movies\r3401 movies with 8 variables of information which include numeric and categorical. There are 202 distributors for movies of four types of ratings which are G, PG, PG-13 and R, but 137 movies have no record of them. Also there are five categories for genre, where Drama with 1236, while horror with 298 movies.\n# Loading the packages\rlibrary(magrittr)\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(ggthemr)\rlibrary(knitr)\rlibrary(kableExtra)\rlibrary(lubridate)\r# load the data\rMovie \u0026lt;- read_csv(\u0026quot;movie_profit.csv\u0026quot;, col_types = cols(X1 = col_skip(), release_date = col_date(format = \u0026quot;%m/%d/%Y\u0026quot;)))\r# Load the theme\rggthemr(\u0026quot;flat dark\u0026quot;)\r# looking at dimensions\rdim(Movie)\rattach(Movie)\r# Bar plot to Genre\rggplot(Movie,aes(genre))+\rgeom_bar()+stat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rxlab(\u0026quot;Genre\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;All movies in perspevtive of Genre \u0026quot;)\rThere are five types of ratings but around half of them are R rated, while 1094 are PG-13. While 573 are in category of PG and G rated movies are only 85. Finally, 137 movies do not have any ratings.\n# Bar plot to MPAA Ratings\rggplot(Movie,aes(mpaa_rating))+\rgeom_bar()+geom_bar()+\rstat_count(aes(y=..count.., label=..count..),geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rxlab(\u0026quot;Rating\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;All movies in persepvtie of MPAA Rating\u0026quot;)\rComedy movies are mostly R rated(under 17 requires guardian) and PG-13 (some material is inappropriate to under 13). Where the frequencies are respectively 367 and 328. 309 movies of adventure genre could be watched by children with accompanying parents and 67 movies can be watched by all ages.\nYet 645 Drama movies are R-rated.There is only one action movie for general audiences(for all) and obviously no horror film should be watched by children alone, yet there are 7 movies which you can watch with your parents.\n#checking for bias in mpaa rating and genre\rkable(table(mpaa_rating,genre),\u0026quot;html\u0026quot;) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Contingency table in counts for Genre versus MPAA Rating\u0026quot;=6)) \r\r\rContingency table in counts for Genre versus MPAA Rating\r\r\r\r\r\rAction\r\rAdventure\r\rComedy\r\rDrama\r\rHorror\r\r\r\r\r\rG\r\r1\r\r67\r\r6\r\r11\r\r0\r\r\r\rPG\r\r34\r\r309\r\r79\r\r144\r\r7\r\r\r\rPG-13\r\r225\r\r83\r\r328\r\r398\r\r58\r\r\r\rR\r\r286\r\r14\r\r367\r\r645\r\r202\r\r\r\r\rWe think horror movies are mostly R-rated then it is true. But only it is explainable by percentage. Yet considering the amount of horror movies made generally it is very low even in this random sample. Action and Comedy movies have very close percentages for PG-13 ratedness, while 52% are R rated for Action and 47% are comedy.\n# column percentage for above table\rkable(table(mpaa_rating,genre) %\u0026gt;%\rprop.table(margin=2) %\u0026gt;%\rround(digits = 2)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Percentage table for Genre versus MPAA Rating\u0026quot;=6))\r\r\rPercentage table for Genre versus MPAA Rating\r\r\r\r\r\rAction\r\rAdventure\r\rComedy\r\rDrama\r\rHorror\r\r\r\r\r\rG\r\r0.00\r\r0.14\r\r0.01\r\r0.01\r\r0.00\r\r\r\rPG\r\r0.06\r\r0.65\r\r0.10\r\r0.12\r\r0.03\r\r\r\rPG-13\r\r0.41\r\r0.18\r\r0.42\r\r0.33\r\r0.22\r\r\r\rR\r\r0.52\r\r0.03\r\r0.47\r\r0.54\r\r0.76\r\r\r\r\rThis data-set contains the release dates from 1956 to 2019. Even though it is not 2019 there is a movie which has been listed here. This explains the domestic and worldwide gross being zero as zero. Then again we have to be careful because there are movies which might not make profit at all, domestic or other wise.\n\rLets Focus of movies which has zero domestic gross\rNo revenue from 66 movies, that is interesting. So obviously Aqua man has a whopping more than 150 million dollars\nproduction budget and no profit because it was not released yet when this data set was compiled. Second rank is for “Wonder park” with 100 million dollars. This movie will be released in 2019.\n\rZero Domestic gross Point of View\rSurprisingly there are movies without any production budget information because I am very sure No movie is done for free. Specially it is odd to see “12 Angry Men” in this list, which leads to the conclusion not all Movies in this list are to be on-it in the first place. We have 66 movies to consider.\n# domestic gross zero only movies\rMovie_domestic_zero\u0026lt;-subset.data.frame(Movie,c(domestic_gross==0)) # checking dimensions\rdim(Movie_domestic_zero)\rattach(Movie_domestic_zero)\r# Scatterplot for production budget\rggplot(Movie_domestic_zero,aes(x=reorder(movie,production_budget),\ry=production_budget))+\rgeom_point()+theme(axis.text.x =element_text(angle = 90, hjust = 1))+\rscale_y_continuous(labels = dollar_format())+\rylab(\u0026quot;Production budget\u0026quot;)+xlab(\u0026quot;Movie names\u0026quot;)+\rggtitle(\u0026quot;Domestic Gross Zero but how production budget varies in Movies\u0026quot;)\r\rGenre and MPAA Rating Point of View\rSo movies with R ratedness have the most count and they are also action and drama genre movies of count 10. Here also there are 11 movies with have not been classified into any rating. Finally, there is no G rated movie in this graphical representation. Majority of movies (31) are from R rated in related to rating. While considering genre the Drama category is represented by 24. Action, Drama and Horror movies includes missing rating.\n#plotting worldwide gross with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(x=mpaa_rating,fill=genre)) +\rgeom_bar(position = \u0026quot;stack\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;MPAA Rating\u0026quot;)+\rgeom_text(aes(label=..count..),stat=\u0026#39;count\u0026#39;,position=position_stack(0.4))+\rggtitle(\u0026quot;MPAA Rating counts with Genre\u0026quot;)\r#plotting worldwide gross with mpaa rating\rMovie_domestic_zero %\u0026gt;% ggplot(aes(fill=mpaa_rating,x=genre)) +\rgeom_bar(position = \u0026quot;stack\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Genre\u0026quot;)+\rgeom_text(aes(label=..count..),stat=\u0026#39;count\u0026#39;,position=position_stack(0.4))+\rggtitle(\u0026quot;Genre counts with MPAA Rating\u0026quot;)\r\rFinding Outliers in Perspective of Genre and MPAA Rating\rConsidering the box-plot there are 3 outliers in Drama while plotting data in perspective of genre. Most amount of production budget is concluded in Action genre, while the least is in Horror.\nIf we focus on the production budget with genre there are 7 outliers and one action movie has spent 100 million dollars, similarly a movie from adventure category spent more than 100 million dollars. Others production budget is way less than 50 million dollars.\n#plotting production budget with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(genre,production_budget)) +\rgeom_boxplot()+ylab(\u0026quot;Production Budget\u0026quot;)+\rxlab(\u0026quot;Genre\u0026quot;)+\rscale_y_continuous(labels = dollar_format())+\rexpand_limits(y=0)+coord_flip()+\rggtitle(\u0026quot;Boxplot for Production budget in perspective of Genre\u0026quot;)\rLeast amount budget is spent on movies of no rating mentioned while most is on PG-13 rated movies and it has one strong outlier. Previously with genre we had 7 outliers but according to MPAA rating there are only 6 outliers.\n#plotting production budget with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(mpaa_rating,production_budget)) +\rgeom_boxplot()+ylab(\u0026quot;Production Budget\u0026quot;)+\rxlab(\u0026quot;MPAA rating\u0026quot;)+\rscale_y_continuous(labels = dollar_format())+\rexpand_limits(y=0)+coord_flip()+\rggtitle(\u0026quot;Boxplot for Production budget in perspective of MPAA Rating\u0026quot;)\r\rProduction Budget and Worldwide Gross\rAccording to the ascending order in the list of 10 movies with lowest production budget only 2 have profited. One movie (All the Boys Love Mandy Lane) has considerably done good, but if you consider this list of 10 movies we have 12 angry men as well.\n# ascending order of top ten movies in production budget\rkable(Movie_domestic_zero[order(Movie_domestic_zero$production_budget),-c(1,4)] %\u0026gt;% head(10),\rcol.names=c(\u0026quot;Movie Name\u0026quot;,\u0026quot;Production Budget\u0026quot;,\u0026quot;Wordlwide Gross\u0026quot;,\u0026quot;Distributor\u0026quot;,\r\u0026quot;MPAA Rating\u0026quot;,\u0026quot;Genre\u0026quot;)) %\u0026gt;% kable_styling(full_width = T,font_size = 13) %\u0026gt;%\radd_header_above(c(\u0026quot;Top 10 Least production budget movies for Domestic gross 0\u0026quot;=6))\r\r\rTop 10 Least production budget movies for Domestic gross 0\r\r\r\r\rMovie Name\r\rProduction Budget\r\rWordlwide Gross\r\rDistributor\r\rMPAA Rating\r\rGenre\r\r\r\r\r\r12 Angry Men\r\r340000\r\r0\r\rUnited Artists\r\rNA\r\rDrama\r\r\r\rMy Beautiful Laundrette\r\r400000\r\r0\r\rOrion Classics\r\rNA\r\rDrama\r\r\r\rEverything Put Together\r\r500000\r\r7890\r\rNA\r\rR\r\rDrama\r\r\r\rAll the Boys Love Mandy Lane\r\r750000\r\r1960521\r\rRadius\r\rR\r\rHorror\r\r\r\rJimmy and Judy\r\r1000000\r\r0\r\rOutrider Pictures\r\rR\r\rAction\r\r\r\rThe Poker House\r\r1000000\r\r0\r\rPhase 4 Films\r\rR\r\rDrama\r\r\r\rProud\r\r1000000\r\r0\r\rCastle Hill Product…\r\rPG\r\rDrama\r\r\r\rSteppin: The Movie\r\r1000000\r\r0\r\rWeinstein Co.\r\rPG-13\r\rComedy\r\r\r\rZombies of Mass Destruction\r\r1000000\r\r0\r\rAfter Dark\r\rR\r\rComedy\r\r\r\rGrand Theft Parsons\r\r1200000\r\r0\r\rSwipe Films\r\rPG-13\r\rDrama\r\r\r\r\rThis indicates that we didn’t have records how much of profit in home and away properly, because there is no way that people did not watch that movie and not make any gross. So we conclude that some movies which were released before 1970s did not pertain any information of gross domestic or worldwide.\n\rYears, Months and Days versus Production Budget\rThere are 4 Movies before 1972 with zero for domestic gross which can conclude loss of information. Oddly in year 2014 there are 8 movies with zero domestic gross and most of the movies are after year 2000.\n# plotting years vs movies released\rggplot(Movie_domestic_zero,aes(x=year(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Years\u0026quot;)+\rscale_x_continuous(label=1956:2019,breaks=1956:2019)+\rscale_y_continuous(labels = 0:8,breaks = 0:8)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rtheme(axis.text.x = element_text(angle = 90,hjust = 2))\rConsidering the months there is no specialty most of the counts are in-between 3 and 8. In January there are only two movies.\nggplot(Movie_domestic_zero,aes(x=month(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Months\u0026quot;)+\rscale_x_continuous(label=1:12,breaks=1:12)+\rscale_y_continuous(labels = 0:8,breaks = 0:8)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)\rOnly the movies with release dates 19th and 25th have no domestic gross, while highest count of 6 occurs on 21st. Most of the days have the count of 1 movie.\nggplot(Movie_domestic_zero,aes(x=day(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Days\u0026quot;)+\rscale_x_continuous(label=1:31,breaks=1:31)+\rscale_y_continuous(labels = 0:6,breaks = 0:6)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rIn the complete data set Drama Genre has most (1236) counts and least (298) count goes to Horror. While, most (1514) of the Movies are R rated, least count goes to Grated movies. Considering Genre and Rating, it is true that Horror movies are R rated while it represents 76%, and should not let children watch alone.\n\rWhile there are movies with No domestic gross some have not been released yet (Aqua man and Wonder Park). Further, some Movies do not even have worldwide gross. This causes missing information. Even though a famous movies such as “12 Angry Men”.\n\rThis missing information could be the related to the fact that there are 4 movies which were released before 1972. Oddly in 2014 there are 8 movies which do not contain domestic gross information. Further, most of these movies were released after year 2000.\n\rBox plot indicates Adventure genre have spent more range in production budget, while in perspective of MPAA rating PG-13 movies have most range in production budget with a clear outlier.\n\r\r\rFurther Analysis\r\rSimilarly we can focus on movies of world wide gross equals to zero with other variables.\n\rConduct scrutinized interest with movies of world wide gross zero and domestic gross zero.\n\r\rPlease see that This is my first post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\n\r","date":1542153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542153600,"objectID":"ecbe15e4570ea80c140b0b32dee79344","permalink":"/post/week_30/week-30-movie-profit/","publishdate":"2018-11-14T00:00:00Z","relpermalink":"/post/week_30/week-30-movie-profit/","section":"post","summary":"Movie Profit, Not So Profit\rThis is my first post on Tidy Tuesday and the data-set in question is Movie profit data-set. Even though the title of data says Movie profit I am going to focus on the movies which did not generate any revenue domestic and suggest on gross in worldwide.\nThe packages that I have used here are magrittr, tidyverse, scales, ggthemr, knitr, kableExtra, ggthemr and lubridate. The theme I am using for plots is “flat dark”.","tags":["Movies","R","R package","TidyTuesday"],"title":"Week 30: Movie Profit","type":"post"},{"authors":null,"categories":null,"content":"I started reading books from month of April, 2018. Which is very useful for my life as a believer of self-learning. I hope to share these books with you so that it will be useful. There is no meaning in numbers except that they are just for ordering from 1 to further in no hierarchy. Genre specificity is not my thing therefore these books are not divided into groups.\n Rise the Dark by Michael Koryta The Bourne Enigma by Eric Van LustBader Jack Reacher thriller, A Wanted Man by Lee Child Digital Fortress by Dan Brown The Selfish Gene by Richard Dawkins How Democracies Die by Steven Levitsky and Daniel Ziblatt Elon Musk by Ashlee Vance Roll of Thunder Hear My Cry by Mildred Taylor Boy by Roald Dohl Animal Farm by George Orwell The power of Habit by Charles Duhigg  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"244490054b04706f9ddeb683c046cfbd","permalink":"/projects/books/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/books/","section":"projects","summary":"The names of books with amazon links.","tags":["Books"],"title":"Books I have read","type":"projects"},{"authors":null,"categories":null,"content":"Watching movies for the last six years(from 2012) with much interest in plot, story line and characters. I have watched more than 300 movies but the below mentioned are in my mind. They are from different decades as well, therefore do not hesistate to watch. If you prefer go through the IMDB link so that the ratings and summary will give you more information. 36 Movies will be mentioned below.\n 12 Angry Men Batman Trilogy Casablanca Dan Brown Trilogy Die Hard Quadropolgy Despicable Me Collection Dr. Strangelove Fight Club Forrest Gump Godfather Inception Interstellar Internal Affairs Ip man Lock, Stock and Two Smoking Barrels Lord of The Rings Collection Memento Mr and Mrs Smith One flew over the cuckoos nest Se7en Saving Private Ryan Shutter Island Schindlers List The Departed The Good The Bad and The Ugly The Shawshank Redemption The Silence of the Lambs The Sting The Terminal The Truman Show The Matrix Trilogy The Bourne Collection The Hobbit Collection 35 The Raid The Prestige V for Vendetta Unbreakable Unforgiven Wall-E  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"e6070f9b978f4b08725e8872e93320f3","permalink":"/projects/movies/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/movies/","section":"projects","summary":"The movies wish are very brilliant in every way with their IMDB links.","tags":["Movies"],"title":"Movies","type":"projects"},{"authors":null,"categories":null,"content":"More than 50 TV shows are on my list and I have watched all of them. There are alot of shows in this list where all would agree be great and enticing. There is no specific genre of my interest and I do not mind rewatching these TV shows again.\n A bit of Fry and Laurie Altered Carbon Band of Brothers Black Mirror Blue Planet I,II BroadChurch Barry Brookynn Nine Nine Chance Condor Cosmos A space time odyssey Doctor Who Fawlty Towers Friends House House of Cards US House of Cards UK How I Met Your Mother Jeeves and Wooster Killing Eve Line of Duty Luther Monty Python The Flying Circus Mind Your Language Mr. Robot Narcos Not the Nine O\u0026rsquo;Clock News Ozark Person of Interest Peaky Blinders Planet Earth I,II Preacher Rick and Morty Seinfeld Southpark Sherlock The Fresh Prince of Bel-Air The Good Place The Mentalist The Blacklist The Grand Tour The Night Manager The Black Adder The Thin Blue Line The Wire Thick of It Tom Clancys Jack Ryan Top Gear Two and a Half Men Unforgotten Westworld Yes Minister Yes, Prime Minister  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"6d07d48d82d1eebc5d6cadd95c933074","permalink":"/projects/tvshows/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/tvshows/","section":"projects","summary":"List of TV shows with their respective IMDB links.","tags":["TVshows"],"title":"TV Shows","type":"projects"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1538265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538265600,"objectID":"13f73cb07ffc7c1c6211e017748215fb","permalink":"/publication/fitodbod-website/","publishdate":"2018-09-30T00:00:00Z","relpermalink":"/publication/fitodbod-website/","section":"publication","summary":" Using [pkgdown](https://pkgdown.r-lib.org/) a genuine website for fitODBOD was generated. There is no extra work to be done here, because pkgdown uses our exisiting man files and vignettes to create this website. It is quie easy and convenient for a user to use this website generated for fitODBOD. Rather than the CRAN or GitHub  versions, which only provides a manual in the website there are clear and concise examples of how functions can be used for researchers","tags":["package","R","CRAN","fitODBOD","GitHub","Website"],"title":"Website-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1536969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536969600,"objectID":"65a60044eeb2d2c5c9baaf4583c37a9a","permalink":"/publication/fitodbod-github/","publishdate":"2018-09-15T00:00:00Z","relpermalink":"/publication/fitodbod-github/","section":"publication","summary":" GitHub version is there for version 1.2.0 with is maintained by me if issues are raised regarding the package. While building under CRAN restrictions some examples and vignettes were not included to submission but there are in R-fitODBOD repository. Readme file includes a badge and a small example of how to fit Binomial outcome data which cannot be modeled by Binomial distribution. Finally, if there are any issues regarding the functions or suggestions you can use the issue tab to report them to me.","tags":["R","GitHub","Package"],"title":"GitHub-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536431400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536431400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+05:30","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["M.Amalan, P.Wijekoon"],"categories":null,"content":"","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"8ead533fab86338ce8e8811de7b71ffb","permalink":"/publication/fitodbod-cran/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/publication/fitodbod-cran/","section":"publication","summary":"Binomial Outcome Data has vast amount use in the field of biology, medicine and epidemiology. Modeling such data relative to Binomial Mixture Distributions were discussed because Binomial distribution fails to model the data. The reason is actual observed variance of the data is greater than the assumed theoretical variance, which is defined as over dispersion. fitODBOD package has two versions until October 2018, but in the future there will be updates regularly each year. Whenever there are new distributions to model BOD the new versions will include these distributions. Further, if there are new techniques to make these distributions more useful they will be also added as functions. Version 1.1.0 was published in February 2018 and Version 1.20 was released in September 2018 with two new distributions.","tags":["package","R","CRAN","fitODBOD"],"title":"CRAN-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"60b94c34fc45f1e50db023bee5de657a","permalink":"/publication/4th-year/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/publication/4th-year/","section":"publication","summary":"This is a least viable product of R package for fitting Binomial outcome data, and an abstract, extended articles were written in related to this minimum viable product.","tags":["package","R","CRAN","fitODBOD"],"title":"R Package Development to Model Over Dispersed Binomial Outcome Data with the use of Binomial Mixture Distributions and Alternate Binomial Distributions.","type":"publication"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483209000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483209000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00+05:30","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]