[{"authors":null,"categories":["Rshiny"],"content":"\rShiny App of Text Analytics for South Park Tv show.\rNo Inputs needed to generate Plots or Results\rTrivia Sub Tab\rLines Sub Tab\rWords Sub Tab\rSpecial Words Sub Tab\rRatings and Votes from IMDB Sub Tab\rSentiment Analysis Sub Tab\rBigram and Trigram Analysis Sub Tab\r\rInputs needed from the user to generate Plots and Results\rCompare Two Seasons Tab.\rCompare Two Characters Tab.\rCompare Two Characters but Same Season Tab.\rCompare Two Seasons but Same Character Tab.\r\rAbout the Author Tab.\r\r\r\rShiny App of Text Analytics for South Park Tv show.\rR Shiny Apps are very popular, that is why I developed my first Shiny App based on Olympic Data from Kaggle. It will be updated after year 2020 Olympic games. Here is the Link. After this I now developed something grand-eloquent, which is for the long running Tv series South Park.\nThis show has 22 seasons aired until now. I was able to find the data for scripts of all the aired episodes until season 18 from the GitHub repository of . While this was more than enough information I was focusing on getting data for the rest of the seasons(19,20,21,22). For this task I briefly studied web scraping and thankfully it was helpful in obtaining that data with a little amount of knowledge of string manipulation.\nFinally I was able to create one massive data set which would have all the script details from season 1 to season 22 for all episodes with who spoke them and what did these Characters say without unnecessary background interpretations or scenario desctiptions.\nBut this was not the end, because I was lucky enough to find the R package in GitHub southparkr. This package included information from the IMDB website regarding Episode Name, Rating and Votes. Further it also included similar information regarding the scripts but more thoroughly until recent seasons. Which gave me an idea to use this package for data regarding the Rating and votes from IMDB because I already had the information for scripts.\nNo Inputs needed to generate Plots or Results\rPlots generated here are to summarize so far how the South park season 1 to 22 has changed. In terms of Swear words, stop words, all words, sentiment and much more. If we focus on sentiment analysis it is more clear when you read documents related to AFINN, bing and nrc.\nTrivia Sub Tab\rThis tab includes information in plots mainly generated by plotly with some memes from South Park. So patiently wait until they load, you can scroll through the page and read stuff.\n\rLines Sub Tab\r\rWords Sub Tab\r\rSpecial Words Sub Tab\r\rRatings and Votes from IMDB Sub Tab\r\rSentiment Analysis Sub Tab\r\rBigram and Trigram Analysis Sub Tab\r\r\rInputs needed from the user to generate Plots and Results\rInputs from user where they can choose their own will be used to generate plots under these several tabs. Below generated are only plots but nothing more. These comparisons are mainly about Most number of lines, Most number of words, Most number of words without stop words, Swear words and sentiment analysis.\nSentiment Analysis is related to AFINN, bing and nrc techniques. Further, nrc has subgroups which are related to 10 different emotions. While AFINN and bing has only two emotions which are positive and negative.\nCompare Two Seasons Tab.\r\rCompare Two Characters Tab.\r\rCompare Two Characters but Same Season Tab.\r\rCompare Two Seasons but Same Character Tab.\r\r\rAbout the Author Tab.\rIf you Click this, it will automatically open a tab which will lead you to my personal website, which is this one.\nTHANK YOU\n\r\r","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"091dfc122a41c47fec5f54906b6f5eb1","permalink":"/post/southpark_rshiny/southpark_rshiny/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/post/southpark_rshiny/southpark_rshiny/","section":"post","summary":"R shiny App to show text analytics elements of the Tv Show South Park from Season 1 to 22.","tags":["SouthPark","Rshiny","tidyverse","gganimate"],"title":"South Park Text Analytics Shiny App","type":"post"},{"authors":["Puthussery S","Mahendran R","M.Amalan"],"categories":null,"content":"","date":1555891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555891200,"objectID":"9de32f9ba31c404262352cc19db2b5ed","permalink":"/publication/southasia_antenatal_depression/","publishdate":"2019-04-22T00:00:00Z","relpermalink":"/publication/southasia_antenatal_depression/","section":"publication","summary":"Meta analysis was conducted by me for the provided data using the packages meta and metafor from R statistical programming software. In the full article the package versions are clearly mentioned. Data was used to generate forrest plots, funnel plots and find the prevalance rates. This article is published in the most prestigious Journal of Epidemiology and Community Health of BMJ Journals.","tags":["package","R","meta","metafor","meta-analysis"],"title":"Prevalence of antenatal depression in South Asia: a systematic review and meta-analysis.","type":"publication"},{"authors":null,"categories":["TidyTuesday"],"content":"library(readr)\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(gganimate)\rplayer_dob \u0026lt;- read_csv(\u0026quot;player_dob.csv\u0026quot;, col_types = cols(date_of_birth = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), date_of_first_title = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;)))\rgrand_slams \u0026lt;- read_csv(\u0026quot;grand_slams.csv\u0026quot;, col_types = cols(gender = col_factor(levels = c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;)), rolling_win_count = col_integer(), tournament_date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), year = col_integer()))\rgrand_slam_timeline \u0026lt;- read_csv(\u0026quot;grand_slam_timeline.csv\u0026quot;, col_types = cols(gender = col_factor(levels = c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;)), year = col_integer()))\rWinning the Wimbledon as your first grand slam looks very thin than other three tournaments. Also among the top 10 most grand slam winners Roger Federer and Chris Evert have reached semi-finals only a lot of times. .code : https://t.co/tOify2Npwd #tidytuesday pic.twitter.com/9Vfa3CXmne\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) April 9, 2019  GitHub Code\nPlayer Information\rDecade of Birth vs First Grand Slam Title Won\rplayer_dob %\u0026gt;% remove_missing() %\u0026gt;%\rmutate(grand_slam=recode_factor(grand_slam,\r\u0026#39;Wimbledon\u0026#39;=\u0026quot;Wimbledon\u0026quot;,\r\u0026#39;US Open\u0026#39;=\u0026quot;US Open\u0026quot;,\r\u0026#39;French Open\u0026#39;=\u0026quot;French Open\u0026quot;,\r\u0026#39;Australian Open\u0026#39;=\u0026quot;Aus Open\u0026quot;,\r\u0026#39;Australian Open (January)\u0026#39;=\u0026quot;Aus Open\u0026quot;,\r\u0026#39;Australian Open (Jan)\u0026#39;=\u0026quot;Aus Open\u0026quot;, \u0026#39;Australian Open (December)\u0026#39;=\u0026quot;Aus Open\u0026quot;, \u0026#39;Australian Open (Jan.)\u0026#39;=\u0026quot;Aus Open\u0026quot; )) %\u0026gt;%\rmutate(Birth=year(date_of_birth)) %\u0026gt;%\rmutate(Birth=cut(Birth,breaks = c(1929,1939,1949,1959,1969,1979,1989,1999),\rlabels = c(1930,1940,1950,1960,1970,1980,1990)\r)) %\u0026gt;%\rgroup_by(Birth,grand_slam) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=factor(Birth),y=n,fill=grand_slam))+\rgeom_col(position = \u0026quot;dodge\u0026quot;)+\rscale_y_continuous(breaks=seq(1,11,1),labels=seq(1,11,1))+\rlabs(fill=\u0026quot;Grand Slam\u0026quot;)+\rgeom_text(aes(label=n),position = position_dodge(width = 1),vjust=1)+\rxlab(\u0026quot;Decade of Birth\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;How Decade of Birth and First Win of Grand Slam changes\u0026quot;)\r\rDecade of Birth vs First Grand Slam with Age\rplayer_dob %\u0026gt;% remove_missing() %\u0026gt;%\rmutate(grand_slam=recode_factor(grand_slam,\r\u0026#39;Wimbledon\u0026#39;=\u0026quot;Wimbledon\u0026quot;,\r\u0026#39;US Open\u0026#39;=\u0026quot;US Open\u0026quot;,\r\u0026#39;French Open\u0026#39;=\u0026quot;French Open\u0026quot;,\r\u0026#39;Australian Open\u0026#39;=\u0026quot;Aus Open\u0026quot;,\r\u0026#39;Australian Open (January)\u0026#39;=\u0026quot;Aus Open\u0026quot;,\r\u0026#39;Australian Open (Jan)\u0026#39;=\u0026quot;Aus Open\u0026quot;, \u0026#39;Australian Open (December)\u0026#39;=\u0026quot;Aus Open\u0026quot;, \u0026#39;Australian Open (Jan.)\u0026#39;=\u0026quot;Aus Open\u0026quot; )) %\u0026gt;%\rmutate(Birth=year(date_of_birth)) %\u0026gt;%\rmutate(Birth=cut(Birth,breaks = c(1929,1939,1949,1959,1969,1979,1989,1999),\rlabels = c(1930,1940,1950,1960,1970,1980,1990)\r)) %\u0026gt;%\rggplot(.,aes(x=grand_slam,size=round(age/365),y=Birth))+\rgeom_jitter()+\rxlab(\u0026quot;Grand Slam\u0026quot;)+ylab(\u0026quot;Decade of Birth\u0026quot;)+\rlabs(color=\u0026quot;Age\u0026quot;,size=\u0026quot;Age in Years\u0026quot;)+\rggtitle(\u0026quot;Birth Decade vs First Grand Slam with Age\u0026quot;)\r\r\rGrand Slams\rGender vs Grand Slam with Name\rgrand_slams %\u0026gt;%\rgroup_by(name,gender) %\u0026gt;%\rcount(sort = TRUE) %\u0026gt;%\rhead(25) %\u0026gt;%\rggplot(.,aes(x=fct_inorder(name),y=n,\rfill=gender,label=n))+\rgeom_col()+xlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rlabs(fill=\u0026quot;Gender\u0026quot;)+\rcoord_flip()+geom_text(hjust =1)+\rggtitle(\u0026quot;Who won most with Gender\u0026quot;)\r\rGender vs Grand Slam with Year\rp\u0026lt;-grand_slams %\u0026gt;%\rggplot(.,aes(x=name,y=rolling_win_count,\rshape=gender,color=grand_slam))+\rgeom_point()+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Cumulative Count\u0026quot;)+\rlabs(color=\u0026quot;Grand Slam\u0026quot;,shape=\u0026quot;Gender\u0026quot;)+\rtransition_time(tournament_date)+ease_aes(\u0026quot;linear\u0026quot;)+\rcoord_flip()+shadow_mark()+\rggtitle(\u0026quot;Cumulative progress with Year: {year(frame_time)}\u0026quot;)\ranimate(p,nframes = 52,fps=1)\r\r\rGrand Slam Timeline\rTop 10 players and their Outcomes\rtop10\u0026lt;-grand_slams %\u0026gt;%\rgroup_by(name) %\u0026gt;%\rcount(sort = TRUE) %\u0026gt;%\rhead(10) %\u0026gt;%\rselect(name)\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rgroup_by(name,outcome) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(name,n,fill=outcome))+\rgeom_col(position=position_dodge(width = 0.95))+\rgeom_text(aes(label=n),position = position_dodge(width = 0.95),hjust=1)+\rcoord_flip()+labs(fill=\u0026quot;Outcome\u0026quot;)+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Top 10 Tennis Players and their Outcomes\u0026quot;)\r\rTop 10 Players and Tournament with outcomes of activeness\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rgroup_by(tournament,outcome) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=tournament,y=n,fill=outcome))+\rgeom_col(position =position_dodge(width=0.95))+\rgeom_text(aes(label=n),position =position_dodge(width=0.95),vjust=1)+\rlabs(fill=\u0026quot;Outcome\u0026quot;)+\rxlab(\u0026quot;Tournament\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Tournament perspective of Top 10 Tennis Players and their Outcomes\u0026quot;)\r\rTop 10 Players Winning based on Tournament\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rsubset(outcome==\u0026quot;Won\u0026quot;) %\u0026gt;%\rgroup_by(name,tournament) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=name,y=n,fill=tournament))+\rgeom_col(position =position_dodge(width=0.95))+\rgeom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\rcoord_flip()+\rlabs(fill=\u0026quot;Tournament\u0026quot;)+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Winning, Top 10 Tennis Players with related to Tournament\u0026quot;)\r\rTop 10 Players Finalist based on Tournament\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rsubset(outcome==\u0026quot;Finalist\u0026quot;) %\u0026gt;%\rgroup_by(name,tournament) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=name,y=n,fill=tournament))+\rgeom_col(position =position_dodge(width=0.95))+\rgeom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\rcoord_flip()+\rlabs(fill=\u0026quot;Tournament\u0026quot;)+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Finalist, Top 10 Tennis Players with related to Tournament\u0026quot;)\r\rTop 10 Players Semi-Finalist based on Tournament\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rsubset(outcome==\u0026quot;Semi-finalist\u0026quot;) %\u0026gt;%\rgroup_by(name,tournament) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=name,y=n,fill=tournament))+\rgeom_col(position =position_dodge(width=0.95))+\rgeom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\rcoord_flip()+\rlabs(fill=\u0026quot;Tournament\u0026quot;)+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Semi-Finalist, Top 10 Tennis Players with related to Tournament\u0026quot;)\r\rTop 10 Players Retired based on Tournament\rgrand_slam_timeline %\u0026gt;%\rrename(name=\u0026quot;player\u0026quot;) %\u0026gt;%\rinner_join(top10,\u0026quot;name\u0026quot;) %\u0026gt;%\rsubset(outcome==\u0026quot;Retired\u0026quot;) %\u0026gt;%\rgroup_by(name,tournament) %\u0026gt;%\rcount() %\u0026gt;%\rggplot(.,aes(x=name,y=n,fill=tournament))+\rgeom_col(position =position_dodge(width=0.95))+\rgeom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\rcoord_flip()+\rlabs(fill=\u0026quot;Tournament\u0026quot;)+\rxlab(\u0026quot;Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rggtitle(\u0026quot;Retired, Top 10 Tennis Players with related to Tournament\u0026quot;)\rTHANK YOU\n\r\r","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"d6885f20f0d77dbf24556d6dbd328e7b","permalink":"/post/tidytuesday2019/week15/week15/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/post/tidytuesday2019/week15/week15/","section":"post","summary":"2019 Week 15 TidyTuesday: Tennis Tournaments","tags":["TidyTuesday","tidyverse","gganimate"],"title":"Week 15 : Tennis Tournaments","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rCrossing vs Direction with Morning or Evening for Everyday\rAverag Bike Count in Different Time Laps\rMonthly Average Bike Count for Everyday with AM or PM\rDay by Average Bike Count for Everyday with AM or PM\rHourly Average Bike Count for Every Month with AM or PM\r\rPedestrian Count With Different Time Laps\rMonthly Pedestrian Count for Everyday with AM or PM when TRUE\rMonthly Pedestrian Count for Everyday with AM or PM when FALSE\rDay by Pedestrian Count for Everyday with AM or PM when TRUE\rDay by Pedestrian Count for Everyday with AM or PM when FALSE\rHourly Pedestrian Count for Every Month with AM or PM when TRUE\rHourly Pedestrian Count for Every Month with AM or PM when FALSE\r\rAverage Bike Count with Crossings\rAverage Bike Count with Directions\rAverage Bike Count with Crossings by Day\rAverage Bike Count with Directions by Day\r\r\rlibrary(tidyverse)\rlibrary(dplyr)\rlibrary(gganimate)\rlibrary(ggthemr)\rlibrary(splitstackshape)\rlibrary(lubridate)\rlibrary(readr)\rbike_traffic \u0026lt;- read_csv(\u0026quot;bike_traffic.csv\u0026quot;)\r#bike_traffic \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-02/bike_traffic.csv\u0026quot;)\rblogdown::shortcode(\u0026quot;tweet\u0026quot;,\u0026quot;1113036696214495232\u0026quot;)\rBurke Gilman Trail, Elliot Bay Trail and MTS Trail are crossings having a higher average count of bikes than others over the Years while plotting them under days of a week. #TidyTuesday pic.twitter.com/wCj6PLv8DM\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) April 2, 2019  GitHub Code\nCrossing vs Direction with Morning or Evening for Everyday\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rselect(DMY,MorE,direction,crossing,bike_count) %\u0026gt;%\rggplot(.,aes(x=direction,y=str_wrap(crossing,15),\rcolor=MorE,size=bike_count))+\rgeom_jitter()+\rxlab(\u0026quot;Direction\u0026quot;)+ylab(\u0026quot;Crossing\u0026quot;)+\rtransition_time(DMY)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Crossing and Direction for Bike Count\u0026quot;,\rsubtitle = \u0026quot;Date: {frame_time}\u0026quot;)\ranimate(p,nframes=1899,fps=1) \r\rAverag Bike Count in Different Time Laps\rMonthly Average Bike Count for Everyday with AM or PM\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsummarise(Average=mean(bike_count,na.rm = TRUE)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rscale_y_continuous(labels=seq(0,175,5),breaks=seq(0,175,5))+\rxlab(\u0026quot;Month\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rDay by Average Bike Count for Everyday with AM or PM\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsummarise(Average=mean(bike_count,na.rm = TRUE)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rscale_y_continuous(labels=seq(0,170,10),breaks=seq(0,170,10))+\rxlab(\u0026quot;Day\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rHourly Average Bike Count for Every Month with AM or PM\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Hour=hour(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Hour) %\u0026gt;%\rsummarise(Average=mean(bike_count,na.rm = TRUE)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Hour\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,55,5),breaks=seq(0,55,5))+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\r\rPedestrian Count With Different Time Laps\rMonthly Pedestrian Count for Everyday with AM or PM when TRUE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsubset(ped_count==TRUE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Month\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,22.5,2.5),breaks=seq(0,22.5,2.5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rMonthly Pedestrian Count for Everyday with AM or PM when FALSE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsubset(ped_count==FALSE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Month\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,75,5),breaks=seq(0,75,5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rDay by Pedestrian Count for Everyday with AM or PM when TRUE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsubset(ped_count==TRUE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rxlab(\u0026quot;Day\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,22.5,2.5),breaks=seq(0,22.5,2.5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rDay by Pedestrian Count for Everyday with AM or PM when FALSE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Day=day(DMY)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Day) %\u0026gt;%\rsubset(ped_count==FALSE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rxlab(\u0026quot;Day\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,70,5),breaks=seq(0,70,5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rHourly Pedestrian Count for Every Month with AM or PM when TRUE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Hour=hour(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Hour) %\u0026gt;%\rsubset(ped_count==TRUE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Hour\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,60,5),breaks=seq(0,60,5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rHourly Pedestrian Count for Every Month with AM or PM when FALSE\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Hour=hour(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,Hour) %\u0026gt;%\rsubset(ped_count==TRUE) %\u0026gt;%\rsummarise(Average=summary.factor(ped_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Hour\u0026quot;)+ylab(\u0026quot;Pedestrian Count\u0026quot;)+\rscale_y_continuous(labels=seq(0,60,5),breaks=seq(0,60,5))+\rggtitle(\u0026quot;Pedestrian Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\r\rAverage Bike Count with Crossings\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,crossing) %\u0026gt;%\rsummarise(Average=mean(bike_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=str_wrap(crossing,8),\rcolor=Month,shape=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Crossing\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rAverage Bike Count with Directions\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rgroup_by(MorE,Year,Month,direction) %\u0026gt;%\rsummarise(Average=mean(bike_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=str_wrap(direction,10),\rcolor=Month,shape=MorE))+\rgeom_jitter()+transition_time(Year)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Direction\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rAverage Bike Count with Crossings by Day\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rmutate(Wday=wday(DMY,label=TRUE)) %\u0026gt;%\rgroup_by(Year,Month,Wday,crossing) %\u0026gt;%\rsummarise(Average=mean(bike_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=Wday,color=Month))+\rgeom_jitter()+transition_time(Year)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;,axis.text.x = element_text(hjust=1,angle = 45))+\rease_aes(\u0026quot;linear\u0026quot;)+facet_grid(~str_wrap(crossing,10))+\rxlab(\u0026quot;Day of the week\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \r\rAverage Bike Count with Directions by Day\rp\u0026lt;-bike_traffic %\u0026gt;%\rcSplit(\u0026quot;date\u0026quot;,sep=\u0026quot; \u0026quot;) %\u0026gt;%\rrename(\u0026quot;DMY\u0026quot;=\u0026quot;date_1\u0026quot;) %\u0026gt;%\rrename(\u0026quot;HMS\u0026quot;=\u0026quot;date_2\u0026quot;) %\u0026gt;%\rrename(\u0026quot;MorE\u0026quot;=\u0026quot;date_3\u0026quot;) %\u0026gt;%\rmutate(DMY=mdy(DMY)) %\u0026gt;%\rmutate(HMS=hms(HMS)) %\u0026gt;%\rmutate(Year=year(DMY)) %\u0026gt;%\rmutate(Month=month(DMY)) %\u0026gt;%\rmutate(Wday=wday(DMY,label=TRUE)) %\u0026gt;%\rgroup_by(Year,Month,Wday,direction) %\u0026gt;%\rsummarise(Average=mean(bike_count)) %\u0026gt;%\rggplot(.,aes(y=Average,x=Wday,color=Month))+\rgeom_jitter()+transition_time(Year)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rease_aes(\u0026quot;linear\u0026quot;)+facet_grid(~direction)+\rxlab(\u0026quot;Day of the Week\u0026quot;)+ylab(\u0026quot;Average Bike Count\u0026quot;)+\rggtitle(\u0026quot;Average Bike Count changing with Time\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,nframes=7,fps=1) \rTHANK YOU\n\r","date":1554163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554163200,"objectID":"34d6a7e946380e7921708694db7cb3df","permalink":"/post/tidytuesday2019/week14/week14/","publishdate":"2019-04-02T00:00:00Z","relpermalink":"/post/tidytuesday2019/week14/week14/","section":"post","summary":"2019 Week 14 TidyTuesday: Seattle Bike Data","tags":["TidyTuesday","tidyverse","gganimate"],"title":"Week 14 : Seattle Bikes","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rYearly Change\rZipcode Counts Over the Years\rSpecies Counts Over the Years\rPrimary Breed Over the Years\rAnimals Name Over the Years\r\rZipCode and Choices\rZipcode with choices of Species\rZipcode with choices of Primary Breed\rZipcode with choices of Animals Name\r\rPrimary and Secondary Breed Choices Over the Years\rSpecies and Name choices for Animals\r\r\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(gganimate)\rlibrary(splitstackshape)\rlibrary(forcats)\rlibrary(ggthemr)\rggthemr(\u0026quot;flat dark\u0026quot;)\rseattle_pets \u0026lt;- read_csv(\u0026quot;seattle_pets.csv\u0026quot;)\rDaisy is a name no longer used after 2016 even though it is popular before that and a sudden surge from 2017 towards registering pets. #TidyTuesday pic.twitter.com/bnYSgnkhac\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) March 28, 2019  GitHub Code\nYearly Change\rZipcode Counts Over the Years\rp\u0026lt;-seattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(zip_code,Year) %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rcount(zip_code) %\u0026gt;%\rremove_missing() %\u0026gt;%\rsubset(Year \u0026gt;=2010) %\u0026gt;%\rtop_n(25) %\u0026gt;%\rggplot(.,aes(x= fct_infreq(zip_code),y=n,fill=factor(Year)))+\rgeom_col()+transition_time(Year)+ease_aes(\u0026quot;linear\u0026quot;) +\rcoord_flip()+xlab(\u0026quot;Zip code\u0026quot;)+ylab(\u0026quot; Count\u0026quot;)+\rlabs(fill=\u0026quot;Year\u0026quot;)+\rscale_y_continuous(breaks=seq(0,3000,250),labels = seq(0,3000,250))+\rggtitle(\u0026quot;Zip Code Over the Years\u0026quot;,subtitle=\u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,fps=1,nframes=7)\r\rSpecies Counts Over the Years\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(species,Year) %\u0026gt;%\rsubset(Year \u0026gt;=2016) %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rcount(species) %\u0026gt;%\rggplot(.,aes(x= species,y=n,fill=factor(Year),label=n))+\rgeom_col()+geom_text()+\rtransition_states(Year,transition_length = 2,state_length = 2)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+\rxlab(\u0026quot;Species\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+ labs(fill=\u0026quot;Year\u0026quot;)+\rscale_y_continuous(breaks=seq(0,23000,1000),labels=seq(0,23000,1000))+\rggtitle(\u0026quot;Species Over the Years\u0026quot;,subtitle = \u0026quot;Year: {closest_state}\u0026quot;)\r\rPrimary Breed Over the Years\rp\u0026lt;-seattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(primary_breed,Year) %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rcount(primary_breed) %\u0026gt;%\rremove_missing() %\u0026gt;%\rsubset(Year \u0026gt;=2010) %\u0026gt;%\rtop_n(15) %\u0026gt;%\rggplot(.,aes(x= str_wrap(primary_breed,20),y=n,label=n,fill=factor(Year)))+\rgeom_col()+transition_time(Year)+ease_aes(\u0026quot;linear\u0026quot;) +\rcoord_flip()+geom_text()+labs(fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;Primary Breed\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rscale_y_continuous(breaks=seq(0,6000,500),labels=seq(0,6000,500))+\rggtitle(\u0026quot;Primary Breed Over the Years\u0026quot;,subtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,fps=1,nframes=8)\r\rAnimals Name Over the Years\rp\u0026lt;-seattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(animals_name,Year) %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rcount(animals_name) %\u0026gt;%\rremove_missing() %\u0026gt;%\rsubset(Year \u0026gt;=2010) %\u0026gt;%\rtop_n(5) %\u0026gt;%\rggplot(.,aes(x= fct_infreq(animals_name),y=n,fill=factor(Year),label=n))+\rgeom_col()+transition_time(Year)+ease_aes(\u0026quot;linear\u0026quot;) +\rcoord_flip()+geom_text()+labs(fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;Animals Name\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rscale_y_continuous(breaks = seq(0,275,25),labels=seq(0,275,25))+\rggtitle(\u0026quot;Animales N ame Over the Years\u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)\ranimate(p,fps=1,nframes=8)\r\r\rZipCode and Choices\rZipcode with choices of Species\rp\u0026lt;-seattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(zip_code,species,Year,Month) %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rcSplit(\u0026quot;zip_code\u0026quot;,sep = \u0026quot;-\u0026quot;) %\u0026gt;%\rremove_missing() %\u0026gt;%\runite(\u0026quot;zip_code\u0026quot;,c(\u0026quot;zip_code_1\u0026quot;,\u0026quot;zip_code_2\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=factor(Month),y=factor(zip_code),\rshape=species,color=factor(Year)))+geom_jitter()+\rtransition_time(Year)+ ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Zipcode vs Species of Choice \u0026quot;,\rsubtitle = \u0026quot;Year : {frame_time}\u0026quot;)+\rshadow_mark()+xlab(\u0026quot;Month\u0026quot;)+ylab(\u0026quot;Zip Code\u0026quot;)+\rlabs(color=\u0026quot;Year\u0026quot;,shape=\u0026quot;Species\u0026quot;)\ranimate(p,nframes=4,fps=1)\ra\u0026lt;-seattle_pets %\u0026gt;%\rcount(zip_code) %\u0026gt;%\rtop_n(10)\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(zip_code,species,Year,Month) %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rsubset(zip_code %in% c(a$zip_code)) %\u0026gt;%\rggplot(.,aes(x=species,y=zip_code,color=Month))+geom_jitter()+\rtransition_states(Year,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+\rxlab(\u0026quot;Species\u0026quot;)+ylab(\u0026quot;Zip Code\u0026quot;)+\rggtitle(\u0026quot;Top 10 Zipcodes vs Species of Choice \u0026quot;,\rsubtitle = \u0026quot;Year : {closest_state}\u0026quot;)\r\rZipcode with choices of Primary Breed\rb\u0026lt;-seattle_pets %\u0026gt;%\rcount(primary_breed) %\u0026gt;%\rtop_n(10)\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(zip_code,primary_breed,Year,Month) %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rsubset(zip_code %in% c(a$zip_code)) %\u0026gt;%\rsubset(primary_breed %in% c(b$primary_breed)) %\u0026gt;%\rggplot(.,aes(x=str_wrap(primary_breed,10),y=zip_code,color=Month))+\rgeom_jitter()+\rtransition_states(Year,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+\rxlab(\u0026quot;Primary Breed\u0026quot;)+ylab(\u0026quot;Zip Code\u0026quot;)+\rggtitle(\u0026quot;Top 10 Zipcode and Primary Breed of Choice \u0026quot;,\rsubtitle = \u0026quot;Year : {closest_state}\u0026quot;)\r\rZipcode with choices of Animals Name\rd\u0026lt;-seattle_pets %\u0026gt;%\rcount(animals_name) %\u0026gt;%\rtop_n(10)\re\u0026lt;-seattle_pets %\u0026gt;%\rcount(zip_code) %\u0026gt;%\rtop_n(15)\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(zip_code,animals_name,Year,Month) %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rsubset(zip_code %in% c(e$zip_code)) %\u0026gt;%\rsubset(animals_name %in% c(d$animals_name)) %\u0026gt;%\rggplot(.,aes(x=animals_name,y=zip_code,color=Month))+geom_jitter()+\rtransition_states(Year,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+\rxlab(\u0026quot;Animals Name\u0026quot;)+ylab(\u0026quot;Zip Code\u0026quot;)+\rggtitle(\u0026quot;Top 10 Zipcode and Top 15 Animals Name of Choice \u0026quot;,\rsubtitle = \u0026quot;Year : {closest_state}\u0026quot;)\r\r\rPrimary and Secondary Breed Choices Over the Years\ra\u0026lt;-seattle_pets %\u0026gt;%\rcount(primary_breed) %\u0026gt;%\rtop_n(10)\rb\u0026lt;-seattle_pets %\u0026gt;%\rcount(secondary_breed) %\u0026gt;%\rtop_n(10)\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(primary_breed,secondary_breed,Year,Month) %\u0026gt;%\rremove_missing() %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rsubset(primary_breed %in% c(a$primary_breed)) %\u0026gt;%\rsubset(secondary_breed %in% c(b$secondary_breed)) %\u0026gt;%\rggplot(.,aes(x=str_wrap(primary_breed,12),y=str_wrap(secondary_breed,12),\rcolor=Month))+\rgeom_jitter()+xlab(\u0026quot;Primary Breed\u0026quot;)+ylab(\u0026quot;Second Breed\u0026quot;)+\rtransition_states(Year,transition_length = 2,state_length = 3)+\renter_fade() + exit_shrink() +ease_aes(\u0026quot;back-in\u0026quot;)+\r#theme(axis.text.x = element_text(angle = 270))+\rggtitle(\u0026quot;Top 10 Primary and Secondary Breeds\u0026quot;, subtitle = \u0026quot;Year : {closest_state}\u0026quot;)\r\rSpecies and Name choices for Animals\rb\u0026lt;-seattle_pets %\u0026gt;%\rcount(animals_name) %\u0026gt;%\rtop_n(15)\rseattle_pets %\u0026gt;%\rcSplit(\u0026quot;license_issue_date\u0026quot;,sep = \u0026quot; \u0026quot;) %\u0026gt;%\rrename(Month =license_issue_date_1) %\u0026gt;%\rrename(Day = license_issue_date_2) %\u0026gt;%\rrename(Year = license_issue_date_3) %\u0026gt;%\rselect(species,animals_name,Year,Month) %\u0026gt;%\rremove_missing() %\u0026gt;%\rmutate(Month = as.integer(Month)) %\u0026gt;%\rsubset(animals_name %in% c(b$animals_name)) %\u0026gt;%\rggplot(.,aes(y=animals_name,x=species,color=Month))+geom_jitter()+\rtransition_states(Year,transition_length = 2,state_length = 3)+\renter_fade() + exit_shrink() +ease_aes(\u0026quot;back-in\u0026quot;)+\rylab(\u0026quot;Animals Name\u0026quot;)+xlab(\u0026quot;Species\u0026quot;)+\rggtitle(\u0026quot;Animals Name and Species\u0026quot;, subtitle = \u0026quot;Year : {closest_state}\u0026quot;)\rTHANK YOU\n\r","date":1553644800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553644800,"objectID":"f565d939d0a24bd541f56803b30b60b7","permalink":"/post/tidytuesday2019/week13/week13/","publishdate":"2019-03-27T00:00:00Z","relpermalink":"/post/tidytuesday2019/week13/week13/","section":"post","summary":"2019 Week 13 TidyTuesday: Pets of Seattle ","tags":["TidyTuesday","tidyverse","gganimate"],"title":"Week 13 : Pets In Seattle","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rMinimum No of Players and Minimum Play Time\rGeneral\rScrutinized\rMore Scrutinized\r\rMaximum No of Players and Maximmum Play Time\rGeneral\rScrutinized\rMore Scrutinized\r\rMaximum No of Players and Average Rating\rGeneral\rScrutinized\rMore Scrutinized\r\rAverage Rating and Users Rated\rAnimated\rNot Animated\r\rCategory and Rating\rGeneral and Category 1\rGeneral and Category 2\r\r\r\r# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(tidylog)\rlibrary(gganimate)\rlibrary(splitstackshape)\rlibrary(ggthemr)\r# load the theme\rggthemr(\u0026quot;chalk\u0026quot;)\r# load the data\rboard_games \u0026lt;- read_csv(\u0026quot;board_games.csv\u0026quot;)\rBoard Games data-set contains a lot of variables yet i will be focusing on only a few of them here. They are category, year_published, average_rating, users_rated, max_players, max_playtime, min_players and min_playtime. Mostly the plots will identify patterns how over the years variables change with the help of jitter/point plots. Slightly above 10,000 observations are in this data-set\n#TidyTuesday In the mid-2010s, card games as the first category are higher!!!!!!!!!!. Code: https://t.co/1e5zotWApC pic.twitter.com/QebdFPAJuE\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) March 12, 2019  GitHub Code\nMinimum No of Players and Minimum Play Time\rIf we consider minimum no of players and minimum play time we can see how the data points are spread out. In order to understand clearly we have changed the limits of x-axis and y-axis twice.\nGeneral\rp\u0026lt;-ggplot(board_games,aes(min_players,min_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Minimum No of Players\u0026quot;)+\rylab(\u0026quot;Minimum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\rscale_y_continuous(breaks = seq(0,60000,5000),labels=seq(0,60000,5000))+\rggtitle(\u0026quot;Minimum No of Players vs Minimum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rScrutinized\rp\u0026lt;-subset(board_games, min_playtime \u0026lt; 5000) %\u0026gt;%\rggplot(.,aes(min_players,min_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Minimum No of Players\u0026quot;)+\rylab(\u0026quot;Minimum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\rscale_y_continuous(breaks = seq(0,5000,500),labels=seq(0,5000,500))+\rggtitle(\u0026quot;Minimum No of Players vs Minimum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rMore Scrutinized\rp\u0026lt;-subset(board_games, min_playtime \u0026lt; 500) %\u0026gt;%\rggplot(.,aes(min_players,min_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Minimum No of Players\u0026quot;)+\rylab(\u0026quot;Minimum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\rscale_y_continuous(breaks = seq(0,500,50),labels=seq(0,500,50))+\rggtitle(\u0026quot;Minimum No of Players vs Minimum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\r\rMaximum No of Players and Maximmum Play Time\rSimilarly, we are generating three plots to understand how Maximum No of Players and Maximum Play Time is behaving. Below are the three plots.\nGeneral\rp\u0026lt;-ggplot(board_games,aes(max_players,max_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Maximum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,1000,50),labels = seq(0,1000,50))+\rscale_y_continuous(breaks = seq(0,60000,5000),labels=seq(0,60000,5000))+\rggtitle(\u0026quot;Maximum No of Players vs Maximum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rScrutinized\rp\u0026lt;-subset(board_games,max_players\u0026lt; 125 \u0026amp; max_playtime \u0026lt; 10000) %\u0026gt;%\rggplot(.,aes(max_players,max_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Maximum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,100,10),labels = seq(0,100,10))+\rscale_y_continuous(breaks = seq(0,6000,500),labels=seq(0,6000,500))+\rggtitle(\u0026quot;Maximum No of Players vs Maximum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rMore Scrutinized\rp\u0026lt;-subset(board_games,max_players\u0026lt; 15 \u0026amp; max_playtime \u0026lt; 1000) %\u0026gt;%\rggplot(.,aes(max_players,max_playtime))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Maximum Playing Time\u0026quot;)+\rscale_x_continuous(breaks=seq(0,15),labels = seq(0,15))+\rscale_y_continuous(breaks = seq(0,1000,50),labels=seq(0,1000,50))+\rggtitle(\u0026quot;Maximum No of Players vs Maximum Playing Time\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\r\rMaximum No of Players and Average Rating\rNext is plotting the variables Maximum No of Players and Average Rating. Where over the time period from 1950 to 2016 the data is plotted here. There is three stages of plotting here also. Below is the code and plots.\nGeneral\rp\u0026lt;-ggplot(board_games,aes(max_players,average_rating))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Average Rating\u0026quot;)+\rscale_x_continuous(breaks=seq(0,1000,50),labels = seq(0,1000,50))+\rscale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\rggtitle(\u0026quot;Maximum No of Players vs Average Rating\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rScrutinized\rp\u0026lt;-subset(board_games,max_players \u0026lt;75 ) %\u0026gt;%\rggplot(.,aes(max_players,average_rating))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Average Rating\u0026quot;)+\rscale_x_continuous(breaks=seq(0,75,5),labels = seq(0,75,5))+\rscale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\rggtitle(\u0026quot;Maximum No of Players vs Average Rating\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rMore Scrutinized\rp\u0026lt;-subset(board_games,max_players \u0026lt;15 ) %\u0026gt;%\rggplot(.,aes(max_players,average_rating))+\rgeom_jitter()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Maximum No of Players\u0026quot;)+\rylab(\u0026quot;Average Rating\u0026quot;)+\rscale_x_continuous(breaks=seq(0,15),labels = seq(0,15))+\rscale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\rggtitle(\u0026quot;Maximum No of Players vs Average Rating\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\r\rAverage Rating and Users Rated\rNow I am focusing on Average Rating with Users Rated, where the former is in a scale from 1 to 10 and the latter is just count.\nAnimated\rp\u0026lt;-ggplot(board_games,aes(average_rating,users_rated))+\rgeom_point()+transition_time(year_published)+\rease_aes(\u0026quot;linear\u0026quot;)+\rylab(\u0026quot;Users Rated\u0026quot;)+\rxlab(\u0026quot;Average Rating\u0026quot;)+\rscale_x_continuous(breaks=seq(0,10,.5),labels = seq(0,10,.5))+\rscale_y_continuous(breaks = seq(0,70000,2500),labels=seq(0,70000,2500))+\rggtitle(\u0026quot;Average Rating vs Users Rated\u0026quot;,\rsubtitle =\u0026quot;Year :{frame_time}\u0026quot; )\ranimate(p,nframes=67,fps=1)\r\rNot Animated\rggplot(board_games,aes(average_rating,users_rated,color=year_published))+\rgeom_point(alpha=0.85)+\rlabs(color=\u0026quot;Year\u0026quot;)+\rylab(\u0026quot;Users Rated\u0026quot;)+\rxlab(\u0026quot;Average Rating\u0026quot;)+\rscale_x_continuous(breaks=seq(0,10,.5),labels = seq(0,10,.5))+\rscale_y_continuous(breaks = seq(0,70000,2500),labels=seq(0,70000,2500))+\rggtitle(\u0026quot;Average Rating vs Users Rated\u0026quot;,\rsubtitle =\u0026quot;Year : 1950 to 2016\u0026quot; )\r\r\rCategory and Rating\rFinally, there are two plots which focuses on board games which has average rating above 7 with category. Even though a game can have multiple categories here I have chosen the first four while ignoring the others. Because most of the games do not have a third category it would be pointless in my perspective.\nGeneral and Category 1\rp\u0026lt;-board_games %\u0026gt;%\rselect(year_published,category,average_rating) %\u0026gt;%\rcSplit(\u0026quot;category\u0026quot;,sep=\u0026quot;,\u0026quot;) %\u0026gt;%\rgather(Groups,category,\u0026quot;category_01\u0026quot;,\u0026quot;category_02\u0026quot;,\u0026quot;category_03\u0026quot;,\u0026quot;category_04\u0026quot;) %\u0026gt;%\rselect(year_published,Groups,category,average_rating) %\u0026gt;%\rsubset(Groups==\u0026quot;category_01\u0026quot; \u0026amp; average_rating \u0026gt; 7)%\u0026gt;%\rggplot(.,aes(category,average_rating))+geom_jitter()+\rcoord_flip()+xlab(\u0026quot;Category\u0026quot;)+ylab(\u0026quot;Average Rating\u0026quot;)+\rtransition_time(year_published)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Average Rating for First Category\u0026quot;,\rsubtitle = \u0026quot;Year: {floor(frame_time)}\u0026quot;)\ranimate(p,fps=1,nframes=54)\r\rGeneral and Category 2\rp\u0026lt;-board_games %\u0026gt;%\rselect(year_published,category,average_rating) %\u0026gt;%\rcSplit(\u0026quot;category\u0026quot;,sep=\u0026quot;,\u0026quot;) %\u0026gt;%\rgather(Groups,category,\u0026quot;category_01\u0026quot;,\u0026quot;category_02\u0026quot;,\u0026quot;category_03\u0026quot;,\u0026quot;category_04\u0026quot;) %\u0026gt;%\rselect(year_published,Groups,category,average_rating) %\u0026gt;%\rsubset(Groups==\u0026quot;category_02\u0026quot; \u0026amp; average_rating \u0026gt;7)%\u0026gt;%\rggplot(.,aes(category,average_rating))+geom_jitter()+\rcoord_flip()+xlab(\u0026quot;Category\u0026quot;)+ylab(\u0026quot;Average Rating\u0026quot;)+\rtransition_time(year_published)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Average Rating for Second Category\u0026quot;,\rsubtitle = \u0026quot;Year: {floor(frame_time)}\u0026quot;)\ranimate(p,fps=1,nframes=54)\rTHANK YOU\n\r\r","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552348800,"objectID":"c7ac7d5fd1311e85935b093be2a7b134","permalink":"/post/tidytuesday2019/week11/week11/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/post/tidytuesday2019/week11/week11/","section":"post","summary":"2019 Week 11 TidyTuesday: Board Games","tags":["TidyTuesday","tidyverse","gganimate"],"title":"Week 11 : Board Games","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rEarnings Female\rAll other age groups\r\rEmployed Gender\rComparing Full Time with Part Time\rMale Occupants with Full Time and Part Time Work\rFemale Occupants with Full Time and Part Time Work\r\rJobs Gender\rMajor Category\rMajor Category and Total Workers\rMajor Category and Male Workers\rMajor Category and Female Workers\rMajor Category and Total Earnings Male Wage\rMajor Category and Total Earnings Female Wage\rMajor Category and Wage Percent for Female relative to Male\r\rMinor Category\rMinor Category and Total Workers\rMinor Category and Male Workers\rMinor Category and Female Workers\rMinor Category and Total Earnings Male Wage\rMinor Category and Total Earnings Female Wage\rMinor Category and Wage Percent for Female relative to Male\r\r\r\r\rGitHub Code\nMinor Categories and Female wage with relative to Male is above 75% over in the year 2016 but less in other years of 2013,2014 and 2015 for some categories. #TidyTuesday Code: https://t.co/8Ck8babjy1 pic.twitter.com/VLMkhozWay\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) March 6, 2019  #load the packages\rlibrary(tidyverse)\rlibrary(gganimate)\rlibrary(readr)\rlibrary(ggthemr)\r# load the theme\rggthemr(\u0026quot;fresh\u0026quot;)\r# load the data\rearnings_female \u0026lt;- read_csv(\u0026quot;earnings_female.csv\u0026quot;)\remployed_gender \u0026lt;- read_csv(\u0026quot;employed_gender.csv\u0026quot;)\rjobs_gender \u0026lt;- read_csv(\u0026quot;jobs_gender.csv\u0026quot;)\rEarnings Female\rAll other age groups\raverage\u0026lt;-earnings_female %\u0026gt;%\rfilter(group==\u0026quot;Total, 16 years and older\u0026quot;) %\u0026gt;%\rmutate(year=cut(Year,breaks = c(1978,1989,1999,2011),\rlabels =c(\u0026quot;1980s\u0026quot;,\u0026quot;1990s\u0026quot;,\u0026quot;2000s\u0026quot;) ) ) %\u0026gt;%\rgroup_by(year) %\u0026gt;%\rsummarize(Mean=mean(percent))\rearnings_female %\u0026gt;%\rfilter(group!=\u0026quot;Total, 16 years and older\u0026quot;) %\u0026gt;%\rggplot(.,aes(Year,percent,color=group))+\rgeom_point()+geom_line()+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rgeom_hline(yintercept = average$Mean,color=c(\u0026quot;red\u0026quot;,\u0026quot;maroon\u0026quot;,\u0026quot;brown\u0026quot;),size=1.2)+\rannotate(\u0026quot;text\u0026quot;,x=2010,y=68,label=\u0026quot;1980s Average\u0026quot;)+\rannotate(\u0026quot;text\u0026quot;,x=1983,y=76,label=\u0026quot;1990s Average\u0026quot;)+\rannotate(\u0026quot;text\u0026quot;,x=2008,y=80.5,label=\u0026quot;2000s Average\u0026quot;)\r\r\rEmployed Gender\rComparing Full Time with Part Time\remployed_gender %\u0026gt;%\rselect(year,total_full_time,total_part_time) %\u0026gt;%\rgather(Type,percent,c(total_full_time,total_part_time)) %\u0026gt;%\rggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\rgeom_col()+theme(legend.position = \u0026quot;bottom\u0026quot;)+\rgeom_text(nudge_y = -.75,color=\u0026quot;white\u0026quot;)+xlab(\u0026quot;Year\u0026quot;)+\rylab(\u0026quot;Percentage\u0026quot;)+\rggtitle(\u0026quot;Total Work Force Full Time and Part Time\u0026quot;)\r\rMale Occupants with Full Time and Part Time Work\remployed_gender %\u0026gt;%\rselect(year,full_time_male,part_time_male) %\u0026gt;%\rgather(Type,percent,c(full_time_male,part_time_male)) %\u0026gt;%\rggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\rgeom_col()+theme(legend.position = \u0026quot;bottom\u0026quot;)+\rgeom_text(nudge_y = -.75,color=\u0026quot;white\u0026quot;)+xlab(\u0026quot;Year\u0026quot;)+\rylab(\u0026quot;Percentage\u0026quot;)+\rggtitle(\u0026quot;Male Work Force Full Time and Part Time\u0026quot;)\r\rFemale Occupants with Full Time and Part Time Work\remployed_gender %\u0026gt;%\rselect(year,full_time_female,part_time_female) %\u0026gt;%\rgather(Type,percent,c(full_time_female,part_time_female)) %\u0026gt;%\rggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\rgeom_col()+theme(legend.position = \u0026quot;bottom\u0026quot;)+\rgeom_text(nudge_y = -.75,color=\u0026quot;white\u0026quot;)+xlab(\u0026quot;Year\u0026quot;)+\rylab(\u0026quot;Percentage\u0026quot;)+\rggtitle(\u0026quot;Female Work Force Full Time and Part Time\u0026quot;)\r\r\rJobs Gender\rMajor Category\rMajor Category and Total Workers\rjg_avg\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rgroup_by(year) %\u0026gt;%\rsummarize_if(is.numeric,funs(mean),na.rm=TRUE)\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),total_workers,label=round(total_workers,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+ ggtitle(\u0026quot;Total Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Total Workers\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\rMajor Category and Male Workers\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),workers_male,label=round(workers_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+ ggtitle(\u0026quot;Male Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Male Workers\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\rMajor Category and Female Workers\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),workers_female,label=round(workers_female,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+ ggtitle(\u0026quot;Female Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Female Workers\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\rMajor Category and Total Earnings Male Wage\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),total_earnings_male,label=round(total_earnings_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;) +\rggtitle(\u0026quot;Total Earnings Male Wage changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Total Earnings Male\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\rMajor Category and Total Earnings Female Wage\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),total_earnings_female,label=round(total_earnings_female)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Total Earnings Female Wage changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Total Earnings Female\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\rMajor Category and Wage Percent for Female relative to Male\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;minor_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,major_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(major_category,12),wage_percent_of_male,\rlabel=round(wage_percent_of_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Wage Percent of Female relative to Male changing Over time\u0026quot;,\rsubtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Major Category\u0026quot;)+ylab(\u0026quot;Relative Percentage\u0026quot;)+geom_text(vjust=-1)\ranimate(p,nframes=4,fps=1)\r\r\rMinor Category\rMinor Category and Total Workers\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,20),total_workers,label=round(total_workers,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rggtitle(\u0026quot;Total Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Total Workers\u0026quot;)+geom_text(hjust=\u0026quot;inward\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rMinor Category and Male Workers\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,20),workers_male,label=round(workers_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rggtitle(\u0026quot;Male Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Male Workers\u0026quot;)+geom_text(hjust=\u0026quot;inward\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rMinor Category and Female Workers\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,20),workers_female,label=round(workers_female,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rggtitle(\u0026quot;Female Workers changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Female Workers\u0026quot;)+geom_text(hjust=\u0026quot;inward\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rMinor Category and Total Earnings Male Wage\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,20),total_earnings_male,label=round(total_earnings_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rggtitle(\u0026quot;Total Earnings Male Wage changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Total earnings Male\u0026quot;)+geom_text(hjust=\u0026quot;inward\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rMinor Category and Total Earnings Female Wage\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,20),total_earnings_female,label=round(total_earnings_female,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rggtitle(\u0026quot;Total Earnings Female Wage changing Over time\u0026quot;,subtitle = \u0026quot;Year :{frame_time}\u0026quot;)+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Total earnings Female\u0026quot;)+geom_text(hjust=\u0026quot;inward\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rMinor Category and Wage Percent for Female relative to Male\rp\u0026lt;-jobs_gender %\u0026gt;%\rselect(-one_of(c(\u0026quot;occupation\u0026quot;,\u0026quot;major_category\u0026quot;))) %\u0026gt;%\rgroup_by(year,minor_category) %\u0026gt;%\rsummarise_all(funs(mean),na.rm=TRUE) %\u0026gt;%\rggplot(.,aes(str_wrap(minor_category,18),wage_percent_of_male,\rlabel=round(wage_percent_of_male,2)))+\rgeom_col()+transition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Wage Percent of Female relative to Male changing Over time\u0026quot;,\rsubtitle = \u0026quot;Year :{frame_time}\u0026quot;)+coord_flip()+\rxlab(\u0026quot;Minor Category\u0026quot;)+ylab(\u0026quot;Relative Percentage\u0026quot;)+geom_text(hjust=1)\ranimate(p,nframes=4,fps=1)\rTHANK YOU\n\r\r\r","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"ff517e8817dcf0023a5375ea3c390f99","permalink":"/post/tidytuesday2019/week10/week10/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/tidytuesday2019/week10/week10/","section":"post","summary":"2019 Week 10 TidyTuesday: Women in Workforce","tags":["tidyverse","TidyTuesday","gganimate"],"title":"Week 10: Women in Workforce","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(gganimate)\rlibrary(ggalluvial)\rlibrary(geomnet)\rlibrary(ggthemr)\r# load the theme\rggthemr(\u0026quot;fresh\u0026quot;)\r# load the data\rsmall_trains \u0026lt;- read_csv(\u0026quot;small_trains.csv\u0026quot;)\rGitHub Code\nData set\nWhen Journey Average Time increases the Total Number of Trips will decrease. Obviously. Code : https://t.co/qY2l10OraS #TidyTuesday pic.twitter.com/ZZn0l7E7WZ\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) February 28, 2019  Network Graph for the French City Trains\rSimply drawing a network graph to understand which french cities are mainly urban with capacity to trains arriving and leaving. Cities such as Paris Lyon, Paris Montparnasse, Paris Nord and Paris Est could be cities of concern with much for traffic with related to trains.\nggplot(small_trains,aes(from_id=departure_station,to_id=arrival_station))+\rgeom_net(directed = TRUE,labelon = TRUE,size=0.5,labelcolour = \u0026quot;black\u0026quot;,\rrepel = FALSE,ecolour = \u0026quot;grey70\u0026quot;, arrowsize = 0.75,\rlinewidth = 0.5,layout.alg = \u0026quot;fruchtermanreingold\u0026quot;)+\rtheme_net()+\rggtitle(\u0026quot;Network Graph Showing from City to City of French Trains\u0026quot;)\r\rParis Montparnasse\rLet me focus on Montparnasse which has lot of trains coming towards and leaving outwards according to the network map. Not all are cities of France according to my observations, where I can see Madrid, Zurich and Barcelona.\nChosen City with Total Number of Trips\rTotal number of trips from Paris Montparnasse station to other cities is noted here. Four years of data with more accuracy by months is considered in this plot. There is clear variation in this data for cities.\nClose to 800 trips have been recorded towards the Bordeaux St Jean city but not clearly the same pattern for all years or months as well. Further, St Malo city has the lowest amount of trips compared to other cities in all fours but follows a centered pattern around the 100 mark.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y=total_num_trips,color=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rscale_y_continuous(breaks = seq(0,800,100),labels=seq(0,800,100))+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Total Number of Trips\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Stations\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rChosen City with Average Journey Time\rExcept year 2017 all cities has similar and centered data points. In this exceptional year of 2017 we can see a difference between the first six months and rest. Where most of the Average journey times have been reduced, it is clear according to year 2018 points.\nCity of Toulouse Matabiau has the highest Average Journey Time, while lowest time goes to the city of Le Mans. So what happened after mid of year 2017.\nMaximum Average Journey time before mid of year 2017 is close to 325 but after this period it is centered around 275. The Minimum Average Journey time before mid of year 2017 and after also it is close to 50.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y=journey_time_avg,color=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rscale_y_continuous(breaks=seq(0,350,25),labels=seq(0,350,25))+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Average Journey Time\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Station\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rChosen City with Average Delay with All Departing\rThere is no clear pattern in perspective of months or years because data points are spread all over the place. Yet there is an odd occurring of negative values for average delay with all departing for some cities after mid of year 2017.\nWell none of these negative values does not exceed -2.5, where the maximum average delay in all departing is close to 5.5. I believe unit measured is in minutes.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y= avg_delay_all_departing,color=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rscale_y_continuous(breaks=seq(-2.5,5.5,0.5),labels=seq(-2.5,5.5,0.5))+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rgeom_hline(yintercept = 0,color=\u0026quot;red\u0026quot;)+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Average Delay All Departing\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Station\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rChosen City with Average Delay with All Arriving\rHighest delay could occur close to 18 minutes for average delay in Arriving and the lowest is close to -3. only in 2018 we see such negative values. These negative values occurs for the city of St Malo. Also there is no clear pattern in any city with relative to year or months.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y=avg_delay_all_arriving,color=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rgeom_hline(yintercept = 0,color=\u0026quot;red\u0026quot;)+\rscale_y_continuous(breaks=seq(-3,18),labels=seq(-3,18))+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Average Delay All Arriving\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Station\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rChosen City with Number of Late Departures\rNumber of Late departures over the years increases for all cities. It is more clear for Bordeaux St Jean where the counts go beyond 150 and close to 200 in the year of 2018, but in 2015 the highest point is close to 75 for the same city.\nSt Malo has the lowest number of late departures where it fails to reach the count of 30 in all four years.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y=num_late_at_departure,\rcolor=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rscale_y_continuous(breaks=seq(0,200,25),labels=seq(0,200,25))+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Number of Lates at Departure\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Station\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\rChosen City with Number of Late Arrivals\rCity of St Malo has the lowest amount of late arrivals for all fours in general. Most amount of highest late arrivals occur in the city of Bordeaux St Jean. In year 2015 most of these data points are centered towards their specific values. In the next few years we can see that is not the case and they are with a lot of variation.\np\u0026lt;-ggplot(subset(small_trains,departure_station==\u0026quot;PARIS MONTPARNASSE\u0026quot;),\raes(x=str_wrap(arrival_station,20),y=num_arriving_late,\rcolor=month))+\rgeom_jitter()+coord_flip()+ labs(color=\u0026quot;Month\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rscale_y_continuous(breaks=seq(0,200,20),labels=seq(0,200,20))+\rxlab(\u0026quot;Arrival Station\u0026quot;)+ylab(\u0026quot;Number of Lates at Arriving\u0026quot;)+\rggtitle(\u0026quot;Paris Montparnasse and its arrival Station\u0026quot; ,subtitle =\u0026quot;Year :{frame_time}\u0026quot;)\ranimate(p,nframes=4,fps=1)\r\r\rDeparture Station with Average Journey Time and Total Number of Trips\rSummary of this below plot is that when Average Journey Time increases clearly Total Number of Trips will decrease.\np\u0026lt;-ggplot(small_trains,aes(x=journey_time_avg,y=total_num_trips,color=month))+\rgeom_point()+transition_states(departure_station)+labs(color=\u0026quot;Month\u0026quot;)+\rggtitle(\u0026quot;Average Journey Time and Total Number of Trips\u0026quot;,\rsubtitle=\u0026quot;Departure Station : {closest_state}\u0026quot;)+\rscale_y_continuous(breaks=seq(0,900,50),labels=seq(0,900,50))+\rscale_x_continuous(breaks=seq(0,500,50),labels=seq(0,500,50))+ xlab(\u0026quot;Average Journey Time\u0026quot;)+ylab(\u0026quot;Total Number of Trips\u0026quot;)+\rshadow_mark()\ranimate(p,nframes=59,fps=1)\r\rDeparture Station with Average Delay All Departing and Number of Late at Departures\rp\u0026lt;-ggplot(small_trains,aes(x=num_late_at_departure,y=avg_delay_all_departing,\rcolor=month))+\rgeom_point()+transition_states(departure_station)+labs(color=\u0026quot;Month\u0026quot;)+\rggtitle(\u0026quot;Average Delay at All Departing and Number of Lates at Departure\u0026quot;,\rsubtitle=\u0026quot;Departure Station : {closest_state}\u0026quot;)+\rgeom_vline(xintercept = 0,color=\u0026quot;red\u0026quot;)+\rgeom_hline(yintercept = 0,color=\u0026quot;red\u0026quot;)+\rscale_y_continuous(breaks=seq(-5,175,5),labels=seq(-5,175,5))+\rscale_x_continuous(breaks=seq(0,500,50),labels=seq(0,500,50))+ xlab(\u0026quot;Number of Lates at Departure\u0026quot;)+ylab(\u0026quot;Average Delays at all Departing\u0026quot;)+\rshadow_mark()\ranimate(p,nframes=59,fps=1)\r\rDeparture Station with Average Delay All Arriving and Number of Arriving Late\rp\u0026lt;-ggplot(small_trains,aes(x=num_arriving_late,y=avg_delay_all_arriving,\rcolor=month))+\rgeom_point()+transition_states(departure_station)+labs(color=\u0026quot;Month\u0026quot;)+\rggtitle(\u0026quot;Average Delay at All Arriving and Number of Lates at Arriving\u0026quot;,\rsubtitle=\u0026quot;Departure Station : {closest_state}\u0026quot;)+\rgeom_vline(xintercept = 0,color=\u0026quot;red\u0026quot;)+\rgeom_hline(yintercept = 0,color=\u0026quot;red\u0026quot;)+\rscale_y_continuous(breaks=seq(-150,40,5),labels=seq(-150,40,5))+\rscale_x_continuous(breaks=seq(0,250,25),labels=seq(0,250,25))+ xlab(\u0026quot;Number of Lates at Arriving\u0026quot;)+ylab(\u0026quot;Average Delays at all Arriving\u0026quot;)+\rshadow_mark()\ranimate(p,nframes=59,fps=1)\r\rDelayed Cause and Delayed Number\rDelays caused by the travelers is very less likely to happen, where most of these delays are caused by external causes. Other causes such as rolling stock and rail infrastructure also has effect but not as much from external cause. Station management has limited amount of affect but higher than travelers in causes for delaying trains.\nsmall_trains %\u0026gt;%\rmutate(delay_cause = str_remove(delay_cause,\u0026quot;delay_cause_\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=delay_cause,y=delayed_number))+\rxlab(\u0026quot;Delay Cause\u0026quot;)+ylab(\u0026quot;Delayed Number\u0026quot;)+\rggtitle(\u0026quot;Delayed Causes and Delayed Number as percentage\u0026quot;)+\rgeom_jitter()+coord_flip()\rTHANK YOU\n\r","date":1551312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551312000,"objectID":"dee32378efa956238c4d7c3dc2b129a3","permalink":"/post/tidytuesday2019/week9/week9/","publishdate":"2019-02-28T00:00:00Z","relpermalink":"/post/tidytuesday2019/week9/week9/","section":"post","summary":"2019 Week 9 TidyTuesday: Train Delays in French Land","tags":["tidyverse","TidyTuesday","gganimate"],"title":"Week 9 : French Train Delays","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rBroad Field\rAll fields\rDropping Psychology and Social Sciences\r\rMajor Field\rMajor Fields with Box plot\rMajor Fields without Psychology but still in a Boxplot\r\rMathematics and Computer Sciences\rMathematics and Computer Science as a Broad field\rMathematics and Computer Science as a Major Field\rMajor Field of Mathematics and Computer Science but now all Fields\r\rMajor Field, Field and Year For Mathematics and Computer Sciences\r\r\r# load the packages\rlibrary(tidyverse)\rlibrary(ggthemr)\rlibrary(readr)\rlibrary(gganimate)\rlibrary(ggridges)\rlibrary(ggalluvial)\rggthemr(\u0026quot;flat\u0026quot;)\r#load the data\rphdlist \u0026lt;- read_csv(\u0026quot;phd_by_field.csv\u0026quot;)\rFive variables are representing this entire data-set and three of them are factors while one column represents the year and the final column is for counts. There are few missing values. We can focus on Phds awarded from 2008 to 2017 in perspective of Broad Field, Major Field and Field.\nDataset\nGitHub Code\nComputer science PhDs more than others is that the boom of AI. beginning from 2008 itself !!!!!. My first ridge plot and alluvial diagram.\nCode: https://t.co/TtWLzNk1ga #tidytuesday pic.twitter.com/AhB01IPuv6\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) February 19, 2019  Broad Field and Major Field are considered specially but not the column Field as it would be difficult to plot based on the amount of categories.\nBroad Field\rBroad field has 7 categories and clearly Psychology and social sciences has produced more than 4000 Phds each and every year. Which is twice comparing to other categories. If we drop Psychology and Social sciences, now the changes over the years for other categories are clear.\nThere are more outliers in the field of Life sciences where some programs produce more than 1000 Phds each year comparatively to the rest categories. Except Life sciences other categories tend to behave rarely as above producing more than 1000 Phds.\nEngineering field has the lowest distribution with relative to other categories according to the box plot in every year.\nAll fields\rp\u0026lt;-ggplot(phdlist,aes(x=str_wrap(broad_field,20),y=n_phds))+\rgeom_boxplot()+\rxlab(\u0026quot;Broad Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Broad Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(axis.text.x = element_text(hjust=1,angle = 90))\ranimate(p,nframes=9,fps=1)\r\rDropping Psychology and Social Sciences\rp\u0026lt;-ggplot(subset(phdlist,broad_field != \u0026quot;Psychology and social sciences\u0026quot;),\raes(x=str_wrap(broad_field,20),y=n_phds))+\rgeom_boxplot()+\rxlab(\u0026quot;Broad Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Broad Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(axis.text.x = element_text(hjust=1,angle = 90))\ranimate(p,nframes=9,fps=1)\r\r\rMajor Field\rFocus now solely switches towards the Major Field column and here also we can see the strong outlier in Psychology over the years. Further, Physics and Astronomy field also has a very strong outlier where over the years it reaches 2000 Phds.\nWithout dropping Psychology we can see the odd behavior from the fields “Education Research”, “Economics” and “Computer and Information Sciences”. Specially the gradual decrease of “Education Research” from 2008 to 2017.\nAlso in “Computer and Information Sciences” field there is an odd increase in 2012.\nAfter dropping the “Psychology” field we can now clearly see how other Major fields behave over the years.\nMajor Fields with Box plot\rp\u0026lt;-ggplot(phdlist,aes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+\rgeom_boxplot()+coord_flip()+\rxlab(\u0026quot;Major Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Major Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(axis.text.x = element_text(hjust=1,angle = 90),\rlegend.position = \u0026quot;bottom\u0026quot;)+\rlabs(fill=\u0026quot;Broad Field\u0026quot;)\ranimate(p,nframes=9,fps=1)\r\rMajor Fields without Psychology but still in a Boxplot\rq\u0026lt;-ggplot(subset(phdlist,major_field != \u0026quot;Psychology\u0026quot;),\raes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+\rgeom_boxplot()+coord_flip()+\rxlab(\u0026quot;Major Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Major Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(axis.text.x = element_text(hjust=1,angle = 90),\rlegend.position = \u0026quot;bottom\u0026quot;)+\rlabs(fill=\u0026quot;Broad Field\u0026quot;)\ranimate(q,nframes=9,fps=1)\r\r\rMathematics and Computer Sciences\rI am a Statistics student with a glimpse of Computer science background, therefore my next intention is to focus on the Broad Field “Mathematics and Compute Sciences”.\nMathematics and Computer Science as a Broad field\rMathematics and Statistics has a gradual increase until 2012, but wavers higher and lower in the next years, but in 2016 there is a sudden increase of which would lead to around 700 Phds awarded. Next year this decreases to 500 Phds.\nComparing the 2 major fields “Computer and Information Sciences” with “Mathematics and Statistics” indicate the strong gap between them awarding Phds. “Computer and Information Sciences” award more than twice the amount of Phds what “Mathematics and Statistics” award each year.\n“Computer and Information Sciences” also hold a clear pattern with the Phds awarded.\nsubset(phdlist,broad_field == \u0026quot;Mathematics and computer sciences\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=factor(year),y=n_phds,fill=major_field))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position = \u0026quot;dodge\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rxlab(\u0026quot;Major Field\u0026quot;)+ylab(\u0026quot;Number of Phds\u0026quot;)+\rggtitle(\u0026quot;Number of Phds awarded under Mathematics and CS\u0026quot;,\rsubtitle = \u0026quot;Year : 2008 to 2017\u0026quot;)+\rscale_y_continuous(breaks=seq(0,1700,100),labels=seq(0,1700,100))+\rlabs(fill=\u0026quot;Major Field\u0026quot;)\r\rMathematics and Computer Science as a Major Field\rBox plot indicates the clear variation among these two major fields over years which could be used for comparison. The sudden peak in year 2012 for “Computer and Information Sciences” interests me alot. It should be noted that “Mathematics and Statistics” has more outliers than “Computer and Information Sciences”.\np\u0026lt;-ggplot(subset(phdlist,broad_field == \u0026quot;Mathematics and computer sciences\u0026quot;),\raes(x=str_wrap(major_field,20),y=n_phds))+\rgeom_boxplot()+\rxlab(\u0026quot;Major Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Major Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)\ranimate(p,nframes=9,fps=1)\rBelow is a ridge plot to describe the same thing which would clearly indicate the data spread.\nggplot(subset(phdlist,broad_field == \u0026quot;Mathematics and computer sciences\u0026quot;),\raes(y=str_wrap(major_field,20),x=n_phds))+\rgeom_density_ridges()+\rxlab(\u0026quot;No of Phds\u0026quot;)+ ylab(\u0026quot;Major Field\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rggtitle(\u0026quot;Ridge plot for Major Fields in Mathematics and Computer Sciences\u0026quot;,\rsubtitle = \u0026quot;Year : 2008 to 2017\u0026quot;)\r\rMajor Field of Mathematics and Computer Science but now all Fields\rConsidering the sub categories of the chosen broad field in a box plot did not work quite well, but clearly this plot indicates the Computer Science Phds being awarded with highest amount over the years. Would that mean the boom of Artificial Intelligence in Computer Science.\np\u0026lt;-ggplot(subset(phdlist,broad_field == \u0026quot;Mathematics and computer sciences\u0026quot;),\raes(x=str_wrap(field,20),y=n_phds,fill=major_field))+\rgeom_boxplot()+coord_flip()+\rxlab(\u0026quot;Field\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Boxplot to Number of Phds in Field\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rlabs(fill=\u0026quot;Major Field\u0026quot;)\ranimate(p,nframes=9,fps=1)\rTo get a clear view here is the ridge plot, where Computer Science is very strong for “Computer and Information Sciences”. It should be noted though there is only three other fields in this major field which are “Information Science systems”, “Computer and Information Sciences, other” and “Computer and Information sciences, general”.\nMore than 10 fields for the Major field “Mathematics and Statistics”, where higher counts occur to “Statistics(Mathematics)”, “Applied mathematics” and “Mathematics and Statistics,general”. Still non of these fields have passed the 1000 Phds awarded mark.\nggplot(subset(phdlist,broad_field == \u0026quot;Mathematics and computer sciences\u0026quot;),\raes(y=str_wrap(field,20),x=n_phds,fill=major_field))+\rgeom_density_ridges()+\rxlab(\u0026quot;No of Phds\u0026quot;)+ ylab(\u0026quot;Field\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rggtitle(\u0026quot;Ridge plot for Fields in Mathematics and Computer Sciences\u0026quot;,\rsubtitle = \u0026quot;Year : 2008 to 2017\u0026quot;)+\rlabs(fill=\u0026quot;Major Field\u0026quot;)\r\r\rMajor Field, Field and Year For Mathematics and Computer Sciences\rFinally an alluvial diagram just to look at the impact of Computer science field with respective to each year, which is very strong.\ndata.frame(subset(phdlist,broad_field==\u0026quot;Mathematics and computer sciences\u0026quot;)) %\u0026gt;%\rna.omit() %\u0026gt;%\rggplot(.,aes(axis2=factor(str_wrap(year,10)), axis1= factor(str_wrap(major_field,10)), axis3= factor(field), y=as.numeric(n_phds)))+\rscale_x_discrete(limits=c(\u0026quot;Major Field\u0026quot;,\u0026quot;Year\u0026quot;,\u0026quot;Field\u0026quot;),expand = c(.05, .05))+\rgeom_alluvium(aes(fill=factor(major_field)),width = 1/2)+\rgeom_stratum(width=1/2,fill=\u0026quot;white\u0026quot;,color=\u0026quot;grey\u0026quot;)+\rgeom_text(stat = \u0026quot;stratum\u0026quot;, label.strata = TRUE)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+ylab(\u0026quot;No of Phds\u0026quot;)+\rggtitle(\u0026quot;Major Field and Fields For Years 2008 to 2017\u0026quot;,\rsubtitle=\u0026quot;Mathematics and Computer Science\u0026quot;)+\rlabs(fill=\u0026quot;Major Field\u0026quot;)\rTHANK YOU\n\r","date":1550534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550534400,"objectID":"149854f213b6ef2de72eea11576a09c6","permalink":"/post/tidytuesday2019/week8/week8/","publishdate":"2019-02-19T00:00:00Z","relpermalink":"/post/tidytuesday2019/week8/week8/","section":"post","summary":"2019 Week 8 TidyTuesday: Phds Awarded in USA between 2008 and 2017","tags":["tidyverse","TidyTuesday","gganimate"],"title":"Week 8 : Phds Awarded in USA between 2008 and 2017","type":"post"},{"authors":null,"categories":["Blog","R","SL","Journal"],"content":"\rIntroduction\rPackages and Ideas\rCountry Information\rRanking of Sri Lanka from 1996 to 2017\rRanking of South Asian Region Countries from 1996 to 2017\rDocuments , Citable Documents and Self Citations from Sri Lanka Journals\rCitations per Document from Sri Lankan Journals\r\rJournals from Sri Lanka\rRank changes for Sri Lankan Journals\rSJR values changing for Sri Lankan Journals\rReferences per Document changing for Sri Lankan Journals\rPublishers and Journals from Sri Lanka\rTotal Documents and References for Journals from Sri Lanka\rThree year time period for Journals from Sri Lanka\rJournals and categories\r\r\r\rIntroduction\r“SCImago Journal and Country Rank provides valuable estimates of academic journals’ prestige.”, this is according to its GitHub repository which could be found here. I completely agree with this statement, last year(2018) while searching for journals to publish a research of my interest and contribution this website came in useful comparative to most of my Google searches.\nIt is crucial to find a journal which summarizes the research work we intend to contribute with more relative information such as ranking, h-index, citations and etc which could be compared for our benefit. Usually people tend to collect information by themselves and might miss a valuable and more suitable opportunity to publish because they did not know where to find them. That will not occur if you use SciMagoJr.\n\rPackages and Ideas\rAs a Researcher from Sri Lanka I am much more interested in how we as a country has made progress in related to journals or research publishing in perspective of our neighboring countries and fields of research interest.\nThe package sjrdata has three data-sets and they provide information regarding to journals which is mostly seeked by researchers to find an appropriate place to publish their research or find places which would benefit them in finding a unique research problem.\n# Data\rlibrary(sjrdata)\r# Packages\rlibrary(tidyverse)\rlibrary(gganimate)\rlibrary(ggrepel)\rlibrary(magrittr)\rlibrary(lubridate)\rlibrary(splitstackshape)\rlibrary(qdap)\rlibrary(ggthemr)\rggthemr(\u0026quot;flat dark\u0026quot;)\rThe three data-sets in concern are\nsjr_countries - Contains information regarding h_index over the years for countries and their ranks with other information related to documents.\n\rsjr_journals - Records for type of document with their basic information which can be connected to country of origin, publisher and fields of interest and much more.\n\rsjr_countries_1996_2017 - Similar to sjr_countries but this is an accumulation of the data-set which would indicate the overall performance as a country from 1996 to 2017.\n\r\r\rCountry Information\rThis section will simply scrutinize how Sri Lanka over the years has performed and it should be noted that only the ranks of South Asian Region countries will be compared for the purpose of understanding the progress as a whole community.\nRanking of Sri Lanka from 1996 to 2017\rSri Lanka(SL) had its ups and downs in the rankings where the worst place was to be in 85th and the best place to be in 75th. As of in 2017 it has reached 78th which is good considering the previous three years(2014-2016) the ranks were higher.\nIn 1996 SL was holding 85th and this occurs again in the years 2011 and 2014. Also it reached the 75th ranking position in year 2007. I sincerely hope that in the coming years SL would reach better ranks.\nsubset(sjr_countries,country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=year,y=rank,label=rank))+\rgeom_point()+geom_line()+\rgeom_text_repel()+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rank\u0026quot;)+\rggtitle(\u0026quot;Rank of Sri Lanka Changing from 1996 to 2017\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 45, hjust = 1))\r\rRanking of South Asian Region Countries from 1996 to 2017\rIndia is way ahead of other seven countries in the South Asian region where it has gradually moved from 13th position to 5th over the years. Sri Lanka is currently behind India, Pakistan and Nepal.\nFurther Bhutan, Maldives and Afghanistan have ranks above 130 for the period of time given, while Maldives has stayed in the range of 213 and 167. Afghanistan and Bhutan have made improvements where now(2017) they are holding ranks of 136 and 159 respectively while in 1996 they had ranks of 207 and 186. These three countries tend to oscillate very frequently among ranks than the other 5 countries in the region.\nsubset(sjr_countries,\rcountry ==\u0026quot;Sri Lanka\u0026quot; | country==\u0026quot;India\u0026quot;| country==\u0026quot;Pakistan\u0026quot;|\rcountry ==\u0026quot;Nepal\u0026quot; | country== \u0026quot;Maldives\u0026quot; | country ==\u0026quot;Afghanistan\u0026quot;|\rcountry == \u0026quot;Bhutan\u0026quot; | country==\u0026quot;Bangladesh\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=year,y=rank,label=rank,color=country))+\rgeom_point()+geom_line()+\rgeom_text_repel()+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rank\u0026quot;)+\rggtitle(\u0026quot;Rank of South Asian Region Changing from 1996 to 2017\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 45, hjust = 1))\r\rDocuments , Citable Documents and Self Citations from Sri Lanka Journals\rOver the years Self citations were higher than citable documents and documents in counts, but this is not the case after the year 2012. Self citations reached its highest peak of 1331 in 2012 but in the next few years they gradually decreased to 175 counts in year 2017. Even though in 1996 the self citations count was 275 which is higher than current counts in 2017 for self citation.\nThis is not the case for citable documents and documents where they start with counts of close to 200. Over the years they rapidly increase and reach the close to thousand mark 2012. This form of improvement continues further and reaches the close to two thousand mark in the year 2017, which I feel is impressive. The gap between documents and citable documents is very thin in the early 2000s but while counts increase the gap also considerably increases.\nsjr_countries%\u0026gt;%\rgather(citing,values,\u0026quot;documents\u0026quot;,\u0026quot;citable_documents\u0026quot;,\u0026quot;self_citations\u0026quot;) %\u0026gt;%\rsubset(country==\u0026quot;Sri Lanka\u0026quot;,select=c(citing,values,year)) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=year,y=values,label=values,color=citing,shape=citing))+\rgeom_point()+geom_line()+\rgeom_text_repel()+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Counts\u0026quot;)+\rscale_x_continuous(breaks=seq(1996,2017),labels=seq(1996,2017))+\rscale_y_continuous(breaks=seq(0,2000,100),labels=seq(0,2000,100))+\rggtitle(\u0026quot;Document Counts Changing from 1996 to 2017 for Sri Lanka\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))\r\rCitations per Document from Sri Lankan Journals\rCitations per document has a steady rise from 14.28 to 24.29 in the years of 1996 to 2000, while there is a sudden decline next year it manages to increases and reach the peak point of 29.29 in year 2003. After the millenium we can clearly see a clear steep in numbers over the next two decades, where now in year 2017 the citations per document has dropped to 0.61. It is very alarming even though we have alot of citable documents in the same year according to the previous plot.\nsjr_countries %\u0026gt;%\rsubset(country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=year,y=citations_per_document,\rlabel=citations_per_document))+\rgeom_point()+geom_line()+\rgeom_text_repel()+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Citations Per Document\u0026quot;)+\rscale_x_continuous(breaks=seq(1996,2017),labels=seq(1996,2017))+\rscale_y_continuous(breaks=seq(0,30),labels=seq(0,30))+\rggtitle(\u0026quot;Citations Per Document from 1996 to 2017 for Sri Lanka\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))\r\r\rJournals from Sri Lanka\r9 titles are listed and considered here from Sri Lanka where some began publishing recently.\nRank changes for Sri Lankan Journals\rJournals which started close to year 2000 have ranks close to ten thousand, while recent publications has ranks leading upto the rank of higher than thirty thousand as well. Journal of the National Science Foundation of Sri Lanka and Ceylon Medical Journal has published more than other publications and they do have better rankings through out the years.\np\u0026lt;-subset(sjr_journals,country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=rank,color=year))+\rgeom_jitter()+coord_flip()+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Title of Journals\u0026quot;)+ylab(\u0026quot;Rank\u0026quot;)+\rggtitle(\u0026quot;Ranks Changing over time for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)\ranimate(p,fps=1,nframes=18) \r\rSJR values changing for Sri Lankan Journals\rp\u0026lt;-subset(sjr_journals,country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=sjr,color=year,shape=sjr_best_quartile))+\rgeom_jitter()+coord_flip()+\rshadow_mark()+\rlabs(shape=\u0026quot;SJR Best Q\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Title of Journals\u0026quot;)+ylab(\u0026quot;SJR values\u0026quot;)+\rggtitle(\u0026quot;SJR values Changing over time for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\ranimate(p,fps=1,nframes=18) \r\rReferences per Document changing for Sri Lankan Journals\rHighest amount of reference per document is close to 100 while the minimum is close to zero. There is no specific pattern over the years which indicate an increase or decrease.\np\u0026lt;-subset(sjr_journals,country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=ref_doc,color=year))+\rgeom_jitter()+coord_flip()+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Title of Journals\u0026quot;)+ylab(\u0026quot;References per Document\u0026quot;)+\rggtitle(\u0026quot;References per Document Changing over time for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)\ranimate(p,fps=1,nframes=18) \r\rPublishers and Journals from Sri Lanka\rWithout any hesitation I could say most of these publications are from the field of medicine and institutions related to medicine. There are some publishers such as Internations Centre for Ethnic Studies and International Irrigation Managament Institue, but they have published very few publications.\np\u0026lt;-subset(sjr_journals,country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=str_wrap(factor(publisher),30),color=year))+\rgeom_jitter()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rshadow_mark()+\rxlab(\u0026quot;Title\u0026quot;)+ylab(\u0026quot;Publisher\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Title and Publishers changing over time for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)\ranimate(p,fps=1,nframes=18) \r\rTotal Documents and References for Journals from Sri Lanka\rThe Gaps between Total documents per year and Total references are compared here. Obviously in most of the cases Total documents per year is very low considering to Total references. There are anomalies such as Ceylon Medical Journal but only in the earlier publications.\np\u0026lt;-sjr_journals %\u0026gt;%\rgather(Group,values,\u0026quot;total_docs_year\u0026quot;,\u0026quot;total_refs\u0026quot;) %\u0026gt;%\rsubset(country==\u0026quot;Sri Lanka\u0026quot;,select=c(Group,values,year,title)) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=values,fill=Group,group=Group))+\rgeom_col(position = \u0026quot;dodge\u0026quot;)+\rcoord_flip()+\rxlab(\u0026quot;Title\u0026quot;)+ylab(\u0026quot;Count\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Counts changing for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(legend.position=\u0026quot;bottom\u0026quot;)\ranimate(p,nframes=18,fps=1)\r\rThree year time period for Journals from Sri Lanka\rp\u0026lt;-sjr_journals %\u0026gt;%\rgather(Group,values,\u0026quot;total_docs_3years\u0026quot;,\u0026quot;total_cites_3years\u0026quot;,\u0026quot;citable_docs_3years\u0026quot;) %\u0026gt;%\rsubset(country==\u0026quot;Sri Lanka\u0026quot;,select=c(Group,values,year,title)) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=values,fill=Group,group=Group))+\rgeom_col(position=\u0026quot;dodge\u0026quot;)+coord_flip()+\rxlab(\u0026quot;Title\u0026quot;)+ylab(\u0026quot;Count for 3 years\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rggtitle(\u0026quot;Counts changing for Sri Lankan Journals from 1999 to 2017\u0026quot;,\rsubtitle = \u0026quot;Year : {round(frame_time)}\u0026quot;)+\rtheme(legend.position=\u0026quot;bottom\u0026quot;)\ranimate(p,nframes=18,fps=1)\r\rJournals and categories\rp\u0026lt;-sjr_journals %\u0026gt;%\rsubset(country==\u0026quot;Sri Lanka\u0026quot;) %\u0026gt;%\rselect(year,title,categories) %\u0026gt;%\rcSplit(\u0026quot;categories\u0026quot;,sep=\u0026quot;;\u0026quot;) %\u0026gt;%\rgather(Groups,Categories,\u0026quot;categories_1\u0026quot;,\u0026quot;categories_2\u0026quot;,\u0026quot;categories_3\u0026quot;,\u0026quot;categories_4\u0026quot;) %\u0026gt;%\rmutate(Categories=genX(Categories,\u0026quot;(Q\u0026quot;,\u0026quot;)\u0026quot;)) %\u0026gt;%\rmutate(year=year(as.Date(year,\u0026quot;%Y\u0026quot;))) %\u0026gt;%\rggplot(.,aes(x=str_wrap(factor(title),30),y=str_wrap(Categories,30),shape=Groups,color=year)) +\rgeom_jitter()+ theme(axis.text.x =element_text(angle = 90, hjust = 1),\rlegend.position=\u0026quot;bottom\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Title\u0026quot;)+ ylab(\u0026quot;Categories\u0026quot;)+shadow_mark()+\rggtitle(\u0026quot;Title and Categories between the years 1999 to 2017\u0026quot;,\rsubtitle=\u0026quot;Year :{round(frame_time)}\u0026quot;)\ranimate(p,nframes=18,fps=1) \rTHANK YOU\n\r\r","date":1550448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550448000,"objectID":"9e60370d37596fa7c340998142b3b722","permalink":"/post/slandjournal/slandjournal/","publishdate":"2019-02-18T00:00:00Z","relpermalink":"/post/slandjournal/slandjournal/","section":"post","summary":"Exploring How has Sri Lanka done in related to Academic Journals.","tags":["gganimate","tidyverse","R package","R","SL"],"title":"Sri Lanka and its affect on/in Journal Articles ","type":"post"},{"authors":null,"categories":["Binomial","Blog","fitODBOD"],"content":"\r\r\r\r\r\r\rBinomial Distribution\rApplications\r\rBinomial Mixture Distributions\rMain Groups for Binomial Distribution\rOther With No Sub-Groups\rGrassia Binomial Distribution\rZero Modified Binomial Distribution\rDandekar’s Modified Binomial Distribution\rSimplex Binomial Mixture Model\rDouble Binomial Distribution\rFinite Biomial Mixtures\rBinomial Distribution of order K\rTruncated Binomial Distribution\rWeighted Binomial Distribution\r\rBinomial Not Parent\rHypergeometric + Binomial\rNegative Binomial + Binomial\rPoisson + Binomial\r\rAlternate Binomial Distributions\rAdditive Binomial Distribution\rBeta-Correlated Binomial Distribution\rCOM-Poisson Binomial Distribution\rCorrelated Binomial Distribution\rMultiplicative Binomial Distribution\r\rNeyman Type A Distribution\rPoisson + Binomial + Beta\rPoisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta\r\rHermite Distribution\rBinomial + Poisson\rPoisson + Binomial\r\rBinomial Parent\rN/n , K and Y Mixtures\rp Transformed Binomial\rLog Inverse Distribution [0,1] Domain\rCumulative Distribution Function\rp Binomial\r\r\rReferences\r\r\rBinomial Distribution\rThe binomial distribution can be defined, using the binomial expansion\n\\[ (q+p)^n = \\sum_{x=0}^{n} {n \\choose k} p^k q^{(n-k)} = \\sum_{x=0}^{n} \\frac{n!} {k! (n-k)!} p^k q^{(n-k)}\\]\nas the distribution of a random variable X for which\n\\[Pr[X=x] = {n \\choose k} p^k q^{(n-k)}\\]\n\\(x=0,1,2,...,n.\\) where \\(q+p=1\\), \\(p,q\u0026gt;0\\) and \\(n\\) is a positive integer. When \\(n=1\\), the distribution is known as the Bernoulli distribution. The mean and variance are \\(\\mu=np\\) and \\(\\mu_2 = npq\\).\nIf \\(n\\) independent trials are made and in each there is probability \\(p\\) that the outcome \\(E\\) will occur, then the number of trials in which \\(E\\) occurs can be represented by a random variable \\(X\\) having the binomial distribution with parameters \\(n\\), \\(p\\).\nThis situation occurs when a sample of fixed size \\(n\\) is taken from an infinite population where each element in the population has an “equal” and “independent” probability \\(p\\) of possession of a specified attribute. The situation also arises when a sample of fixed size \\(n\\) is taken from a infinite population where each element in the population has an “equal” and “independent” probability \\(p\\) of having a specified attribute and elements are sampled independently and sequentially with replacement.\nThe distribution was derived by James Bernoulli (in his treatise Ars Conjectandi, published in 1713), for the case \\(p = r/(r+s)\\), where \\(r\\) and \\(s\\) are positive integers.\nThe binomial distribution is of such importance in applied probability and statistics that it is frequently necessary to calculate probabilities based on this distribution. Although the calculation of sums of the form\n\\[ \\sum_{x} {n \\choose x} p^x q^{(n-x)}\\]\nis straightforward, it can be tedious, especially when n and x are large and when there are a large number of terms in the summation. It is not surprising that a great deal of attention and ingenuity have been applied to constructing useful approximations for sums of this kind.\nApplications\rThe binomial distribution arises whenever underlying events have two possible outcomes, the chances of which remain constant. The importance of the distribution has extended from its original application in gaming to many other areas.\n\rIts use in genetics arises because the inheritance of biological characteristics depends on genes that occur in pairs; see, for example, Fisher and Matheras (1936) analysis of data on straight versus wavy hair in mice.\n\rMore recent application in genetics is the study of the number of nucleotides that are in the same state in two DNA sequences (Kaplan and Risko, 1982).\n\rThe number of defectives found in random samples of size n from a stable production process is a binomial variable; acceptance sampling is a very important application of the test for the mean of a binomial sample against a hypothetical value.\n\rSeber (1982b) has given a number of instances of the use of the binomial distribution in animal ecology, for example, in mark-recapture estimation of the size of an animal population. Boswell, Ord, and Patil (1979) gave applications in plant ecology.\n\r\rAlthough appealing in their simplicity, the assumptions of independence and constant probability for the binomial distribution are not often precisely satisfied. Published critical appraisals of the extent of departure from these assumptions in actual situations are rather rare.\n\r\rBinomial Mixture Distributions\rThe binomial distribution has two parameters, \\(n\\) and \\(p\\), and either or both of these may be supposed to have a probability distribution. We will not discuss cases in which both \\(n\\) and \\(p\\) vary, though it is easy to construct such examples.\nIn most cases discussed in the statistical literature, \\(p\\) has a continuous distribution, while \\(n\\) is discrete. The latter restriction is necessary, but the former is not.\nHowever, discrete distributions for \\(p\\) have not been found to be useful and have not attracted much attention from a theoretical point of view.\n\rMain Groups for Binomial Distribution\rMore than 50 distributions are in this blog post but I shall not explain them even briefly. This blog post is to introduce them and let you know the reader about these distributions with article references.\nIn this section I introduce the main groups, where the number of groups are 6. Further, the Binomial Parent group has a very broad approach. “Binomial Parent” means, Where Binomial Distribution is the Parent distribution and by mixing them with other distributions we can produce a new distribution which would satisfy real data but not fully satisfy the condition of Binomial Distribution.\nClearly with the vast number of distributions and sub groups it is not possible to draw them in one diagram, therefore I have divided them in sub groups and sub topics in the following sections.\n\r{\"x\":{\"diagram\":\"graph TB;\\n A[Binomial Distribution]--B[Binomial Not Parent];\\n A--D[Other with no sub groups];\\n A--C[Binomial Parent];\\n A--M[Alternate Binomial Distribution];\\n A--N[Neyman Type A Distribution];\\n A--O[Hermite Distribution];\\n C--CA[Mixing Parameter];\\n CA--CAA[N/n Binomial];\\n CA--CAB[K Binomial];\\n CA--CAC[Y Binomial];\\n CA--CAE[p transformed Binomial];\\n CA--CAF[Cumulative Distribution Function];\\n CA--CAD[p Binomial];\\n CA--CAG[Log Inverse Distribution 0,1 Domain]\\n \"},\"evals\":[],\"jsHooks\":[]}\rOther With No Sub-Groups\rFirst sub group in discussion is “Other With No Sub-Groups”, which is an abbreviation for Binomial distributions which were direcly created for the purpose of satisfying specific real world situations and theoretical concepts.\n\r{\"x\":{\"diagram\":\"graph TB;\\n B[Binomial Distribution]--A[Other with no sub groups];\\n A--D[Grassia Binomial Distribution];\\n A--E[Zero Modified Binomial Distribution];\\n A--F[Dandekar's Modified Binomial Distribution];\\n A--G[Simplex Binomial Mixture Model];\\n A--H[Double Binomial Distribution];\\n A--I[Finite Binomial Mixtures];\\n A--J[Binomial Distribution of order K];\\n A--K[Truncated Binomial Distribution];\\n A--L[Weighted Binomial Distribution];\\n \"},\"evals\":[],\"jsHooks\":[]}\rBelow is the list of Distributions in this sub group with article references.\nGrassia Binomial Distribution\rKemp, A. W., and Kemp, C. D (2004). Factorial moment characterizations for certain binomial-type distributions, Communications in Statistics-Theory and Methods, 33, 3059-3068.\nHarkness, W. L. (1970). The classical occupancy problem revisited, Random Counts in Scientific Work, Vol. 3:Random Counts in Physical Science, Geo Science, and Business, G. P. Patil (editor), 107-126. University Park: Pennsylvania State University Press.\nWeiss, G. H. (1965). A model for the spread of epidemics by carriers, Biometrics, 21, 481-490.\nDietz, K. (1966). On the model of Weiss for the spread of epidemics by carriers, Journal of Applied Probability, 3, 375-382.\nDownton, F. (1967). Epidemics with carriers: A note on a paper by Dietz, Journal of Applied Probability, 4, 264-270.\nDaley, D. J., and Gani, J. (1999). Epidemic Modelling:An Introduction,Cambridge: Cambridge University Press.\nGrassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformation of the gamma and derived compound distributions, Australian Journal of Statistics, 19, 108-114.\nAlanko, T., and Duffy, J. C. (1996). Compound binomial distributions for modelling consumption data, The Statistician, 45, 269-286.\nChatfield, C., and Goodhardt, G. J. (1970). The beta-binomial model for consumer purchasing behaviour, Applied Statistics, 19, 240-250.\nConsul, P. C., and Jain, G. C. (1971). On the log-gamma distribution and its properties, Statistische Hefte, 12, 100-106.\n\rZero Modified Binomial Distribution\rDowling, M. M., and Nakamura, M. (1997). Estimating parameters for discrete distributions via the empirical probability generating function, Communications in Statistics-Simulation and Computation, 26, 301-313.\nKhatri, C. G. (1961). On the distributions obtained by varying the number of trials in a binomial distribution, Annals of the Institute of Statistical Mathematics, Tokyo, 13, 47-51.\n\rDandekar’s Modified Binomial Distribution\rPatil, G. P., Boswell, M. T., Joshi, S. W., and Ratnaparkhi, M. V. (1984). Dictionary and Bibliography of Statistical Distributions in Scientific Work, Vol. 1: Discrete Models, Fairland, MD: International Co-operative Publishing House.\nDandekar, V. M. (1955). Certain modified forms of binomial and Poisson distributions, Sankhya, 15, 237-250.\n\rSimplex Binomial Mixture Model\rBarndorff-Neilsen, O. E., and Jorgensen, B. (1991). Some parametric models on the simplex, Journal of Multivariate Analysis, 39, 106-116.\nJorgensen, B. (1997). The Theory of Regression Models, London: Chapman \u0026amp; Hall/CRC.\n\rDouble Binomial Distribution\rLindsey, J. K. (1995). Modelling Frequency and Count Data, Oxford: Oxford University Press.\nEfron, B. (1986). Double exponential families and their use in generalized linear regression, Journal of the American Statistical Association, 81, 709-721.\n\rFinite Biomial Mixtures\rTeicher, H. (1961). Identifiability of mixtures, Annals of Mathematical Statistics, 32, 244-248.\nBlischke, W. R. (1962). Moment estimation for the parameters of a mixture of two binomial distributions, Annals of Mathematical Statistics, 33, 444-454.\nBlischke, W. R. (1964). Estimating the parameters of mixtures of binomial distributions, Journal of the American Statistical Association, 59, 510-528.\nBlischke, W. R. (1965). Mixtures of discrete distributions, Classical and Contagious Discrete Distributions, G. P. Patil (editor), 351-372. Calcutta: Statistical Publishing Society; Oxford: Pergamon.\nEveritt, B. S., and Hand, D. J. (1981). Finite Mixture Distributions, London: Chapman \u0026amp; Hall.\nBondesson, L. (1988). On the gain by spreading seeds: A statistical analysis of sowing experiments, Scandinavian Journal of Forest Research, 305-314.\nGelfand, A. E., and Soloman, H. (1975). Analysing the decision making process of the American jury, Journal of the American Statistical Association, 70, 305-310.\nHasselblad, V. (1969). Estimation of finite mixtures of distributions from the exponential family, Journal of the American Statistical Association, 64, 1459-1471.\nRider, P. R. (1962a). Estimating the parameters of mixed Poisson, binomial and Weibull distributions, Bulletin of the International Statistical Institute, 39(2), 225-232.\n\rBinomial Distribution of order K\rLing, K. D. (1988). On binomial distributions of order k, Statistics and Probability Letters, 6, 371-376.\nShanthikumar, J. G. (1985). Discrete random variate generation using uniformization, European Journal of Operational Research, 21, 387-398.\nChiang, D., and Niu, S. C. (1981). Reliability of consecutive-k-out-of-n:F systems, IEEE Transactions on Reliability, R-30, 87-89.\nBollinger, R. C., and Salvia, A. A. (1982). Consecutive-k-out-of-n: F networks, IEEE Transactions on Reliability, R-31, 53-55.\nHirano, K. (1986). Some properties of the distributions of order k, Fibonacci Numbers and Their Applications, A. N. Philippou, G. E. Bergum, and A. F. Horadam (editors), 43-53. Dordrecht: Reidel.\nPhilippou, A. N., and Makri, F. S. (1986). Success runs and longest runs, Statistics and Probability Letters, 4, 101-105 (corrected version 211-215).\nFeller, W. (1957). An Introduction to Probability Theory and Its Applications (second edition), Vol. 1, New York: Wiley.\nAki, S., and Hirano, K. (1988). Some characteristics of the binomial distribution of order k and related distributions, Statistical Theory and Data Analysis, Vol. 2, K. Matusita (editor), 211-222. Amsterdam: Elsevier.\n\rTruncated Binomial Distribution\rStephan, F. F. (1945). The expected value and variance of the reciprocal and other negative powers of a positive Bernoullian variate, Annals of Mathematical Statistics, 16, 50-61.\nShah, S. M. (1966). On estimating the parameter of a doubly truncated binomial distribution, Journal of the American Statistical Association, 61, 259-263.\nNewell, D. J. (1965). Unusual frequency distributions, Biometrics, 21, 159-168.\nGrab, E. L., and Savage, I. R. (1954). Tables of the expected value of 1/x for positive Bernoulli and Poisson variables, Journal of the American Statistical Association, 49, 169-177.\nFinney, D. J. (1949). The truncated binomial distribution, Annals of Eugenics, London, 14, 319-328.\n\rWeighted Binomial Distribution\rPatil, G. P., Rao, C. R., and Zelen, M. (1986). A Computerized Bibliography of Weighted Distributions and Related Weighted Methods for Statistical Analysis and Interpretations of Encountered Data, Observational Studies, Representativeness Issues, and Resulting Inferences, University Park, PA: Centre for Statistical Ecology and Environmental Statistics, Pennsylvania State University.\nPatil, G. P., Rao, C. R., and Ratnaparkhi, M. V. (1986). On discrete weighted distributions and their use in model choice for observed data, Communications in Statistics-Theory and Methods, 15, 907-918.\nRao, C. R. (1965). On discrete distributions arising out of methods of ascertainment, Classical and Contagious Discrete Distributions,G. P Patil (editor), 320-332. Calcutta: Statistical Publishing Society; Oxford: Pergamon. (Republished Sankhya, A27, 1965, 311-324.)\nRao, C. R. (1985). Weighted distributions arising out of methods of ascertainment: What populations does a sample represent?, A Celebration of Statistics: ISI Centenary Volume, A. C. Atkinson and S. E. Fienberg (editors), 543-569. New York: SpringerVerlag.\n\r\rBinomial Not Parent\rThis is the sub group where Binomial Distribution is not the parent but rather some other different distribution like Hypergeometric or Negative Binomial or Poisson.\n\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[Binomial Not Parent];\\n B--BA[Hypergeometric + Binomial];\\n B--BB[Negative Binomial + Binomial];\\n B--BC[Poisson + Binomial];\\n \"},\"evals\":[],\"jsHooks\":[]}\rHypergeometric + Binomial\r\\[Hypergeometric (n,Y,N) \\bigwedge_Y Binomial(N,p)\\]\n\rNegative Binomial + Binomial\r\\[Negative Binomial (kY,P) \\bigwedge_Y Binomial(n,p)\\]\n\rPoisson + Binomial\r\\[ Poisson (\\theta) \\bigwedge_{\\theta/\\phi} Binomial(n,p)\\]\n\r\rAlternate Binomial Distributions\rThis is a sub group of distributions which can be used as an alternative to Binomial Distribution.\n\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[Alternate Binomial Distribution];\\n B--C[Additive Binomial Distribution];\\n B--D[Beta-Correlated Binomial Distribution];\\n B--E[COM-Poisson Binomial Distribution];\\n B--F[Correlated Binomial Distribution];\\n B--G[Multiplicative Binomial Distribution];\\n \"},\"evals\":[],\"jsHooks\":[]}\rAdditive Binomial Distribution\rJohnson, N. L., Kemp, A. W., \u0026amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\n\rBeta-Correlated Binomial Distribution\rPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\n\rCOM-Poisson Binomial Distribution\rBorges, P., Rodrigues, J., Balakrishnan, N. and Bazan, J., 2014. A COM-Poisson type generalization of the binomial distribution and its properties and applications. Statistics \u0026amp; Probability Letters, 87, pp.158-166.\n\rCorrelated Binomial Distribution\rJohnson, N. L., Kemp, A. W., \u0026amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\nJorge G. Morel and Nagaraj K. Neerchal. Overdispersion Models in SAS. SAS Institute, 2012.\n\rMultiplicative Binomial Distribution\rJohnson, N. L., Kemp, A. W., \u0026amp; Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\n\r\rNeyman Type A Distribution\r\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[ Neyman Type A Distribution];\\n B--C[Poisson + Binomial + Beta];\\n B--D[Poisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta];\\n B--E[Poisson + Binomial + Poisson + Beta or Binomial + Poisson + Poisson + Beta];\\n \"},\"evals\":[],\"jsHooks\":[]}\rPoisson + Binomial + Beta\r\\[Poisson(\\phi) \\bigvee Binomial(1,P) \\bigwedge_P Beta(\\alpha,\\beta)\\]\n\rPoisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta\r\\[ Poisson(\\lambda) \\bigvee [\\{Poisson(\\phi) \\vee Binomial(1,P) \\} \\bigwedge_P Beta(\\alpha,\\beta)]\\] or \\[ Poisson(\\lambda) \\bigvee [\\{Binomial(M,P) \\bigvee_M Poisson(\\phi) \\} \\bigwedge_P Beta(\\alpha,\\beta)]\\] ### Poisson + Binomial + Poisson + Beta or Binomial + Poisson + Poisson + Beta\n\\[[\\{ Poisson(\\lambda) \\bigvee Binomial(1,P)\\} \\bigvee Poisson(\\phi)] \\bigwedge_P Beta(a,b)\\] or\n\\[[\\{ Binomial(N,P) \\bigvee Poisson(\\phi)\\} \\bigwedge_N Poisson(\\lambda)] \\bigwedge_P Beta(a,b)\\]\nGurland, J. (1958). A generalized class of contagious distributions, Biometrics, 14, 229-249.\nFeller, W. (1943). On a general class of “contagious” distributions, Annals of Mathematical Statistics, 14, 389-400.\nNeyman, J. (1939). On a new class of “contagious” distributions applicable in entomology and bacteriology, Annals of Mathematical Statistics, 10, 35-57.\nSubrahmaniam, Kocherlakota (1966). On a general class of contagious distributions: The Pascal-Poisson distribution, Trabajos de Estadistica, 17, 109-127.\nSubrahmaniam, Kathleen (1978). The Pascal-Poisson distribution revisited: Estimation and efficiency, Communications in Statistics-Theory and Methods, A7, 673-683.\n\r\rHermite Distribution\r\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[Hermite Distribution];\\n B--C[Binomial + Poisson];\\n B--D[Poisson + Binomial];\\n \"},\"evals\":[],\"jsHooks\":[]}\rBinomial + Poisson\r\\[Binomial(N,p) \\bigwedge_{N/2} Poisson(\\lambda)\\]\n\rPoisson + Binomial\r\\[Poisson(\\lambda) \\bigvee Binomial(2,p)\\]\nSkellam, J. G. (1952). Studies in statistical ecology I: Spatial pattern, Biometrika, 39, 346-362.\nMcGuire, J. U., Brindley, T. A., and Bancroft, T. A. (1957). The distribution of European corn borer Pyrausta Nubilalis (Hbn.) in field corn, Biometrics, 13, 65-78 [errata and extensions (1958) 14, 432-434].\nKemp, C. D., and Kemp, A. W. (1965). Some properties of the “Hermite” distribution, Biometrika, 52, 381-394.\nFisher, R. A. (1951). Properties of the functions, (Part of introduction to) British Association Mathematical Tables (third edition), Vol. 1, London: British Association.\n\r\rBinomial Parent\rThis is the sub group where Binomial distribution is the parent distribution. By considering the distribution and its parameters we can use possible different mixing distributions and generate new Binomial Mixture Distributions.\nN/n , K and Y Mixtures\rThe first mixtures with few distributions are mentioned below.\n\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[Binomial Parent];\\n B--C[Mixing Parameter];\\n C--D[N/n Binomial];\\n D--DA[Poisson];\\n D--DB[Binomial];\\n D--DC[Negative Binomial];\\n D--DD[Logarithmic];\\n C--E[K Binomial];\\n E--EA[Poisson];\\n C--F[Y Binomial];\\n F--FA[Hypergeometric];\\n \"},\"evals\":[],\"jsHooks\":[]}\rPoisson Mix for N/n of Binomial distributon\r\\[Binomial(N,p) \\bigwedge_{N/n} Poisson(\\lambda)\\]\n\rNegative Binomial Mix for N/n of Binomial distributon\r\\[Binomial(N,p) \\bigwedge_{N/n} Negative Binomial(k,P`)\\]\n\rBinomial Mix for N/n of Binomial distributon\r\\[Binomial(N,p) \\bigwedge_{N/n} Binomial(N`,p`)\\]\n\rLogarithmic Mix for N/n of Binomial distribution\r\\[Binomial(N,p) \\bigwedge_{N/n} Logarithmic(\\theta)\\]\n\rPoisson Mix for K of Poisson distributon\r\\[Binomial(nK,p) \\bigwedge_K Poisson(\\theta)\\]\n\rHypergeometric Mix for Y of Binomial distributon\r\\[Binomial(m,\\frac{Y}{n}) \\bigwedge_{Y} Hypergeometric(n,Np,N)\\]\n\r\rp Transformed Binomial\r\r{\"x\":{\"diagram\":\"\\n graph LR;\\n A[Binomial Distribution]--B[Binomial Parent];\\n B--C[Mixing Parameter];\\n C--D[p transformed Binomial];\\n D--E[p = 1 - exp_-t_ ];\\n E--EA[Binomial Exponential Distribution];\\n E--EB[Binomial Gamma 1 Distribution];\\n E--EC[Binomial Gamma 2 Distribution];\\n E--ED[Binomial Generalized Exponential 1 Distribution];\\n E--EE[Binomial Generalized Exponential 2 Distribution];\\n D--F[p = exp_-t_ ];\\n F--FA[Binomial Exponential Distribution];\\n F--FB[Binomial Gamma 1 Distribution];\\n F--FC[Binomial Gamma 2 Distribution];\\n F--FD[Binomial Generalized Exponential 1 Distribution];\\n F--FE[Binomial Generalized Exponential 2 Distribution];\\n F--FF[Binomial Variated Exponential Distribution];\\n F--FG[Binomial Variated Gamma 2,alpha Distribution];\\n F--FH[Binomial Inverse Gaussian Distribution];\\n D--G[p = cy];\\n G--GA[Binomial Generalized Beta 4 Distribution];\\n \"},\"evals\":[],\"jsHooks\":[]}\rBowman, K. O., Shenton, L. R., Kastenbaum, M. A., \u0026amp; Broman, K. (1992). Overdispersion: Notes on Discrete distributions. Oak Ridge Tennessee : Oak Ridge National Laboratory.\nAlanko, T., \u0026amp; Duffy, J. C. (1996). Compound Binomial distributions for modeling consumption data. Journal of the Royal Statistical society, series D (The Statistician) Vol. 45, No. 3 ,269-286.\nGerstenkorn, T. (2004). A compound of the Generalized Negative Binomial distribution with the Generalized Beta distribution. Central European Science journals, CEJM 2 (4), 527-537.\n\rLog Inverse Distribution [0,1] Domain\r\r{\"x\":{\"diagram\":\"\\n graph LR;\\n A[Binomial Distribution]--B[Binomial Parent];\\n B--C[Mixing Parameter];\\n C--D[Log Inverse Distribution 0,1 Domain];\\n D--DA[Binomial Type 1 Log Inverse Exponential Distribution];\\n D--DB[Binomial Type 2 Log Inverse Exponential Distribution];\\n D--DC[Binomial Type 1 Log Inverse Gamma 1 Distribution];\\n D--DD[Binomial Type 2 Log Inverse Gamma 1 Distribution];\\n D--DE[Binomial Type 1 Log Inverse Gamma 2 Distribution];\\n D--DF[Binomial Type 2 Log Inverse Gamma 2 Distribution];\\n D--DG[Binomial Type 1 Log Inverse Generalized Exponential 1 Distribution];\\n D--DH[Binomial Type 2 Log Inverse Generalized Exponential 1 Distribution];\\n D--DJ[Binomial Type 1 Log Inverse Generalized Exponential 2 Distribution];\\n D--DK[Binomial Type 2 Log Inverse Generalized Exponential 2 Distribution];\\n D--DL[Binomial Type 1 Log Inverse Gamma 3 Distribution];\\n \"},\"evals\":[],\"jsHooks\":[]}\rGrassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformations of the Gamma and derived compound distributions. Australian journal of Statistics, 19 (2) 108-114.\nMcDonald, J. B., \u0026amp; Yexiao, J. X. (1995). A generalization of the Beta distribution with applications. Journal of Econometrics 66,133-152.\n\rCumulative Distribution Function\r\r{\"x\":{\"diagram\":\"\\n graph TB;\\n A[Binomial Distribution]--B[Binomial Parent];\\n B--C[Mixing Parameter];\\n C--D[Cumulative Distribution];\\n D--DA[Beta Generated Distribution];\\n DA--DAA[Binomial Beta Exponential Distribution];\\n DA--DAB[Binomial Beta Generalized Exponential Distribution];\\n DA--DAC[Binomial Beta Power Distribution];\\n D--DB[Kumaraswamy Generated Distribution];\\n DB--DBA[Binomial Kumaraswamy Power Distribution];\\n DB--DBB[Binomial Kumaraswamy Exponential Distribution]; \\n \"},\"evals\":[],\"jsHooks\":[]}\rEugene, N., Lee, C., \u0026amp; Famoye, F. (2002). Beta-normal distributions and its applications. Communications in Statistics-Theory and Methods 31, 4 ,497-512.\nNadarajah, S., \u0026amp; Kotz, S. (2006). The Beta Exponential distribution. Reliability engineering and system safety, Vol. 91, Issue 6 ,689-697\nBarreto-Souza, W., Santos, A., \u0026amp; Cordeiro, G. M. (2009). The Beta Generalized Exponential distribution. Journal of Statistical Computation and Simulation , 1-14.\n\rp Binomial\r\r{\"x\":{\"diagram\":\"\\n graph LR;\\n A[Binomial Distribution]--B[ Binomial Parent];\\n B--C[Mixing Parameter];\\n C--D[p Binomial];\\n D--DA[Beyond Beta Distribution];\\n DA--DAA[Binomial Triangular Distribution];\\n DA--DAB[Binomial Kumaraswamy 2 Distribution];\\n DA--DAC[Binomial Kumaraswamy 1 Distribution];\\n DA--DAD[Binomial Truncated Exponential Distribution];\\n DA--DAE[Binomial Truncated Gamma Distribution];\\n DA--DAF[Binomial - MinusLog Distribution];\\n DA--DAG[Binomial Standard Two Sied Power Distribution];\\n DA--DAH[Binomial Ogive Distribution];\\n DA--DAI[Binomial - Two Sided Ogive Distribution];\\n D--DB[Beta Distribution];\\n DB--DBA[Beta - Binomial Distribution];\\n DB--DBB[McDonald Generalized Beta - Binomial Distribution];\\n DB--DBC[Libby and Novick Generalized Beta - Binomial Distribution];\\n DB--DBD[Gauss Hypergeometric Binomial Distribution];\\n DB--DBE[Confluent Hypergeometric Binomial Distribution];\\n DB--DBF[Binomial Uniform Distribution];\\n DB--DBG[Binomial Power Function Distribution];\\n DB--DBH[Binomial Truncated Beta Distribution];\\n DB--DBI[Binomial Arcsine Distribution];\\n \"},\"evals\":[],\"jsHooks\":[]}\rKarlis, D., \u0026amp; Xekalaki, E. (2006). The Polygonal distributions, ntemational Conference on Mathematical and Statistical modeling in honnor of Enrique Castillo. University of Castilla-La Mancha.\nJones, M. C. (2009). Kumaraswamy’s distribution: a beta-type distibution with some tractability advantages. Statistical Methodology Vol. 6, Issue 1 ,70-81.\nJones, M. C. (2007). The Minimax distribution: A Beta type distribution with some tractability advantages. Retrieved from The Open University: http://stats-www.open.ac.uk/TechnicalReports/minimax.pdf\nKumaraswamy, P. (1980). Genralized probability density functions for double-bounded random processes. Journal of hydrology, 79-88.\nDorp, J. R., \u0026amp; Kotz, S. (2003). Generalizations f Two-Sided Power distributions and their Convolution. Communications in Statistics-Theory and Methods, Vol. 32, Issue 9 ,1703-1723.\nArmero, S., \u0026amp; Bayarri, M. J. (1994). Prior assessments for prediction in queues. The Statistician 43,139- 153.\nBhattacharya, S. K. (1968). Bayes approach to compound distributions arising from truncated mixing densities. Annals of the institute of Statistical Mathematics, Vol 20, No. 1 ,375-381.\nJohnson, N. L., Kotz, S., \u0026amp; Kemp, A. (1992). Univariate Discrete distributions, Second Edition. New York: John Wiley and Sons.\nLibby, D. I., 8t Novick, M. R. (1982). Multivariate generalized beta-distributions with applicatons to utility assessment. Journal of Educational Statistics 9 ,163-175.\nNadarajah, S., 8i Kotz, S. (2007). Multitude of Beta distributions with applications. Statistics: a journal of theoretical and applied statistics, Vol. 41, No. 2 ,153-179.\nSivaganesan, S., 8i Berger, J. (1993). Robust Bayesian analysis of the Binomial empirical Bayes problem. The Canadian journal of Statistics, 21,107-119.\n\r\r\rReferences\rFisher, R. A., and Mather, K. (1936). A linkage test with mice, Annals of Eugenics, London, 7, 265-280.\nBoswell, M. T., Ord, J. K., and Patil, G. P. (1979). Chance mechanisms underlying univariate distributions, Statistical Ecology, Vol. 4:Statistical Distributions in Ecological Work, J. K. Ord, G. P. Patil, and C. Taillie (editors), 1-156. Fairland, MD: International Co-operative Publishing House.\nKaplan, N., and Risko, K. (1982). A method for estimating rates of nucleotide substitution using DNA sequence data, Theoretical Population Biology, 21, 318-328.\nSeber, G. A. F. (1982b). The Estimation of Animal Abundance (second edition), London: Griffin.\n\r","date":1550102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550102400,"objectID":"99696c0e60ea3b3c01cd36f63d544da7","permalink":"/post/binomialdistribution/binomialdistribution/","publishdate":"2019-02-14T00:00:00Z","relpermalink":"/post/binomialdistribution/binomialdistribution/","section":"post","summary":"Brief introduction on Binomial Distribution and its expansion based on data.","tags":["Beta-Binomial","Binomial","fitODBOD"],"title":"Tree of Binomial Distribution","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rClimate Change Research\rEnergy\rFederal\r\r\r# load the packages library(readr)\rlibrary(tidyverse)\rlibrary(gganimate)\rlibrary(dplyr)\rlibrary(magrittr)\r# load the data\rclimate \u0026lt;- read_csv(\u0026quot;climate_spending.csv\u0026quot;)\renergy \u0026lt;- read_csv(\u0026quot;energy_spending.csv\u0026quot;, col_types = cols(year = col_integer()))\rfederal \u0026lt;- read_csv(\u0026quot;fed_r_d_spending.csv\u0026quot;)\rEven though I can go further and do an investigative plotting from the rest data it is not done here. I was more focused on the scientific notation values in the plotting and scales, which were bothering me a lot.\n3 Data sets are given here, they are\nGlobal Climate Change Research Program Spending. - climate\rEnergy Departments Data. - energy\rTotal Federal R \u0026amp; D Spending by Department. - federal\r\rOddly though climate data-set did not have year values, I checked the downloaded csv file and the GitHub upload as well. Well, that did not stop me from doing some tidy plotting.\nYou can obtain the data from here. It should be noted that I am not going to rename the abbreviation of departments with their full names, so below is a screen shot which would come in handy.\nDepartment Full Names with Abbreviations\n\rScientific notation of numbers and plotting them was fun. Except for the R and D budget, other 3 have a steady increase over the years. Further, the R and D budget is very small than the others. Code: https://t.co/jmzfTGMRaT #tidytuesday pic.twitter.com/3WfBU72kWW\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) February 12, 2019  GitHub Code\nClimate Change Research\rAs I mentioned earlier for the climate data there are no values in the year column, but according to summary I was able to deduce that we have 18 years of information. When we do plot it is going to be the summation for each department in a bar.\nClearly NASA has the most amount ( above than 2.5 x 10^10 USD) of spending because rockets are expensive, second place goes to NSF (5 x 10^9 USD) and third place to NOAA. Lowest amount of spending is to the Department of Interior (8.47 x 10^8 USD).\nggplot(climate,aes(x=fct_inorder(department),y=gcc_spending,fill=department))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,show.legend = FALSE)+\rggtitle(\u0026quot;Total GCC Spending for 18 Years\u0026quot;)+\rscale_y_continuous(labels = scales::scientific,breaks = seq(0,2.75e+10,0.25e+10))+\rxlab(\u0026quot;Sub Agency / Department\u0026quot;)+ylab(\u0026quot;GCC Spending (in USD)\u0026quot;)\r\rEnergy\rSince 1997 to 2018 how Energy Department funding has changed with sub agency/ department is the purpose of the below bar plot. Office of Science R \u0026amp; D and Atomic Energy Defense are competitive over the years and for a short period of time the latter has less funding than the former, this was between 2006 to 2010.\nOther agencies oscillates over the years while reaching new highs and lows.\np\u0026lt;-ggplot(energy,aes(x=department,y=energy_spending,fill=year))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,position =\u0026quot;identity\u0026quot;)+\rtransition_time(year)+\rgeom_text(aes(label=scales::scientific(energy_spending)),\rvjust = \u0026quot;inward\u0026quot;, hjust = \u0026quot;inward\u0026quot;)+\rease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rylab(\u0026quot;Energy Spending (in USD)\u0026quot;)+\rtheme(legend.position = \u0026quot;right\u0026quot;)+\rxlab(\u0026quot;Sub Agency / Department\u0026quot;)+\rscale_fill_continuous(breaks = seq(1997,2018,3))+\rscale_y_continuous(labels = scales::scientific)+\rggtitle(\u0026quot;Energy Spending Of Year : {frame_time}\u0026quot;)\ranimate(p,fps=1,nframes=22)\r\rFederal\rData of Federal funding has four different types to be compared and they are mentioned below in the description image which would make explanation more easier.\n\rExcept rd_budget others have a very clear increase in amount between 1976 to 2018. Further, all four plots have different scales and the limits are widely different for each plot.\np\u0026lt;-federal %\u0026gt;%\rgather(funding,amount,c(rd_budget,total_outlays,discretionary_outlays,gdp)) %\u0026gt;%\rggplot(.,aes(x=factor(department),y=amount,color=year))+\rgeom_jitter()+transition_time(year)+\rease_aes(\u0026quot;linear\u0026quot;)+coord_flip()+\rshadow_mark()+\rtheme(legend.position = \u0026quot;right\u0026quot;)+\rylab(\u0026quot;Spending in USD\u0026quot;)+xlab(\u0026quot;Department\u0026quot;)+\rggtitle(\u0026quot;Total Federal R\u0026amp;D for Year : {frame_time}\u0026quot;)+\rscale_color_continuous(breaks = seq(1976,2018,6),labels=seq(1976,2018,6))+\rscale_y_continuous(labels = scales::scientific)+\rfacet_wrap(~funding,scales = \u0026quot;free\u0026quot;)\ranimate(p,fps=1,nframes=42)\rTHANK YOU\n\r","date":1549843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549843200,"objectID":"44a2cdefcd9c6ee2a9116e0d506b1494","permalink":"/post/tidytuesday2019/week7/week7/","publishdate":"2019-02-11T00:00:00Z","relpermalink":"/post/tidytuesday2019/week7/week7/","section":"post","summary":"2019 Week 7 TidyTuesday: Spending On Science Stuff","tags":["gganimate","tidyverse","TidyTuesday"],"title":"Week 7: Spending On Science Stuff","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rMortgage\rFixed Rate 30 Years from 1971 to 2018\rFixed Rate 15 Years from 1991 to 2018\rFees and Points of 30 Years from 1971 to 2018\rFees and Points of 15 Years from 1991 to 2018\r\rStates\rNew England Region\rMideast Region\rGreat Lakes Region\rPlains Region\rSoutheast Region\rSouthwest Region\rRocky Mountain Region\rFar West Region\r\r\r\r# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(bbplot)\rlibrary(gganimate)\rlibrary(magrittr)\rlibrary(lubridate)\r# load the data\rmortgage \u0026lt;- read_csv(\u0026quot;mortgage.csv\u0026quot;, col_types = cols(adjustable_margin_5_1_hybrid = col_double(), adjustable_rate_5_1_hybrid = col_double(), fees_and_pts_15_yr = col_double(), fees_and_pts_30_yr = col_double(), fees_and_pts_5_1_hybrid = col_double(), fixed_rate_15_yr = col_double(), spread_30_yr_fixed_and_5_1_adjustable = col_double())\r)\rrecessions \u0026lt;- read_csv(\u0026quot;recessions.csv\u0026quot;)\rstate_hpi \u0026lt;- read_csv(\u0026quot;state_hpi.csv\u0026quot;)\rWeek 6 has three data-sets, which are mortgage, recession and state_hpi. Number of variables in each data-set is less than 10. You can acquire the data-set from here.\nGitHub Code\n#TidyTuesday What happened in the year 2000 !, and the 2007 recession has made some drastic changes in US avg and the Price index for all regions. What is happening to Hawaii? Code: https://t.co/WuZD9k8X3S pic.twitter.com/0ecmqnUsrJ\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) February 6, 2019  According to the description there is not much of variation in the recession data-set, but this is not the case in other two data-sets.\nMortgage\rMortgage data-set has 9 variables with 8 of them are related to the financial sector and one is refereed to date. So the below analysis or interpretation will be values changing over time. These values will be\nFixed Rate 30 Years\rFixed Rate 15 Years\rFees and Percentage Points (30 Years) of the loan amount.\rFees and Percentage Points (15 Years) of the loan amount.\r\rFixed Rate 30 Years from 1971 to 2018\rEach week the Fixed Rate of 30 Years has been set and I am exploring how it changes in each year from 1971 to 2018. We can clearly see in the early Weeks of 1980 it has significantly increased higher than 17.5%, but in early 1970 it was only 7.5%.\nBy 1990 it has dropped to 7.5% and this pattern continues further until year 2018 where in December the Fixed Rate of 30 Years is slightly less than 5%.\nEach year there can be one of the below patterns I mentioned if the year is divided into two half’s.\nFirst and Second Half of the Year hold the same Percentage points.\rFirst Half of the Year has Higher percentage Points than the second half.\rVice versa of 2.\r\rp\u0026lt;-ggplot(mortgage,aes(x=factor(year(date)),y=fixed_rate_30_yr,color=week(date)))+\rgeom_jitter()+transition_time(year(date))+ease_aes(\u0026quot;linear\u0026quot;)+\rshadow_mark()+xlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Fixed Rate 30 Year Mortgage (%)\u0026quot;)+\rggtitle(\u0026quot;Fixed Rate 30 Year Morgage Change by the Year: {round(frame_time)}\u0026quot;)+\rlabs(color=\u0026quot;Week of the Year\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;,\raxis.text.x =element_text(angle = 90, hjust = 1))\ranimate(p,nframes=48, fps=1)\r\rFixed Rate 15 Years from 1991 to 2018\rFrom 1991 only we have Fixed Rate for 15 Years and in the beginning we can see the percentage slightly above 8. and over the years it is decreasing while some fluctuations occur. This fluctuations happen in the years of 2000, 2006, 2007 and 2018, where they brake pattern of decreasing.\nIn the year 2018 it reaches slightly less than 4% in the first 20 or so weeks, but the last 20 weeks the percentage is above 4%.\np\u0026lt;-ggplot(subset(mortgage,year(date)\u0026gt;=1991),\raes(x=factor(year(date)),y=fixed_rate_15_yr,color=week(date)))+\rgeom_jitter()+transition_time(year(date))+ease_aes(\u0026quot;linear\u0026quot;)+\rshadow_mark()+xlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Fixed Rate 15 Year Mortgage (%)\u0026quot;)+\rggtitle(\u0026quot;Fixed Rate 15 Year Morgage Change by the Year: {round(frame_time)}\u0026quot;)+\rlabs(color=\u0026quot;Week of the Year\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;,\raxis.text.x =element_text(angle = 90, hjust = 1))\ranimate(p,nframes=28, fps=1)\r\rFees and Points of 30 Years from 1971 to 2018\rHighest peek occurs in 1983 which is 2.7 and it decreases over the years gradually. While in the year 1971 the points were close to 1. The gradual decrease is not in effect between the years 1995 and 1996 and its clear in the plot. Yet, we can see no other anomaly in the next few years after 1996, while in 2007 it reaches its lowest point of slightly less than 0.3 (Could be related to the Great recession)\nAnyway by year 2018 after this 2007 recession the points have increased but has not reached 1 and is always oscillating between 0.4 and 0.6 in the years of 2015 to 2018.\np1\u0026lt;-ggplot(mortgage,aes(x=factor(year(date)),y=factor(fees_and_pts_30_yr),color=week(date)))+\rgeom_jitter()+ theme(legend.position = \u0026quot;bottom\u0026quot;,\raxis.text.x =element_text(angle = 90, hjust = 1))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Fees and Percentage points of the Loan Amount\u0026quot;)+\rlabs(color=\u0026quot;Week of the Year\u0026quot;)+\rggtitle(\u0026quot;Fess and Percentage points (30 Years) of the Loan Amount \\n by the Year : {round(frame_time)}\u0026quot;)+\rtransition_time(year(date))+ease_aes(\u0026quot;linear\u0026quot;)+\rshadow_mark()\ranimate(p1,nframes=48, fps=1)\r\rFees and Points of 15 Years from 1991 to 2018\rIn 1991 the points are close to 1.9 and it wavers in between 1.6 and 1.8 until 1997. There is a significant drop from 1997 to 1998 where the points end up averaged around 1 and over the years it slowly decreases until year 2007. Where the lowest point of 0.3 occurs.\nAfter this new low it struggles to maintain any steady increase and rather holds below 0.8 over the next few years until 2018.\np\u0026lt;-ggplot(subset(mortgage,year(date)\u0026gt;=1991),\raes(x=factor(year(date)),y=factor(fees_and_pts_15_yr),color=week(date)))+\rgeom_jitter()+ theme(legend.position = \u0026quot;bottom\u0026quot;,\raxis.text.x =element_text(angle = 90, hjust = 1))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Fees and Percentage points of the Loan Amount\u0026quot;)+\rlabs(color=\u0026quot;Week of the Year\u0026quot;)+\rggtitle(\u0026quot;Fees and Percentage points (15 Year)of the Loan Amount by the Year : {round(frame_time)}\u0026quot;)+\rtransition_time(year(date))+ease_aes(\u0026quot;linear\u0026quot;)+\rshadow_mark()\ranimate(p,nframes=28, fps=1)\r\r\rStates\rUnited States of America has 50 states and comparing all of them at the same time is a ludicrous idea. Therefore, I decided to combine few states and compare them as regions. In order to do this clustering I chose the Wikipedia page which was helpful for me.\nThere are multiple reasons to make different regions out of the 50 states of USA. But according to the Wikipedia page I figured it would be best to focus on the financial side or to be precise cluster of states based on the “Bureau of Economic Analysis Regions”.\nWikipedia for US Regions\nSo according to the above choice we have 8 regions clustering 50 states and they are\nNew England\rMideast\rGreat Lakes\rPlains\rSoutheast\rSouthwest\rRocky Mountain\rFar West\r\rNew England Region\rClear visibility of 2007 recession where US Avg and Price Index declining until 2010 and then improving over the next few years. All states begin very closely but end up very differently in 2018 and in troubled times.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;CT\u0026quot;|state==\u0026quot;ME\u0026quot;|state==\u0026quot;MA\u0026quot;|\rstate==\u0026quot;NH\u0026quot;| state==\u0026quot;RI\u0026quot;|state==\u0026quot;VT\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rMideast Region\rAfter the 2007 recession there is clear difference among DC and other states and the gap cannot be ignored at all.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;DE\u0026quot;|state==\u0026quot;DC\u0026quot;|state==\u0026quot;MD\u0026quot;|\rstate==\u0026quot;NJ\u0026quot;| state==\u0026quot;NY\u0026quot;|state==\u0026quot;PA\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rGreat Lakes Region\rAfter year 2000 there is clear difference among the 5 states and it becomes more complex with the 2007 recession and recovery periods. But this is not the case in year 2018 because all five states are now closely intact with the increase with both variables.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;IL\u0026quot;|state==\u0026quot;OH\u0026quot;|state==\u0026quot;WI\u0026quot;|\rstate==\u0026quot;IN\u0026quot;| state==\u0026quot;MI\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rPlains Region\rBefore the 2007 recession all states behaved very similarly, but this is not the case after year 2011 where North Dakota has a higher Price index and US Average than other states which is clearly seen in the plot.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;IO\u0026quot;|state==\u0026quot;MN\u0026quot;|state==\u0026quot;NE\u0026quot;|\rstate==\u0026quot;KS\u0026quot;| state==\u0026quot;MS\u0026quot;|state==\u0026quot;ND\u0026quot;|state==\u0026quot;SD\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rSoutheast Region\rSoutheast region has alot of states therefore it would be time consuming to compare. Clearly the 2007 recession has a toll on both variables, but not as the effect from year 2000.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;AL\u0026quot;|state==\u0026quot;FL\u0026quot;|state==\u0026quot;KY\u0026quot;|\rstate==\u0026quot;AR\u0026quot;|state==\u0026quot;GA\u0026quot;|state==\u0026quot;MS\u0026quot;|\rstate==\u0026quot;LA\u0026quot;|state==\u0026quot;NC\u0026quot;|state==\u0026quot;SC\u0026quot;|\rstate==\u0026quot;TN\u0026quot;|state==\u0026quot;VA\u0026quot;|state==\u0026quot;WV\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rSouthwest Region\rBefore the 2007 recession and after also we can see the clear changes. Before that in year 2000 also we can see rapid changes which lead up-to the recession. The damage done by the recession have not been recovered in some states even by 2018 according to the gap in Price index.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;AZ\u0026quot;|state==\u0026quot;OK\u0026quot;|\rstate==\u0026quot;TX\u0026quot;|state==\u0026quot;NM\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rRocky Mountain Region\rChanges after 2000 are very different for the 5 states in this region and after the 2007 recession also we can see the rapid set back in Us avg and price index. But this is not the case after 2013 even though it has already made significant amount of divide between the state of MO and other states, which is clearly seen at the end of year 2018.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;CO\u0026quot;|state==\u0026quot;MO\u0026quot;|state==\u0026quot;WY\u0026quot;|\rstate==\u0026quot;ID\u0026quot;| state==\u0026quot;UT\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\r\rFar West Region\rEarly 1990 has a sudden raise and it quickly settles down close to year 1998. Where by 2000 all six states share the same price index value, but this changes over time with clear difference among two groups. Each group containing 3 states, but this progress entirely changes by the 2007 recession and its recovery. Because clearly after 2013 there is no more 2 groups, it is now 3 groups. Where state of Hawaii has the highest pricing index and lowest goes to Alaska, this is by the end of year 2018.\np\u0026lt;-ggplot(subset(state_hpi,state==\u0026quot;AL\u0026quot;|state==\u0026quot;NV\u0026quot;|state==\u0026quot;OR\u0026quot;|\rstate==\u0026quot;CA\u0026quot;| state==\u0026quot;HI\u0026quot;|state==\u0026quot;WA\u0026quot;),\raes(x=us_avg,y=price_index,color=state))+\rgeom_point()+xlab(\u0026quot;US Average\u0026quot;)+ylab(\u0026quot;Price Index\u0026quot;)+\rggtitle(\u0026quot;Price Index vs Us Avg change over Year: {round(frame_time)}\u0026quot;)+\rshadow_mark()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes = 44,fps=1)\rIt might look that I have not done enough justice for the changes which occurred before the year 2000, and I do agree with you. But if I do add them into my consideration this article would be very long. Hopefully, the animated plots clearly indicate the strong changes which occurred in the pre-y2k era.\nTHANK YOU\n\r\r","date":1549411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549411200,"objectID":"425d5384271595404eb7e79d66a934fa","permalink":"/post/tidytuesday2019/week6/week6/","publishdate":"2019-02-06T00:00:00Z","relpermalink":"/post/tidytuesday2019/week6/week6/","section":"post","summary":"2019 Week 6 TidyTuesday: Mortgage and Recession","tags":["tidyverse","TidyTuesday","R package"],"title":"Week 6 : Mortgage, Recession and States","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rIntroduction\rFluid Milk Sales\rState Milk Production\rUS States and Milk Production Over the Years\rSumming States of Same Regions Over the Years\rAveraging Regions Considering All the States Over the Years\r\rCheese\rCheese with Other\rCheese with Total\rCheese with known Type Names\r\r\r\r# load the packages\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(ggthemr)\rlibrary(readr)\rlibrary(gganimate)\rlibrary(usmap)\r# load the data\rfluid_milk_sales \u0026lt;- read_csv(\u0026quot;fluid_milk_sales.csv\u0026quot;)\rstate_milk_production \u0026lt;- read_csv(\u0026quot;state_milk_production.csv\u0026quot;)\rclean_cheese \u0026lt;- read_csv(\u0026quot;clean_cheese.csv\u0026quot;)\r# load the theme\rggthemr(\u0026quot;flat dark\u0026quot;)\rIntroduction\r5 data-sets are given here, but I will be only discussing 3. They are Fluid Milk Sales, State Milk production and Clean Cheese. Clean Cheese has only few rows (48) and few columns (17). This is not the case in Fluid Milk Sales and State Milk production.\nCode: https://t.co/3bSXxttydA Wisconsin and California have a milk interest and it is very high. #TidyTuesday pic.twitter.com/Ide8jfalyd\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) January 29, 2019  GitHub Code\n\rFluid Milk Sales\rFluid Milk Sales has information for 9 different types of milk and how their Consumption in Pounds has changed over time from 1970 to 2017. We can see how Whole and Reduced Fat(2%) type milk are changing over the years with significant amount. Further, We can see how other types are changing from the initial order in 1970 of lower(Eggnog) amount to higher(Whole) amount in pounds over the years.\nattach(fluid_milk_sales)\rfluid_milk_sales$pounds\u0026lt;-fluid_milk_sales$pounds/(10^7)\r# how sales change over the years for 9 different types of milk\rp\u0026lt;-ggplot(fluid_milk_sales,aes(x=fct_inorder(milk_type) ,y=pounds,fill=year))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+\rxlab(\u0026quot;Milk Type\u0026quot;)+ylab(\u0026quot;Pounds (in 10^7)\u0026quot;)+\rcoord_flip()+\rggtitle(\u0026quot;Milk Type vs Pounds in year: {round(frame_time)}\u0026quot;)\ranimate(p,nframes=48,fps=1)\rdetach(fluid_milk_sales)\r\rState Milk Production\rIn the 48 years we are separating them by 24 years each for a plot. Using usmap package I am going to plot it into their respective states in perspective of Milk Produced lbs in 10^6.\nUS States and Milk Production Over the Years\rIn the first half of 1970 to 1993 we can see how a few states are having steady increase over the years.\nattach(state_milk_production)\r# dividing the milk produced by 10^6\r#summary(state_milk_production$milk_produced)\rstate_milk_production$milk_produced\u0026lt;-state_milk_production$milk_produced/(10^6)\r# plot in us map the milk produced by state for years 1970 to 1993\rplot_usmap(data=subset(state_milk_production,year \u0026lt;=\u0026quot;1993\u0026quot;),values = \u0026quot;milk_produced\u0026quot;)+\rfacet_wrap(~factor(year),ncol = 4)+\rggtitle(\u0026quot;Over the years Milk production changing in USA\u0026quot;)+\rtheme(legend.position = \u0026quot;left\u0026quot;)+\rlabs(fill=\u0026quot;lbs in 10^6\u0026quot;)\rThis is similar to the next half which is from 1994 to 2017 as well. Similar increase occurs for the above same states as I see in the below plot. Well it is not very accurately described in the two plots for us to see.\n# plot in us map the milk produced by state for years 1994 to 2017\rplot_usmap(data=subset(state_milk_production,year \u0026gt;\u0026quot;1993\u0026quot;),values = \u0026quot;milk_produced\u0026quot;)+\rfacet_wrap(~factor(year),ncol = 4)+\rggtitle(\u0026quot;Over the years Milk production changing in USA\u0026quot;)+\rtheme(legend.position = \u0026quot;left\u0026quot;)+\rlabs(fill=\u0026quot;lbs in 10^6\u0026quot;)\rTo understand the above same change over the years clearly I have created a bar plot how the increase occurs. This plot also indicates how much change has occurred over the 48 years for each state who produces milk. Clearly the states California and Wisconsin have higher increase over the years, which is very strong. There are some states which have not produced more amount each year than their previous years.\nThe states Wyoming, Rhode island, Hawaii, Delaware and Alaska have very low amount of milk production over the years.\n# over the years how milk production has changed for each year in a bar plot\rp\u0026lt;-ggplot(state_milk_production,aes(x=state,y=milk_produced,\rfill=year))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+coord_flip()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+shadow_mark()+\rxlab(\u0026quot;State\u0026quot;)+ylab(\u0026quot;Milk Produced (in 10^6)\u0026quot;)+\rggtitle(\u0026quot;States vs Milk Produced in year: {round(frame_time)}\u0026quot;)\ranimate(p,nframes = 48,fps=1)\r\rSumming States of Same Regions Over the Years\rThere are 50 states but only 10 regions and not all regions have equal amount of states. Therefore I am going to sum up the milk production for all regions over the years and try to understand if there is any pattern.\nIn order to do this I have used the dplyr package and created a function which would sum up the production for each region of each year. Similarly, this function has the ability to get the average production for each region of each year as well.\n# manipulating the data by sum and mean\rby_region_sum\u0026lt;-function(i,ch_sum)\r{\rif(ch_sum==TRUE)\r{\r# subsetting by summation over all years for each region\rtemp\u0026lt;-subset(state_milk_production,year==i,select=c(\u0026quot;region\u0026quot;,\u0026quot;milk_produced\u0026quot;)) %\u0026gt;%\rgroup_by(region) %\u0026gt;%\rsummarise_each(funs(sum))\routput\u0026lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)\r}\relse\r{\r# subsetting by Average over all years for each region temp\u0026lt;-subset(state_milk_production,year==i,select=c(\u0026quot;region\u0026quot;,\u0026quot;milk_produced\u0026quot;)) %\u0026gt;%\rgroup_by(region) %\u0026gt;%\rsummarise_each(funs(mean))\routput\u0026lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)\r}\rreturn(output)\r}\rUsing the by_region_sum function I am now finding the sum as below.\n# subsetting by summation\rmilk_region_data\u0026lt;-rbind.data.frame(by_region_sum(1970,T),by_region_sum(1971,T),\rby_region_sum(1972,T),by_region_sum(1973,T),\rby_region_sum(1974,T),by_region_sum(1975,T),\rby_region_sum(1976,T),by_region_sum(1977,T),\rby_region_sum(1978,T),by_region_sum(1979,T),\rby_region_sum(1980,T),by_region_sum(1981,T),\rby_region_sum(1982,T),by_region_sum(1983,T), by_region_sum(1984,T),by_region_sum(1985,T), by_region_sum(1986,T),by_region_sum(1987,T), by_region_sum(1988,T),by_region_sum(1989,T),\rby_region_sum(1990,T),by_region_sum(1991,T), by_region_sum(1992,T),by_region_sum(1993,T), by_region_sum(1994,T),by_region_sum(1995,T), by_region_sum(1996,T),by_region_sum(1997,T), by_region_sum(1998,T),by_region_sum(1999,T), by_region_sum(2000,T),by_region_sum(2001,T),\rby_region_sum(2002,T),by_region_sum(2003,T), by_region_sum(2004,T),by_region_sum(2005,T), by_region_sum(2006,T),by_region_sum(2007,T), by_region_sum(2008,T),by_region_sum(2009,T),\rby_region_sum(2010,T),by_region_sum(2011,T),\rby_region_sum(2012,T),by_region_sum(2013,T),\rby_region_sum(2014,T),by_region_sum(2015,T),\rby_region_sum(2016,T),by_region_sum(2017,T) )\rIf we consider the summation we can see clearly how centered and very limited variation is there for some regions such as Southeast, Northern Plains, Delta States, Corn Belt and Appalachian. There is some variation in the Northeast region. Clear and highest variation is in for Pacific, Mountain and Lake States regions.\n# Region wise total milk production changing over the year ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+\rgeom_jitter()+ coord_flip()+\rxlab(\u0026quot;Region\u0026quot;)+ylab(\u0026quot;Milk Produced (in 10^6)\u0026quot;)+\rggtitle(\u0026quot;Total Milk Produced by Year in All Regions\u0026quot;)\rBelow is the same graph with points animated by year.\n# Region wise total milk production changing over the year animated\rp\u0026lt;-ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+\rgeom_jitter()+ coord_flip()+\rxlab(\u0026quot;Region\u0026quot;)+ylab(\u0026quot;Milk Produced (in 10^6)\u0026quot;)+\rggtitle(\u0026quot;Total Milk Produced for All Regions for Year: {frame_time}\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+shadow_mark()\ranimate(p,nframes=48,fps=1) \r\rAveraging Regions Considering All the States Over the Years\rIf we consider the same approach but for the average of each region we can develop the same two plots. Here also we can see the same variation and centering for points for the same regions over the years.\n# subsetting by average\rmilk_region_data_new\u0026lt;-rbind.data.frame(by_region_sum(1970,F),by_region_sum(1971,F),\rby_region_sum(1972,F),by_region_sum(1973,F),\rby_region_sum(1974,F),by_region_sum(1975,F),\rby_region_sum(1976,F),by_region_sum(1977,F),\rby_region_sum(1978,F),by_region_sum(1979,F),\rby_region_sum(1980,F),by_region_sum(1981,F),\rby_region_sum(1982,F),by_region_sum(1983,F), by_region_sum(1984,F),by_region_sum(1985,F), by_region_sum(1986,F),by_region_sum(1987,F), by_region_sum(1988,F),by_region_sum(1989,F),\rby_region_sum(1990,F),by_region_sum(1991,F), by_region_sum(1992,F),by_region_sum(1993,F), by_region_sum(1994,F),by_region_sum(1995,F), by_region_sum(1996,F),by_region_sum(1997,F), by_region_sum(1998,F),by_region_sum(1999,F), by_region_sum(2000,F),by_region_sum(2001,F),\rby_region_sum(2002,F),by_region_sum(2003,F), by_region_sum(2004,F),by_region_sum(2005,F), by_region_sum(2006,F),by_region_sum(2007,F), by_region_sum(2008,F),by_region_sum(2009,F),\rby_region_sum(2010,F),by_region_sum(2011,F),\rby_region_sum(2012,F),by_region_sum(2013,F),\rby_region_sum(2014,F),by_region_sum(2015,F),\rby_region_sum(2016,F),by_region_sum(2017,F) )\rBelow is the plot for the average of regions over the years.\n# Region wise average milk production changing over the year ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+\rgeom_jitter()+ coord_flip()+ xlab(\u0026quot;Region\u0026quot;)+ylab(\u0026quot;Milk Produced (in 10^6)\u0026quot;)+\rggtitle(\u0026quot;Average Milk Produced by Year in All Regions\u0026quot;)\rThe same plot is now animated for each year and all regions.\n# Region wise average milk production changing over the year animated\rp\u0026lt;-ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+\rgeom_jitter()+ coord_flip()+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)+ shadow_mark()+ xlab(\u0026quot;Region\u0026quot;)+ylab(\u0026quot;Milk Produced (in 10^6)\u0026quot;)+\rggtitle(\u0026quot;Average Milk Produced for All Regions for Year: {frame_time}\u0026quot;)+\rtransition_time(year)+ease_aes(\u0026quot;linear\u0026quot;)\ranimate(p,nframes=48,fps=1) \rdetach(state_milk_production)\r\r\rCheese\r16 types of cheese are provided in this clean cheese data-set. I will divide these types into 3 types and will not consider few types of cheese. The unit of measurement for the consumption is lbs per person.\nCheese with Other\rI am going to consider the types American Other, Italian Other and Swiss for this plot. Red color indicates to American Other, yellow color refers to Italian other and blue for Swiss. Alot of fluctuation for American other type, but this is not the case for Swiss type cheese. There is steady increase for the Italian other type cheese over the years. All of these are less than 4 lbs per person and it is animated.\n# 3 types of cheese change per person over the year in lbs\rp\u0026lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ geom_point(aes(y=`American Other`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;red\u0026quot;)+\rgeom_point(aes(y=`Italian other`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;yellow\u0026quot;)+\rgeom_point(aes(y=Swiss),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;blue\u0026quot;)+\rtransition_time(Year)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Consumption in lbs per person\u0026quot;)+\rggtitle(\u0026quot;Cheese Consumption Over the Years\u0026quot;)+\rease_aes(\u0026quot;linear\u0026quot;)+shadow_mark()\ranimate(p,nframes=48,fps=1)\r\rCheese with Total\rThis is also an animated plot but for the cheese types which has the word Total. They are Total American Cheese, Total Italian Cheese, Total Natural Cheese and Total Processed Cheese Products with the colors represented respectively red, yellow, blue and green.\nAll the Consumption units are in between 0 to 40 lbs per person. Clearly Total Natural Cheese has a steady amount of increase from 1970(slightly above 10) to 2017(approximately less than 40). Considering the other three types we can see it is not the same order that it is in 1970 over the years.\n# 4 types of cheese change per person over the year in lbs\rp\u0026lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ geom_point(aes(y=`Total American Chese`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;red\u0026quot;)+\rgeom_point(aes(y=`Total Italian Cheese`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;yellow\u0026quot;)+\rgeom_point(aes(y=`Total Natural Cheese`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;blue\u0026quot;)+\rgeom_point(aes(y=`Total Processed Cheese Products`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;green\u0026quot;)+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Consumption in lbs per person\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+ ggtitle(\u0026quot;Cheese Consumption Over the Years\u0026quot;)+\rtransition_time(Year)+ ease_aes(\u0026quot;linear\u0026quot;)+shadow_mark()\ranimate(p,nframes=48,fps=1)\r\rCheese with known Type Names\rNext group of cheese types include Cheddar, Mozzarella, Brick, Processed Cheese and Foods \u0026amp; spreads for the colors representing respectively red, yellow, blue, green and white. Clearly Cheddar and mozzarella type cheese are are mostly consumed by 2017 above 10 lbs per person, but this is not the case in 1970 where consumption is less than 6 lbs per person.\nWell Processed Cheese and Foods \u0026amp; Spreads have changed very small over the years. The consumption is always less than 6 lbs per person. This is not the case for Brick type cheese where the consumption is close to zero over the years from 1970 until 2017.\n# 5 types of cheese change per person over the year in lbs\rp\u0026lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ geom_point(aes(y=Cheddar),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;red\u0026quot;)+\rgeom_point(aes(y=Mozzarella),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;yellow\u0026quot;)+\rgeom_point(aes(y=`Brick`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;blue\u0026quot;)+\rgeom_point(aes(y=`Processed Cheese`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;green\u0026quot;)+\rgeom_point(aes(y=`Foods and spreads`),stat=\u0026quot;identity\u0026quot;,color=\u0026quot;white\u0026quot;)+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Consumption in lbs per person\u0026quot;)+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Cheese Consumption Over the Years\u0026quot;)+\rtransition_time(Year)+ ease_aes(\u0026quot;linear\u0026quot;)+shadow_mark()\ranimate(p,nframes=48,fps=1)\rTHANK YOU\n\r\r","date":1548720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548720000,"objectID":"542212a918d16827e8824a7e20488fde","permalink":"/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/","publishdate":"2019-01-29T00:00:00Z","relpermalink":"/post/tidytuesday2019/week5/week-5-dairy-products-in-usa/","section":"post","summary":"2019 Week 5 TidyTuesday: Dairy Products in USA.","tags":["R","tidyverse","TidyTuesday"],"title":"Week 5: Dairy Products in USA","type":"post"},{"authors":null,"categories":["R","Website","Blog"],"content":"\rIntroduction\rMaterials\r“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”\r“Up and Running with Blogdown by Alison Presmanes Hill.”\r“Rmarkdown Websites.”\r“How to make an RMarkdown Website by Nick Strayer \u0026amp; Lucy D’Agostino McGowan.”\r“Creating websites in R by Emily C Zabor.”\r“Getting Started with Blogdown by David Selby.”\r“Getting Started with Blogdown by Danielle Navarro.”\r“Blogdown by Peter’s Blog.”\r“Academic by George Cushen”\r“Happy Git and GitHub for the User.”\r“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”\r“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”\r\rMy Personal Website / Blog\r\r\rIntroduction\rR enthusiasts are focused on developing packages and websites to promote their profile and share their knowledge to the world. Recently released packages such as hugo, blogdown, Rmarkdown, bookdown has played a significant amount of role in this popularity for R statistical software among general users and academics in every field of statistics.\nDue to this reason, I also wanted to develop my own R package to solve problem in hand and share it with the #rstats community. Even though Social Media is a strong way of sharing this amount of information, it is not sturdy over time. To resolve this only I chose to develop my own website using R and supportive tools from R. You are reading this post on my website which I developed in a very short period of time and have being maintaining regularly by posting articles.\n\rMaterials\rI could have written an extensive and long article describing how I developed this website with screenshots and explanatory steps. As it should be a valuable experience I am only going to give you the materials which were used with important points with facts. Further, I shall give you certain specifics of my own website.\n“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”\rLink\n\rPresentation of 20 slides.\rMost of the basic information for packages which are necessary for website development.\rBrief introduction about the website structure and process.\r\r\r“Up and Running with Blogdown by Alison Presmanes Hill.”\rLink\n\rBrief information for blogdown and other development materials.\rDescribed information Deployment and maintaining the website with other tools related to R and Rstudio.\r\r\r“Rmarkdown Websites.”\rLink\n\rBriefest description about using Rmarkdown/Rmd files for website development.\rThere are few other links which could be considered useful.\r\r\r“How to make an RMarkdown Website by Nick Strayer \u0026amp; Lucy D’Agostino McGowan.”\rLink\n\rOne example sample website developed and explained briefly by the authors.\rSeveral links to spark curiosity about website development using Rmarkdown.\r\r\r“Creating websites in R by Emily C Zabor.”\rLink\n\rExplanation on different type of websites which can be produced by R.\rDeployment and additional requirements for website development.\r\r\r“Getting Started with Blogdown by David Selby.”\rLink\n\rLimited amount of information regarding website development.\rThis was written for a talk for the “Warwick R User Group Talk” in 2017.\r\r\r“Getting Started with Blogdown by Danielle Navarro.”\rLink\n\rExtensive amount of information about website development using blogdown.\rMore than enough information about the insides of the website.\rDetailed steps of writing posts and changing elements of the website.\r\r\r“Blogdown by Peter’s Blog.”\rLink\n\r4 Tutorials explaining from scratch about how to develop your website using blogdown.\rTutorial 1 explains about website themes and setting up R and Rstudio.\rTutorial 2 is about hosting the website locally or sharing the work with others.\rTutorial 3 will be information about getting the site live.\rTutorial 4 describes how to bring the website online through Netlify or GitHub.\r\r\r“Academic by George Cushen”\rLink\n\rAll the information about the academic theme this is very popular among people in rstats community.\rDescriptive amount of information about changing elements in the academic theme to make it more homely fo the user.\r\r\r“Happy Git and GitHub for the User.”\rLink\n\rGit and GitHub with R with all the information that anyone needs to know.\r\r\r“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”\rLink\n\rBook with all the information related to Rmarkdown files.\rNo need to look anywhere for clarification regarding Rmarkdown.\r\r\r“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”\rLink\n\rSimilar to the previous two books this is also the most useful for blogdown.\rNo need to look anywhere else for further understanding regarding blogdown.\r\r\r\rMy Personal Website / Blog\rInformation related to my website / blog will be discussed here. Mostly encouraging other people to make necessary changes in the original template to satisfy their curiosity and interest.\n\rI also used the academic theme but made alot of changes.\rThese changes include with a new css, color changing, font changing and others.\rIn the Home page I explored adding icons, heading names and changing the header successfully.\rWhile writing blog posts I was able to learn inventive ways to make them interesting.\r\rFor now that is all I have done in related to developing my own website. It should be your own choice what you are going to develop, a website / blog. Mine is a website where articles are regularly posted. Further, it should be noted that the above materials were active when I was writing this post, therefore if it is not active do use google and try to find it.\nTHANK YOU\n\r","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548633600,"objectID":"79aaf5aee23ef6b5193c17bb66918b58","permalink":"/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/post/personalwebsite/build-your-own-website-or-blog-using-r-rstudio-and-r-packages/","section":"post","summary":"Useful information on how to develop your own website or blog using R.","tags":["R","R package","Website"],"title":"Build Your Own Website or Blog Using R, RStudio and R Packages.","type":"post"},{"authors":null,"categories":["Rshiny","R"],"content":"\rIntroduction\rMaterial Useful for Rsiny Development\rHow To Use The Olympic Rshiny App ?\rStep 1\rStep 2\rStep 3\rStep 4\rStep 5\rStep 6\rStep 7\rStep 8\r\r\r\r\rIntroduction\rRshiny is very popular in the rstats community. The glamourous interface and functionality has helped for this level of popularity. In perspective of using an Rshiny App anyone can use it with minimal amount of knowledge. Which is very useful in bringing statistical analysis to consumers or general public without any trouble.\nI initially wanted to develop an Rshiny App for my fitODBOD package, but I thought it would be best to test the waters. That is what I have done here. Using the Olympic data from kaggle I have found a very convenient way to understand specific results for a choosen country from the Rshiny App.\nAt the beginning I wanted to compare between diferent countries or sports or seasons and come to a conclusion. Well, what kind of a conclusion would make sense bothered me, therefore I turned towards an Rshiny Approach.\nThis data-set includes information from 1896 to 2016. Analyzing the data-set would take tedious amount of time and in my opinion unnecessary amount of complications will arise when it comes to concluding. Information from the data-set includes about Medals, participants name, country, sports, events, season and year.\nKaggle Olympic Data\nOlympic Rshiny App\nGitHub Code\nMaterial Useful for Rsiny Development\rEasiest way to build your own shiny app is to refer the official website. It provides an extensive amount of information regarding Rshiny development. Already developed Rshiny Apps and Templates are also available, which would come in handy. Further, when you do start an Rshiny App through Rstudio you will initially receive a sample App with its code. A few tweaks and changes would lead to necessary changes that you need.\nOfficial Rshiny Website\n\rHow To Use The Olympic Rshiny App ?\rInstructions are also listed in the Rshiny App panel.\nStep 1\rFirst Choose a country that you want to study and find the three letter NOC CODE from the “NOC CODE” tab.\n\rStep 2\rChoose the “GRAPH” tab to understand how medals were won for a chosen country over the years with respective to Gender.\n\rStep 3\rChoose the “DATA” tab to look at the data for the chosen. Further you can scroll through this data and find specific attendee’s Name, Sex, Age, Year, Season, City, Sport, Event and Medal.\n\rStep 4\rUsing “DESCRIBE” tab you can simply study the descriptive statistics for the data of the chosen country.\n\rStep 5\r“G/Years” tab is there to explain the Gender representation over the years of the chosen country through a bar plot.\n\rStep 6\r“S/Years” tab shows a bar plot which has the representation of the Gender of the Sports event participants of the chosen country.\n\rStep 7\r“H/W/Sport” tab explores how participants Height and Weight relationship for each Sporting event with respective to Gender for the chosen country.\n\rStep 8\rRepeat the Steps 1 to 7 and be amused of the results from different countries.\nPLEASE NOTE - You should remember that as a user of this Rshiny Application not all countries have won atleast one medal at the Olympics. At these occurences “MEDAL GRAPH” tab does not show any graph but only an error. This can be confirmed by the “DESCRIBE” tab which will produce the summary for that chosen country.\nTHANK YOU\n\r\r\r","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"163d287ae0dbe13f256ce98ac6db54bb","permalink":"/post/olympicrshiny/olympic-rshiny-approach/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/post/olympicrshiny/olympic-rshiny-approach/","section":"post","summary":"Rshiny application for Olympic data of 1896 to 2016 from Kaggle.","tags":["R package","Rshiny","Kaggle"],"title":"Olympic : Rshiny Approach","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rPrison Summary\rPrison Summary With Gender\rPrison Summary with Ethnicity\rPrison Summary with Other and Total\r\rPretrial Summary with Gender and Total\rComplete Data of Incarceration Trends\rRape Crimes over the Years in States of Rural Area\rRape Crimes over the Years in States of Small or Mid Area\rRape Crimes over the Years in States of Suburban Area\rRape Crimes over the Years in States of Urban Area\r\r\r\r# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(gganimate)\rlibrary(ggthemr)\r# load the theme\rggthemr(\u0026quot;flat dark\u0026quot;)\r# load the data\rpretrial_summary \u0026lt;- read_csv(\u0026quot;pretrial_summary.csv\u0026quot;)\rprison_summary \u0026lt;- read_csv(\u0026quot;prison_summary.csv\u0026quot;)\rincarceration_trends\u0026lt;-read_csv(\u0026quot;incarceration_trends.csv\u0026quot;)\rTidyTuesday Week 4 of 2019 is focused on prison data. You can find the data here. There are 5 csv files, clearly 2 files are a summary of the main data, which are Prison Summary and Pretrial Summary.\nCode: https://t.co/x1Wiq1JzYS . opinion: Decline of prisoners rate after certain periods for different regions. #tidytuesday pic.twitter.com/VkOJDJJBoq\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) January 22, 2019  I have mainly focused on these two data-sets here. Further in order of curiosity I did take a peak at a main data file, which is incarceration_trends.csv.\nPrison Summary\rPrison summary data-set has 4 variables. The year of records begin from 1983 and ends in 2015. The unit of incarceration in is Rate per 100,000. Considering the population categories there are clearly 4 sub groups. Each of these sub groups have been plotted here. Further the variable ‘urbanicity’ is simply grouping the observations according to the developed status. Such as rural, small/mid, suburban and urban.\nPrison Summary With Gender\rThe population increase has an effect on it according to the below plot. Urban area has an increase in these prisoners over the years but after mid 1990 there is a decline. This is true for males. Next considering the suburban area this is quite similar as before, only difference is that the decline begins in year 2005.\nConsidering rural area there is a clear increase of prisoners for both genders in the years. There is an anomaly in year 1986 with alot of prisoners for males. Both rural and small/mid areas behave similarly for both genders as the increase rate gets somewhat slower after 2010.\nsubset(prison_summary,pop_category==\u0026quot;Male\u0026quot; | pop_category==\u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\rfacet_wrap(~urbanicity)+geom_area()+\rtransition_reveal(year)+ labs(fill=\u0026quot;Gender\u0026quot;)+\rscale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\rtheme(axis.text.x = element_text(angle = 90),legend.position = \u0026quot;bottom\u0026quot;)+\rscale_y_continuous(breaks = seq(0,1750,250),labels=seq(0,1750,250))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rate per 100,000\u0026quot;)+\rggtitle(\u0026quot;Gender change over the years from 1983-2015\u0026quot;)\r\rPrison Summary with Ethnicity\r5 ethnicity types are considered here which are Asian, Black, Latino, White and Native American. From 1980 only we can see the active prisoners of Latino and Native American ethnicity. Over the years we can the increase of prisoners for African American Community. The increase is very high considering the other ethnicity types.\nAsian ethnicity people have prisoners but it is only negligible considering the other ethnicity types. Except the suburban area others have an increase rate until 2005 and there is a decline followed in the next years. This is not the case for suburban area. Here less change after year 2000 and the decline begins only in year 2010.\nsubset(prison_summary,pop_category == \u0026quot;Asian\u0026quot; | pop_category==\u0026quot;Black\u0026quot; |\rpop_category == \u0026quot;Latino\u0026quot; |pop_category==\u0026quot;White\u0026quot; |\rpop_category == \u0026quot;Native American\u0026quot; ) %\u0026gt;%\rggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\rfacet_wrap(~urbanicity)+geom_area()+\rtransition_reveal(year)+ labs(fill=\u0026quot;Ethnicity\u0026quot;)+\rscale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\rtheme(axis.text.x = element_text(angle = 90),legend.position = \u0026quot;bottom\u0026quot;)+\rscale_y_continuous(breaks = seq(0,5250,250),labels=seq(0,5250,250))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rate per 100,000\u0026quot;)+\rggtitle(\u0026quot;Ethnicity change over the years from 1983-2015\u0026quot;)\r\rPrison Summary with Other and Total\rThe Other category is no longer active since 1989, but before that we can see different anomalies in all four areas. Clearly urban area has more prisoners and final place goes to suburban according to the below area plot.\nRural area has an increase in prisoners over the years and there is no decline. This is not the case for small/mid and suburban areas. Urban areas has a sudden decline in between year 1995 to 2000 and again there is a steep decline after year 2005.\nsubset(prison_summary,pop_category == \u0026quot;Other\u0026quot; | pop_category==\u0026quot;Total\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\rfacet_wrap(~urbanicity)+geom_area()+\rtransition_reveal(year)+ labs(fill=\u0026quot;Category\u0026quot;)+\rscale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\rtheme(axis.text.x = element_text(angle = 90),legend.position = \u0026quot;bottom\u0026quot;)+\rscale_y_continuous(breaks = seq(0,1000,100),labels=seq(0,1000,100))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rate per 100,000\u0026quot;)+\rggtitle(\u0026quot;Total and Other category change over the years from 1983-2015\u0026quot;)\r\r\rPretrial Summary with Gender and Total\rYear 1970 to 2015 is the range of time considered here and the genders male and female are considered with the total. The data-set is for Pretrial prisoners. There is sudden increase after mid 1980s to all the areas. This sudden increase occurs to both genders and the total as well.\nHere, also we can see an odd behavior for urban area in the entire time range with sudden steeps and peaks.\nggplot(pretrial_summary,aes(x=year,y=rate_per_100000,fill=pop_category))+\rgeom_area()+facet_wrap(~urbanicity)+\rtransition_reveal(year)+ labs(fill=\u0026quot;Category\u0026quot;)+\rscale_x_continuous(breaks=c(1970:2015),labels=c(1970:2015))+\rtheme(axis.text.x = element_text(angle = 90),legend.position = \u0026quot;bottom\u0026quot;)+\rscale_y_continuous(breaks = seq(0,800,50),labels=seq(0,800,50))+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Rate per 100,000\u0026quot;)+\rggtitle(\u0026quot;Total and Gender category change over the years from 1970-2015\u0026quot;)\r\rComplete Data of Incarceration Trends\rThis data-set has all the necessary information related to Incarceration. Further, it includes data for 11 different crime types. Rather than exploring all the crimes I have explored only one crime here, which is Rape.\nThe are four areas in concern are rural, suburban, mid/small and urban. 51 states and 4 regions are considered to see the diversity of these prisoners. We have dropped the years from 1970 to 1976, 2015 and 2016 because they had no data. Even the years 1979 and 1993 has missing data but still I am including this in the plot.\nRape Crimes over the Years in States of Rural Area\rI developed these plots to understand a pattern in state wise or region wise, apparently its very difficult but still I am keeping these plots here. Well, frankly I think there could be some other better way to visualize the above selective data.\np1\u0026lt;-subset(incarceration_trends,year!=\u0026quot;1970\u0026quot; \u0026amp; year!=\u0026quot;1971\u0026quot; \u0026amp; year!=\u0026quot;1972\u0026quot; \u0026amp;\ryear!=\u0026quot;1973\u0026quot; \u0026amp; year!=\u0026quot;1974\u0026quot; \u0026amp; year!=\u0026quot;1975\u0026quot; \u0026amp;\ryear!=\u0026quot;1976\u0026quot; \u0026amp; year!=\u0026quot;2015\u0026quot; \u0026amp; year!=\u0026quot;2016\u0026quot; \u0026amp;\rurbanicity == \u0026quot;rural\u0026quot;,\rselect = c(\u0026quot;year\u0026quot;,\u0026quot;state\u0026quot;,\u0026quot;rape_crime\u0026quot;,\u0026quot;urbanicity\u0026quot;,\u0026quot;region\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\rgeom_jitter(width = 0.1)+transition_time(year)+\rcoord_flip()+ ylab(\u0026quot;Rape Crime\u0026quot;)+xlab(\u0026quot;State\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rlabs(title=\u0026quot;Rape Crime for Rural Areas on the Year : {round(frame_time)}\u0026quot;)\ranimate(p1,fps=1,duration=38)\r\rRape Crimes over the Years in States of Small or Mid Area\rp2\u0026lt;-subset(incarceration_trends,year!=\u0026quot;1970\u0026quot; \u0026amp; year!=\u0026quot;1971\u0026quot; \u0026amp; year!=\u0026quot;1972\u0026quot; \u0026amp;\ryear!=\u0026quot;1973\u0026quot; \u0026amp; year!=\u0026quot;1974\u0026quot; \u0026amp; year!=\u0026quot;1975\u0026quot; \u0026amp;\ryear!=\u0026quot;1976\u0026quot; \u0026amp; year!=\u0026quot;2015\u0026quot; \u0026amp; year!=\u0026quot;2016\u0026quot; \u0026amp;\rurbanicity == \u0026quot;small/mid\u0026quot;,\rselect = c(\u0026quot;year\u0026quot;,\u0026quot;state\u0026quot;,\u0026quot;rape_crime\u0026quot;,\u0026quot;urbanicity\u0026quot;,\u0026quot;region\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\rgeom_jitter(width = 0.1)+transition_time(year)+\rcoord_flip()+ ylab(\u0026quot;Rape Crime\u0026quot;)+xlab(\u0026quot;State\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rlabs(title=\u0026quot;Rape Crime for Small or Mid Areas on the Year: {round(frame_time)}\u0026quot;)\ranimate(p2,fps=1,duration=38)\r\rRape Crimes over the Years in States of Suburban Area\rp3\u0026lt;-subset(incarceration_trends,year!=\u0026quot;1970\u0026quot; \u0026amp; year!=\u0026quot;1971\u0026quot; \u0026amp; year!=\u0026quot;1972\u0026quot; \u0026amp;\ryear!=\u0026quot;1973\u0026quot; \u0026amp; year!=\u0026quot;1974\u0026quot; \u0026amp; year!=\u0026quot;1975\u0026quot; \u0026amp;\ryear!=\u0026quot;1976\u0026quot; \u0026amp; year!=\u0026quot;2015\u0026quot; \u0026amp; year!=\u0026quot;2016\u0026quot; \u0026amp;\rurbanicity == \u0026quot;suburban\u0026quot;,\rselect = c(\u0026quot;year\u0026quot;,\u0026quot;state\u0026quot;,\u0026quot;rape_crime\u0026quot;,\u0026quot;urbanicity\u0026quot;,\u0026quot;region\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\rgeom_jitter(width = 0.1)+transition_time(year)+\rcoord_flip()+ ylab(\u0026quot;Rape Crime\u0026quot;)+xlab(\u0026quot;State\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rlabs(title=\u0026quot;Rape Crime for Suburban Areas on the Year: {round(frame_time)}\u0026quot;)\ranimate(p3,fps=1,duration=38)\r\rRape Crimes over the Years in States of Urban Area\rp4\u0026lt;-subset(incarceration_trends,year!=\u0026quot;1970\u0026quot; \u0026amp; year!=\u0026quot;1971\u0026quot; \u0026amp; year!=\u0026quot;1972\u0026quot; \u0026amp;\ryear!=\u0026quot;1973\u0026quot; \u0026amp; year!=\u0026quot;1974\u0026quot; \u0026amp; year!=\u0026quot;1975\u0026quot; \u0026amp;\ryear!=\u0026quot;1976\u0026quot; \u0026amp; year!=\u0026quot;2015\u0026quot; \u0026amp; year!=\u0026quot;2016\u0026quot; \u0026amp;\rurbanicity == \u0026quot;urban\u0026quot;,\rselect = c(\u0026quot;year\u0026quot;,\u0026quot;state\u0026quot;,\u0026quot;rape_crime\u0026quot;,\u0026quot;urbanicity\u0026quot;,\u0026quot;region\u0026quot;)) %\u0026gt;%\rggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\rgeom_jitter(width = 0.1)+transition_time(year)+\rcoord_flip()+ ylab(\u0026quot;Rape Crime\u0026quot;)+xlab(\u0026quot;State\u0026quot;)+\rtheme(legend.position = \u0026quot;bottom\u0026quot;)+\rlabs(title=\u0026quot;Rape Crime for Urban Areas on the Year: {round(frame_time)}\u0026quot;)\ranimate(p4,fps=1,duration=38)\rTHANK YOU\n\r\r","date":1548115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548115200,"objectID":"3362d7fbdac640771535003fe9a7cdfb","permalink":"/post/tidytuesday2019/week4/week-4-prison-data/","publishdate":"2019-01-22T00:00:00Z","relpermalink":"/post/tidytuesday2019/week4/week-4-prison-data/","section":"post","summary":"2019 Week 4 TidyTuesday: Prisons and Incarceration.","tags":["TidyTuesday","tidyverse","gganimate","R package"],"title":"Week 4: Prison Data","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rAGENCIES\rAgency vs Count\rType vs Count\rClass vs Count\rAgency Type vs Count\rState Code vs Count\rLocation vs Count\rStart Year and End Year vs agency\r\rLAUNCHES\rSuccess or Failure of these missions vs Category Variables\rSuccess or Failure vs Launch Year\rSuccess or Failure vs Agency Type\rSuccess or Failure vs State Code\r\rState Code vs Category Over time for Success and Failure\r\r\r\r# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(ggalt)\rlibrary(magrittr)\rlibrary(dplyr)\rlibrary(ggthemr)\rlibrary(gganimate)\r# Load the Agency data\ragencies\u0026lt;-read_csv(\u0026quot;agencies.csv\u0026quot;)\r# Load the Launches data\rlaunches\u0026lt;-read_csv(\u0026quot;launches.csv\u0026quot;)\rattach(agencies)\rattach(launches)\r# load a theme\rggthemr(\u0026quot;flat dark\u0026quot;)\rAGENCIES\rSpace related agencies of 74 are in the world from this data set. Another, data set is for launches from the agencies in concern. In the agencies data set there are 19 variables and launches data set has 11 variables. You can find the data set and information regarded to it here.\nEarly years of space launches had more mistakes and they were owned by the state. After the cold war, there is short of enthusiasm, but now we have an increase in launches. Code: https://t.co/R0BLjFOH4U #tidytuesday pic.twitter.com/6dX338hpPD\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) January 16, 2019  GitHub Code\nAgency vs Count\rMost amount of launches are from Rakentiye Voiska Strategicheskogo Naznacheniye (RVSN) and it is 1528 and next is Upravleniye Nachalnika Kosmicheskikh Sredstv (UNKS) with 904. Out of the Top 10 places (considering the most launches) it is clear that class D agencies has the most amount (of 5),while 3 from class C agencies and the rest with class B. NASA is in third place with 469 launches and it is a class C agency. My favorite agency Space X (SPX) has launched 65 times and it is a class B agency.\nggplot(agencies,aes(x=fct_inorder(agency),y=count,\rcolor=class,fill=class))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,width=0.75)+coord_flip()+\rgeom_text(label=agencies$count, hjust=-0.15)+\rxlab(\u0026quot;Space Agency\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Space Agency vs Frequency By Class\u0026quot;)\rSimilarly, for the same bar plot if we change color according to agency type we have different insight. Top 10 agencies is 90% filled with state ownership and 10% is with private ownership. It should be noted that overall there are only two start-ups and close to 10 have private ownership, rest is state owned. The highest amount of launches for a private ownership is from Arian Space(AE) and for start-up its Space X (SPX). Respectively, their counts are 258 and 65.\nggplot(agencies,aes(x=fct_inorder(agency),y=count,\rcolor=agency_type,fill=agency_type))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;,width=0.75)+coord_flip()+\rgeom_text(aes(label=count), hjust=-0.15)+\rxlab(\u0026quot;Space Agency\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rlabs(color=\u0026quot;Agency Type\u0026quot;,fill=\u0026quot;Agency Type\u0026quot;)+\rggtitle(\u0026quot;Space Agency vs Frequency By Agency Type\u0026quot;)\r\rType vs Count\rType of agencies is very complex because an agency can play multiple roles. Highest amount of count is for O/LA type with 3227 and second place is for LA type with 821 counts. There are 145 agencies with the highest combination of types and this category is O/LA/LV/PL/E/S with 145 counts.\nagencies[,c(\u0026#39;type\u0026#39;,\u0026#39;count\u0026#39;)] %\u0026gt;% group_by(type) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;% arrange(count) %\u0026gt;%\rggplot(.,aes(fct_inorder(type),count))+\rgeom_bar(stat = \u0026quot;identity\u0026quot;)+\rgeom_text(aes(label=count),hjust=-0.15)+coord_flip()+\rxlab(\u0026quot;Type\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Type vs Frequency\u0026quot;)\r\rClass vs Count\rClass C and B has similar amounts of count which is close to 1100 and most launches are from D class agencies with the count of 3584.\nagencies[,c(\u0026#39;class\u0026#39;,\u0026#39;count\u0026#39;)] %\u0026gt;% group_by(class) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;% arrange(count) %\u0026gt;%\rggplot(.,aes(fct_inorder(class),count))+\rgeom_bar(stat = \u0026quot;identity\u0026quot;)+\rgeom_text(aes(label=count),vjust=-0.15)+\rxlab(\u0026quot;Class\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Class vs Frequency\u0026quot;)\r\rAgency Type vs Count\rIn perspective of agency type there are 4765 state owned launches, but only 67 launches from start-ups.\nagencies[,c(\u0026#39;agency_type\u0026#39;,\u0026#39;count\u0026#39;)] %\u0026gt;% group_by(agency_type) %\u0026gt;% summarise_each(funs(sum)) %\u0026gt;% arrange(count) %\u0026gt;%\rggplot(.,aes(fct_inorder(agency_type),count))+\rgeom_bar(stat = \u0026quot;identity\u0026quot;)+\rgeom_text(aes(label=count),vjust=-0.15)+\rxlab(\u0026quot;Agency Type\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Agency Type vs Frequency\u0026quot;) \r\rState Code vs Count\rClose to 2500 missions were launched by Soviet Union and 1709 were done by Unite States.\nagencies[,c(\u0026#39;state_code\u0026#39;,\u0026#39;count\u0026#39;)] %\u0026gt;% group_by(state_code) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;% arrange(count) %\u0026gt;%\rggplot(.,aes(fct_inorder(state_code),count))+\rgeom_bar(stat = \u0026quot;identity\u0026quot;)+ coord_flip()+\rgeom_text(aes(label=count),hjust=-0.15)+\rxlab(\u0026quot;State Code\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;State Code vs Count\u0026quot;)\r\rLocation vs Count\rMore than 1500 launches are from Mosvka? and exactly 1204 launches from Moskva. Further, 469 launches from Washington D.C.\nagencies[,c(\u0026#39;location\u0026#39;,\u0026#39;count\u0026#39;)] %\u0026gt;% group_by(location) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;% arrange(count) %\u0026gt;%\rggplot(.,aes(fct_inorder(location),count))+\rgeom_bar(stat = \u0026quot;identity\u0026quot;)+ coord_flip()+\rgeom_text(aes(label=count),hjust=-0.15)+\rxlab(\u0026quot;Location\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Location vs Count\u0026quot;)\r\rStart Year and End Year vs agency\rBelow is a Dumbbell plot to see at the agencies which are no longer active. Before 1960 there was very small activity and they are all owned by the state. With the American and Russian Space race we have private sector also being part of this adventure, but most of them are ending their service around the first half of 1990. There is more activity after this regularly but they are short lived for these agencies. Royal Aircraft Establishment (RAE) has long life for space adventure which was begun around late 1915, and ends its service in around 1990.\nsubset(agencies, substr(tstart,1,4) != \u0026quot;-\u0026quot; \u0026amp; substr(tstop,1,4) != \u0026quot;-\u0026quot; \u0026amp; substr(tstop,1,4) != \u0026quot;*\u0026quot; ) %\u0026gt;%\rggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),\rx=as.numeric(substr(tstart,1,4)),xend=as.numeric(substr(tstop,1,4)),\rfill=agency_type,color=agency_type))+\rgeom_dumbbell(size_x = 2,size_xend = 2.75,size=1.25)+ xlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Agency\u0026quot;)+ scale_x_continuous(breaks=seq(1910,2020,5),labels=seq(1910,2020,5))+\rlabs(fill=\u0026quot;Agency Type\u0026quot;,color=\u0026quot;Agency Type\u0026quot;)+\rtheme(axis.text.x = element_text(angle = 90))+\rggtitle(\u0026quot;Start Year and End Year vs Agency If We Know When\u0026quot;)\rIt should be effectively noted that “-” means still active and “*\u0026quot; means unknown in my perspective. Here we cannot consider the years as numeric because of the characters used. Agencies like NASA and Space X are still active according to my knowledge therefore I considered the above assumption for characters. Most of these agencies are state owned and after Space X there is Rocket Lab USA (RLABU). Most of these agencies were launched after 1980.\nsubset(agencies, substr(tstart,1,4) == \u0026quot;-\u0026quot; | substr(tstop,1,4) == \u0026quot;-\u0026quot; | substr(tstop,1,4) == \u0026quot;*\u0026quot; ) %\u0026gt;%\rggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),\rx=substr(tstart,1,4),\rxend=substr(tstop,1,4),\rfill=agency_type,color=agency_type))+\rgeom_dumbbell(size_x = 2,size_xend = 3,size=1.25)+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Agency\u0026quot;)+\rlabs(fill=\u0026quot;Agency Type\u0026quot;,color=\u0026quot;Agency Type\u0026quot;)+\rtheme(axis.text.x = element_text(angle = 90))+\rggtitle(\u0026quot;Start Year and End Year vs Agency If Do Not Know When\u0026quot;)\r\r\rLAUNCHES\rCounts of above missions are mentioned here thoroughly.\nSuccess or Failure of these missions vs Category Variables\rThere are few categorical variables which could be associated with the success or failure of these missions.\nSuccess or Failure vs Launch Year\rLess mistakes over the year with technologies improving and in between 1960 to 1990 we can see alot of launches always above 100 per year. This enthusiasm no longer exists until 2005. After 2005 there is positive increase in launches and failures also less.\nggplot(launches,aes(x=factor(launch_year),fill=category))+\rgeom_bar()+\rtheme(axis.text.x = element_text(angle = 90))+\rxlab(\u0026quot;Years\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Years vs Frequency\u0026quot;)+\rscale_y_continuous(labels=seq(0,150,10),breaks=seq(0,150,10))\r\rSuccess or Failure vs Agency Type\rState owned agencies has more failures than private and start-ups because it would be costly. More than 4750 launches are from state owned agencies but in them more than 500 launches are failures. Even though private owned agencies has a history from 1990 they have less than 1000 launches.\nggplot(launches,aes(x=fct_infreq(factor(agency_type)),fill=category))+\rgeom_bar()+\rxlab(\u0026quot;Agency Type\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Agency Type vs Frequency\u0026quot;)+\rscale_y_continuous(labels=seq(0,5000,250),breaks=seq(0,5000,250))\r\rSuccess or Failure vs State Code\rSoviet Union (SU) and United States (US) has the most dominant appearance in this field. More than 2400 launches from SU and for US it is more than 1700 launches. Failures also considerably higher for SU and US.\nggplot(launches,aes(x=fct_infreq(factor(state_code)),fill=category))+\rgeom_bar()+\rtheme(axis.text.x = element_text(angle = 90))+\rxlab(\u0026quot;State Code\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;State Code vs Frequency\u0026quot;)+\rscale_y_continuous(labels=seq(0,2500,100),breaks=seq(0,2500,100))\r\r\rState Code vs Category Over time for Success and Failure\rAnimated jitter plot here explains how over the years these launches occur based on States and Success(O) or Failure(F).\np\u0026lt;-ggplot(launches,aes(y=category,x=state_code,color=agency_type))+\rgeom_jitter()+\rlabs(title = \u0026quot;States vs Success or Failure by : {round(frame_time,0)}\u0026quot;,\rx=\u0026quot;State Code\u0026quot;,y= \u0026quot;Success or Failure\u0026quot;)+\rtransition_time(launch_year)+ease_aes(\u0026#39;linear\u0026#39;)+\rlabs(color=\u0026quot;Agency Type\u0026quot;)\ranimate(p,fps=2,duration = 60)\rTHANK YOU\n\r\r","date":1547596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547596800,"objectID":"2a7ab8fdfed12cfea11b83a6edf3d7df","permalink":"/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/","publishdate":"2019-01-16T00:00:00Z","relpermalink":"/post/tidytuesday2019/week3/week-3-space-agencies-and-launches/","section":"post","summary":"2019 Week 3 TidyTuesday: Space Launches and Agencies.","tags":["R","R package","TidyTuesday","tidyverse"],"title":"Week 3: Space Agencies and Launches","type":"post"},{"authors":null,"categories":["fitODBOD","R"],"content":"\r\r\r\r\r\r\rIntroduction\rCredit to People of R Community\r1) Coding Standards (Coding to Understand)\r2) Package Structure\r3) DESCRIPTION file\r4) README file\r5) /R directory\r6) /data directory\r7) /tests directory\r8) /man directory\r9) NAMSESPACE file\r10) .Rbuildignore file\r11) .gitignore file\r\rBuilding the Package\rDistributing the Package\r\r\rIntroduction\rR package development is no longer as it was before 2010 because now most of the work can be done by just a simple mouse-click or with the use of a function. My intention of writing this blog post is not to give a thorough demonstration of how to develop your own R package. But it will briefly explain the process with the most important steps, and will include valuable blog posts and websites which helped me to develop my own R package fitODBOD.\n\rCredit to People of R Community\rI would definitely recommend to read all of these books and start your package development. Or at least make step by step progress in your work while reading them. If you have a basic knowledge regarding R, R studio, CRAN and writing programs, they are more than enough for you to start.\n\rR packages by Hadley Wickam\r\rThis website contains everything that is in the book. The basic things related to an R package development process are structured properly here. It is very useful to read this book/website. The package structure, the type of files necessary, how should the writing be in these files and further, what kind of ways can we use to achieve the final outputs. The book is mainly focused on producing an R package which can be updated with the highest standards using reproducing ability. Such as CRAN standards and GitHub releases.\n\rWriting R Extensions\r\rEverything related to package development is included here with clear instructions. This document is provided by the CRAN project to make package development more friendlier. It includes the official standards for files and naming conventions related to R package development.\n\rCreating R packages: A Tutorial by Friedrich Leish\r\rA brief article explaining about writing functions, classes and methods for R package development. This is very abstract and useful in package development specially as it is focusing on object oriented programming and S formulas.\n\rR for Beginners by Emmanuel Paradis\r\rA book explaining R and its ability in detail, for example regarding functions, data, abilities and limitations of R. There is also a section for R packages, which has valuable information. Writing functions is very crucial in R package development therefore going through this document is worth. Several packages also include data-sets in them. While you develop functions similarly we can develop data-sets as well. There are sections which includes information regarding data-sets as well.\n\rThe Art of R Programming by Norman Matloff\r\rAnother book that will be useful in understanding how R functions and data-sets can be used in R package development.\n\rThe R Inferno by Patrick Burns\r\rUseful book related to object oriented programming, functions and data objects related to R. Very thorough and scrutinized information with valuable explanation which makes things more clearer.\n\rCheat Sheets\r\rEasy implementation of packages mentioned in these cheatsheets. Very essential for someone who is interested in doing R related stuff efficient and eloquent.\n\rR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund\r\rNotes related to RMarkdown, very useful for vignette building.\n\rHappy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester\r\rUsing GitHub for package development is very useful, specially when it comes to sharing and version control. This book explains it all with simplicity.\n\r{\"x\":{\"diagram\":\"graph TB;\\n A(Code with Standards and comment)--C(Create R package);\\n C--D(R package Structure);\\n D--E(DESCRIPTION file);\\n D--F(README file);\\n D--G(/R directory);\\n D--H(/data directory);\\n D--I(/tests directory);\\n D--J(/man directory);\\n D--K(NAMESPACE file);\\n D--L(/vignettes directory);\\n D--M(NEWS.md file);\\n E--O(Build the package);\\n O--N(Source,Bundle,Binary,Installed,In Memory);\\n F--O; G--O; H--O; I--O; J--O; K--O; L--O; M--O;\"},\"evals\":[],\"jsHooks\":[]}\r1) Coding Standards (Coding to Understand)\r\rFocus on naming conventions.\rFocus on input parameters and outputs.\rFocus on indentation.\rComment regularly to make sense of the functions.\r\rSample Code\n\r\r2) Package Structure\r\rVery Important.\rInitially few files will be originated in the designated project folder.\rOver time we might add folders or create files manually.\rExample - tests directory, README.Rmd, …\r\rPackage structure inside your project folder.\n\r3) DESCRIPTION file\r\rFile explaining basic things related to your package.\rExample - package name, other packages needed, authors name, …\rCan edit manually or use specific R package.\r\rAfter changes the DESCRIPTION file\n\r4) README file\r\rVery much optional.\rOnly used in related to GitHub submission.\rUsing Rmarkdown to generate a GitHub output document.\r\rRmarkdown document\nGitHub document\nPreview of GitHub document\n\r5) /R directory\r\rMost important directory.\rThe place where all your R code is written by you.\rBest to have separate R script files for each function.\rNeed to have a R script file for Data as well.\rR scripts can be modified further in order to create RDocumentation files(Rd files).\rThese RDocumentation files will explain about the function.\rProcessed R script files will automatically generate Rd files in the man directory.\r\rR script file with necessary roxygen tags to develop RDocumentation files.\n\r6) /data directory\r\rNot compulsory.\rEasy to use your own data therefore its worth it.\rThis directory will include the data-sets.\rR directory can have an R script to generate Rd files for these data-sets.\r\rdata directory which includes data-sets.\nRscript file which includes necessary roxygen tags to generate Rd files.\n\r7) /tests directory\r\rIf your package is going to be in CRAN or going to be in a platform with large range of users this would be useful.\rUnit tests to check if functions are working properly.\rTesting if the data sets are in proper form.\r\rtests directory and files in side that directory.\nsub directory testthat which includes test R scripts for all functions and data sets.\nR script to test a function.\nR script to test a data set.\n\r8) /man directory\r\rThis directory will include the Rd files for all functions and data sets.\rIf you use roxygen tags there is no need to manually type them.\r\rWith the help of R script files these RDocumentation files will be generated for each function and will be in the man directory.\nThe RDocumentation files can be processed into html outputs or into a pdf manual.\nRd file of a data-set which is created with the help of data R script.\nHtml file which is generated with the help of Rd file for the data-set.\n\r9) NAMSESPACE file\r\rA file which will have all the functions that you created for your package.\rIf a function is exported then it will be in this file.\r\rNAMESPACE file and its components.\n\r10) .Rbuildignore file\r\rA document which includes what kind of files should not be used when building the package.\rExtensions of a file or partial or full name of the file can be added into this document.\r\r.Rbuildignore file of fitODBOD package.\n\r11) .gitignore file\r\rA document which includes what kind of files should not be pushed to the GitHub repository.\rExtensions of a file or partial or full name of the file can be added into this document.\r\r.gitignore file of fitODBOD package.\n\r\rBuilding the Package\rAll the files should be in their respective directories and names should not be changed for files or folders manually if they are created automatically. After checking all of this we can proceed to building the package. This process has 9 steps and below is a diagram to show how it works.\n\r{\"x\":{\"diagram\":\"graph LR;\\n A(Document Generation)--C(Clean and Rebuild);\\n C--D(Spellcheck Rd files);\\n D--B((Check for issues));\\n B--Z((Make Necessary Changes)); Z--A;\\n D--E(Test the Package); E--B;\\n E--F(Check for Errors); F--B;\\n F--G(Build Source Package);\\n G--H(Build Binary Package);\\n H--I(Generate Manual pdf);\\n I--J(Check Errors on Source Package); J--B\"},\"evals\":[],\"jsHooks\":[]}\rThis process is explained through a small presentation here. This presentation also can be used when you need to update your package version.\n\rDistributing the Package\rThere are several ways of distributing your package. They are mainly\ntar.qz version.\rzip version.\rGitHub Repository.\rThe project folder which includes the package.\rSubmit to CRAN or Bioconductor.\r\rI have written two posts related to R packages as well. One is How to find your R package and Second is Using R packages to develop your own package. These two posts will be very useful as well.\nTHANK YOU\n\r","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"d682a3948a119fb19fa0efaf9b90cadd","permalink":"/post/yourownpackage/developing-an-r-package/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/post/yourownpackage/developing-an-r-package/","section":"post","summary":"Developing your own package in R with steps and helpful materials.","tags":["R","R package","fitODBOD"],"title":"Developing an R package","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rGenre\rGenre and Season\rGenre and Year\rGenre and Month\r\rSeason\rSeason and Year\rSeason and Month\r\rTop 3 Genres\rCrime Drama Mystery\rComedy Drama\rDrama\r\rRating over the years\rTv Series with more than 14 Seasons\r\r\r# loading the packages\rlibrary(tidyverse)\rlibrary(summarytools)\rlibrary(magrittr)\rlibrary(readr)\rlibrary(lubridate)\rlibrary(gganimate)\rlibrary(stringr)\r# load the dataset\rRatings \u0026lt;- read_csv(\u0026quot;IMDb_Economist_tv_ratings.csv\u0026quot;, col_types = cols(date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;)))\rRatings data-set is from the IMDB site. I just found out that IMDb is active from 1990, that is a very long time and new information to me.\nThe code in GitHub: https://t.co/ITQnSRH7Ot .Week 2 of 2019. Rating over the year and changes in sharing. #TidyTuesday pic.twitter.com/xP5F7PWDFV\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) January 8, 2019  GitHub code\nTV shows from 1990 to 2018 with their ratings, genres, sharing and aired dates is in this data-set. I wanted a cool function to summarize the data-set at once, therefore browse through the internet and found the package summarytools. There are quite a few functions in the mix, yet I choose dfSummary.\nBelow is the code of using that function on the Ratings data-set.\n#Basic summary of all variables\rdfSummary(Ratings,style = \u0026#39;grid\u0026#39;)\r## Data Frame Summary ## Ratings ## **Dimensions:** 2266 x 7 ## **Duplicates:** 1 ## ## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | No | Variable | Stats / Values | Freqs (% of Valid) | Text Graph | Valid | Missing |\r## +====+===============+================================+======================+========================================+========+=========+\r## | 1 | titleId | 1. tt0098844 | 20 ( 0.9%) | | 2266 | 0 |\r## | | [character] | 2. tt0203259 | 20 ( 0.9%) | | (100%) | (0%) |\r## | | | 3. tt0118401 | 19 ( 0.8%) | | | |\r## | | | 4. tt0108757 | 15 ( 0.7%) | | | |\r## | | | 5. tt0247082 | 15 ( 0.7%) | | | |\r## | | | 6. tt0413573 | 15 ( 0.7%) | | | |\r## | | | 7. tt0452046 | 14 ( 0.6%) | | | |\r## | | | 8. tt0118375 | 13 ( 0.6%) | | | |\r## | | | 9. tt0460681 | 13 ( 0.6%) | | | |\r## | | | 10. tt0460627 | 12 ( 0.5%) | | | |\r## | | | [ 866 others ] | 2110 (93.1%) | IIIIIIIIIIIIIIIIII | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 2 | seasonNumber | mean (sd) : 3.26 (3.44) | 27 distinct values | : | 2266 | 0 |\r## | | [numeric] | min \u0026lt; med \u0026lt; max : | | : | (100%) | (0%) |\r## | | | 1 \u0026lt; 2 \u0026lt; 44 | | : | | |\r## | | | IQR (CV) : 3 (1.05) | | : | | |\r## | | | | | : . | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 3 | title | 1. Law \u0026amp; Order | 20 ( 0.9%) | | 2266 | 0 |\r## | | [character] | 2. Law \u0026amp; Order: Special Vict | 20 ( 0.9%) | | (100%) | (0%) |\r## | | | 3. Midsomer Murders | 19 ( 0.8%) | | | |\r## | | | 4. CSI: Crime Scene Investig | 15 ( 0.7%) | | | |\r## | | | 5. ER | 15 ( 0.7%) | | | |\r## | | | 6. Grey\u0026#39;s Anatomy | 15 ( 0.7%) | | | |\r## | | | 7. Criminal Minds | 14 ( 0.6%) | | | |\r## | | | 8. King of the Hill | 13 ( 0.6%) | | | |\r## | | | 9. Supernatural | 13 ( 0.6%) | | | |\r## | | | 10. Bones | 12 ( 0.5%) | | | |\r## | | | [ 858 others ] | 2110 (93.1%) | IIIIIIIIIIIIIIIIII | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 4 | date | min : 1990-01-03 | 1808 distinct val. | : | 2266 | 0 |\r## | | [Date] | med : 2012-12-07 | | . : | (100%) | (0%) |\r## | | | max : 2018-10-10 | | . : : | | |\r## | | | range : 28y 9m 7d | | . : : : : | | |\r## | | | | | . . . . : : : : : : | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 5 | av_rating | mean (sd) : 8.06 (0.67) | 1997 distinct values | : | 2266 | 0 |\r## | | [numeric] | min \u0026lt; med \u0026lt; max : | | : : | (100%) | (0%) |\r## | | | 2.7 \u0026lt; 8.11 \u0026lt; 9.68 | | : : | | |\r## | | | IQR (CV) : 0.76 (0.08) | | . : : | | |\r## | | | | | : : : . | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 6 | share | mean (sd) : 1.28 (3.38) | 454 distinct values | : | 2266 | 0 |\r## | | [numeric] | min \u0026lt; med \u0026lt; max : | | : | (100%) | (0%) |\r## | | | 0 \u0026lt; 0.32 \u0026lt; 55.65 | | : | | |\r## | | | IQR (CV) : 0.99 (2.64) | | : | | |\r## | | | | | : | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\r## | 7 | genres | 1. Crime,Drama,Mystery | 369 (16.3%) | III | 2266 | 0 |\r## | | [character] | 2. Comedy,Drama | 174 ( 7.7%) | I | (100%) | (0%) |\r## | | | 3. Drama | 168 ( 7.4%) | I | | |\r## | | | 4. Action,Crime,Drama | 146 ( 6.4%) | I | | |\r## | | | 5. Action,Adventure,Drama | 112 ( 4.9%) | | | |\r## | | | 6. Crime,Drama | 107 ( 4.7%) | | | |\r## | | | 7. Drama,Romance | 86 ( 3.8%) | | | |\r## | | | 8. Comedy,Crime,Drama | 80 ( 3.5%) | | | |\r## | | | 9. Comedy,Drama,Romance | 76 ( 3.4%) | | | |\r## | | | 10. Crime,Drama,Thriller | 63 ( 2.8%) | | | |\r## | | | [ 87 others ] | 885 (39.1%) | IIIIIII | | |\r## +----+---------------+--------------------------------+----------------------+----------------------------------------+--------+---------+\rBasic summary of seasons indicate 27 distinct values and obviously 1 is the minimum value, but the maximum value is 44. Another odd thing is median for being 2, further the average is 3.26. Which means most of the TV shows have only up-to few seasons. I would guess not more than 5.\nSummary of date indicates the earliest TV show from 1990 and latest from 2018. So the year difference is 28 years, but there are only 1808 distinct values. Even though the data-set contains 2266 observations. Some TV shows might have to start on the same day, that is the only plausible conclusion.\n“av_rating” (I presume Audio/Video Rating) is in the scale from 1 to 10, where people had influence. The least rating value is 2.7 and the most rating value is 9.68, but the median is 8.11. Also the mean is 8.06. We can say most of these TV shows are excellent to watch. There is another numeric variable called share and the value ranges from 0 to 55.65 where the average is 1.28.\nGenre\rGenres in this data-set has close to 100 distinct factors. The category “Crime,Drama,Mystery” holds the highest percentage of 16.3, while second place is to “Comedy,Drama” with 7.7% and third place is to “Drama” type with 7.4%. All the other types of genre is represented by less than 7%.\nGenre and Season\rGenre and Season are two categorical variables which should be compared to find out if over time do people like the same genre type. If we look at the bar plot it is clear “Crime,Drama,Mystery” type has seasons from 1 to 20. Mostly in all genres there is clear sign of TV shows with seasons up-to three or four. Some of them make it to season ten or eleven, for example genres like “Drama”, “Drama,Thriller”, “Animation,Comedy,Drama” and “Adventure,Drama,Family”.\nOddly in “Drama,Romance” and “Crime,Drama” there are TV shows which has seasons above 35 but very few. It becomes more weird where for the same genre types the seasons in-between 25 and 34 are missing. Clearly in the legend also until season 20 there is continuity, but this does not carry on for higher seasons.\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(seasonNumber)))+\rgeom_bar()+ coord_flip()+labs(fill=\u0026quot;Season\u0026quot;)+\rxlab(\u0026quot;Genre\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Genre and Seasons\u0026quot;)+\rscale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))\r\rGenre and Year\rBar plot indicates that after 2014 around 90% of these genre type TV shows have been done. In the top ten category according to the counts of TV shows clearly all of these genres have been active since 1990 to now. Some of them were started in mid 1990s which include the genre types “Action,Crime,Drama”, “Crime,Drama” and “Comedy,Crime,Drama”.\nTypes such as “Drama,Romance,Sport” and “Adventure,Drama,Romance” were in active in the mid 2000s but no longer. Genres such as “Drama,History”, “Drama,Horror,Thriller” and “Action,Drama” are a few of them which were popular after 2012.\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(year(date))))+\rgeom_bar()+ coord_flip()+labs(fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;Genre\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Genre Over the Year\u0026quot;)+\rscale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))\r\rGenre and Month\rMonth of airing might have an influence on the TV shows. Bar plot indicates that most of the TV shows are aired in the first quarter or last quarter of the year. Which means shows aired in the fall (September or October) or aired after winter break (January).\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(month(date))))+\rgeom_bar()+ coord_flip()+ labs(fill=\u0026quot;Month\u0026quot;)+\rxlab(\u0026quot;Genre\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Genre Over the Months\u0026quot;)+\rscale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))\r\r\rSeason\rTV shows may run for few seasons or more most of the time. Some are limited seasons close to four or above, but definitely less than 10. It is very rare to see TV shows going beyond the 15 seasons mark.\nSeason and Year\rClearly there is an increase in TV shows aired over the years. All time low occurs in 1992 but all time high occurs in 2017. From 1990 to 2010 the TV shows aired have increased from 25 to 100. By the end of year 2017 the number of shows aired has reached more than 250, but its drops to slightly above 175 the next year. The all time low of less than 12 seasons occurs in 1990.\nIn the years 1990,1996,2005,2007,2010,2011 and 2015 there are TV shows which has season above 30, but it should be reminded that according to the legend after season 20 there is no continuity.\nIf we focus closely until 2005 most of the TV shows have seasons up-to 10 , but after 2005 there are TV shows which aired season until 20. This shows the popularity of certain shows over three decades.\nggplot(Ratings,aes(x=factor(year(date)),fill=factor(seasonNumber)))+\rgeom_bar()+ coord_flip()+labs(fill=\u0026quot;Season\u0026quot;)+\rxlab(\u0026quot;Year\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Seasons over the Years\u0026quot;)+\rscale_y_continuous(breaks=seq(0,230,10),labels=seq(0,230,10))\r\rSeason and Month\rHighest amount of more than 500 shows were aired in January and lowest amount of slightly less than 100 was aired in June. Second place goes to February with shows close to 200 being aired this is not even the half of what aired in the previous month. In the months of January, February and September most of the seasons were aired, while in the other months the seasons aired are from the range of 1 to 10. Where very few of them ever reached the double digits or above season 8.\nggplot(Ratings,aes(x=factor(month(date)),fill=factor(seasonNumber)))+\rgeom_bar()+coord_flip()+labs(fill=\u0026quot;Season\u0026quot;)+\rxlab(\u0026quot;Month\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Seasons over the months\u0026quot;)+\rscale_y_continuous(breaks=seq(0,550,25),labels=seq(0,550,25))\r\r\rTop 3 Genres\rLet focus on the most mentioned genre types which are “Crime,Drama,Mystery”, “Comedy,Drama” and “Drama”. The below section is to graphically represent the TV shows of a certain genre with its seasons and when they were aired.\nTherefore I will not be factual, mostly biased towards the shows I watched and special characteristics.\nCrime Drama Mystery\rLaw and Order of 20 seasons has been over for more than 5 years and it began airing in 1990. Law and Order Special Victims Unit started airing in 1999 even now its still being aired. There are also odd shows like which has not aired continuously and skipped an year or two. Among them Columbo, Agatha Christie’s Marple and II commissario Montalbano are specially noted.\nThere are lot of shows which only aired one or two seasons only and then stopped. They also can be noted from the bar plot. In the legend there are colors to indicate all the years from 1990 to 2018.\nsubset(Ratings,genres==\u0026quot;Crime,Drama,Mystery\u0026quot;) %\u0026gt;%\rggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\rgeom_bar()+ coord_flip()+labs(color=\u0026quot;Year\u0026quot;,fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;TV shows\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;\u0026#39;Crime,Drama,Mystery\u0026#39; Genre type over the Years\u0026quot;)+\rscale_y_continuous(breaks=0:20,labels=0:20)\r\rComedy Drama\rSimilarly as above we can interpret the bar plot as well. But here the highest amount of seasons any TV show has reached is 9. Only the TV shows “Scrubs” and “Shameless” have reached that milestone and both of them begin in different decades. “Scrubs” began in early 2000s, but “Shameless” was aired after 2010.\nIn the years 1998 and 1999 there were no TV shows aired under the genre “Comedy,Drama”.\nsubset(Ratings,genres==\u0026quot;Comedy,Drama\u0026quot;) %\u0026gt;%\rggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\rgeom_bar()+ coord_flip()+labs(color=\u0026quot;Year\u0026quot;,fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;TV shows\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;\u0026#39;Comedy,Drama\u0026#39; Genre type over the Years\u0026quot;)+\rscale_y_continuous(breaks=0:9,labels=0:9)\r\rDrama\rMost of the TV shows in this genre type are limited to one season an a few more with 2 or 3 seasons. Even though most of them were aired after 2015. TV shows “Mad Men”, “Skins” and “The West Wing” has aired for 7 seasons.\nSome of these shows were limited series like “The News Room”. The only very early TV show is “Rebel Highway” which was aired in 1994 and in year 2004 “Summerland” was aired, where both of them were limited to one season.\nAccording to the legend the years 1992,1993,1995 to 1998 were years free of “Drama” genre TV shows.\nsubset(Ratings,genres==\u0026quot;Drama\u0026quot;) %\u0026gt;%\rggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\rgeom_bar()+ coord_flip()+labs(color=\u0026quot;Year\u0026quot;,fill=\u0026quot;Year\u0026quot;)+\rxlab(\u0026quot;TV shows\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;\u0026#39;Drama\u0026#39; Genre type over the Years\u0026quot;)+\rscale_y_continuous(breaks=0:7,labels=0:7)\r\r\rRating over the years\rLets discuss regarding TV shows and their rating over the years with related to sharing. Clearly we can see the number of shows increasing from 1990 to 2018. Oddly there was more sharing related to TV shows before 2000, but this is not the case after the birth of Millenium. To be honest sharing becomes more extinct even with the power of Internet and smart phones.\nIn perspective of rating the range is very much centered and small(between 7.5 - 9), but over the years this changes and expands to a wider range.(6.5 - 9.5). Only a handful of TV shows are rated below 5 in the scale over the year span of 28.\np\u0026lt;-ggplot(Ratings,aes(x=factor(year(date)),y=av_rating,color=genres,size=share))+\rgeom_point(show.legend = FALSE)+ labs(title = \u0026quot;Ratings and Sharing : {frame_time}\u0026quot;\r,x=\u0026quot;Year\u0026quot;,y=\u0026quot;Rating\u0026quot;)+\rscale_y_continuous(breaks=2:10,labels=2:10)+\rtransition_time(date)+ease_aes(\u0026#39;linear\u0026#39;)+\rtheme(axis.text.x = element_text(angle = 90))+\rshadow_mark()\ranimate(p,fps= 5,duration =60)\r\rTv Series with more than 14 Seasons\rThis is simply me trying to focus on the TV shows which has seasons more than 14 and their rating, sharing changes over the years.\np\u0026lt;-subset(Ratings,title==\u0026quot;CSI: Crime Scene Investigation\u0026quot;|\rtitle==\u0026quot;ER\u0026quot;| title==\u0026quot;Grey\u0026#39;s Anatomy\u0026quot;| title==\u0026quot;Midsomer Murders\u0026quot;| title==\u0026quot;Law \u0026amp; Order\u0026quot;|\rtitle==\u0026quot;Law \u0026amp; Order: Special Victims Unit\u0026quot;) %\u0026gt;%\rggplot(aes(x=seasonNumber,y=av_rating,color=title,size=share))+ geom_point()+\rlabs(title = \u0026#39;Season and Rating Year: {frame_time}\u0026#39;,\rx=\u0026quot;Season\u0026quot;,y=\u0026quot;Rating\u0026quot;)+\rscale_x_continuous(breaks=1:20,labels=1:20)+\r#scale_y_continuous(breaks=)\rtransition_time(date)+ease_aes(\u0026#39;linear\u0026#39;)+\rshadow_mark()+theme(legend.position = \u0026quot;bottom\u0026quot;)\ranimate(p,fps=5,duration = 60)\rTHANK YOU\n\r","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"4637d85d2e9b8b4cf76ce47245bcedfc","permalink":"/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/post/tidytuesday2019/week2/week-2-imdb-tv-shows-data/","section":"post","summary":"2019 Week 2 TidyTuesday: Imdb Tv Shows.","tags":["TidyTuesday","R","R package","tidyverse","gganimate"],"title":"Week 2: IMDB TV Shows Data","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r\r#Tidytuesday Tweets\rEarliest Tweet\rAny Verified Profiles ?\rSource of Tweets\rTweets Per Month\rMost Tweets By Screen Name\rMost Tweets By Screen Name and their Source\rMost Tweets By Screen Name with their Retweet Counts\rMost Tweets By Screen Name with their Favorite Counts\r\rRelationship between Favorite Counts vs Retweet Counts ?\rRelationship between Followers Count vs Friends Count ?\r\rConclusion\rFurther Analysis\r\r\r# load the necessary packages\rlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(kableExtra)\rlibrary(ggthemr)\r#load the ggthemr\rggthemr(\u0026quot;flat dark\u0026quot;)\r# load the data set\rtidytuesday_tweets\u0026lt;-readRDS(\u0026quot;tidytuesday_tweets.rds\u0026quot;)\r#Tidytuesday Tweets\rUsing plots and Tables to express the #TidyTuesday data-set. You can obtain the dataset from here.\nFavorite count vs Retweet count. Code: https://t.co/QJwXxzrkFG #TidyTuesday #tidyverse pic.twitter.com/l14pHDS3kq\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) January 1, 2019  GitHub Code\nEarliest Tweet\rThe first tweet is on April 2nd and it has 156 favorites and 64 retweets, where the tweet is from Thomas Mock and the next 3 tweets are also from him.\ntidytuesday_tweets[order(tidytuesday_tweets$created_at),c(3,4,13,14,71)] %\u0026gt;%\rhead(5) %\u0026gt;%\rkable() %\u0026gt;%\rkable_styling()\r\r\rcreated_at\r\rscreen_name\r\rfavorite_count\r\rretweet_count\r\rname\r\r\r\r\r\r2018-04-02 21:35:08\r\rthomas_mock\r\r156\r\r64\r\rThomas Mock\r\r\r\r2018-04-02 21:35:10\r\rthomas_mock\r\r6\r\r0\r\rThomas Mock\r\r\r\r2018-04-02 21:35:11\r\rthomas_mock\r\r4\r\r0\r\rThomas Mock\r\r\r\r2018-04-02 23:31:11\r\rthomas_mock\r\r2\r\r0\r\rThomas Mock\r\r\r\r2018-04-03 00:25:51\r\rumairdurrani87\r\r6\r\r2\r\rUmair Durrani\r\r\r\r\r\rAny Verified Profiles ?\rThere are only 3 verified profiles where Hadley Wickham has the highest amount of followers with 76469, where that tweet has 61 favorites but no retweets. Other profiles are Civis Analytics and grspur, but both of them have friends above 600 counts, but Hadley Wickham friends count is close to 290.\nsubset(tidytuesday_tweets[,c(4,13,14,76,77,82)],verified==TRUE) %\u0026gt;%\rkable() %\u0026gt;%\rkable_styling()\r\r\rscreen_name\r\rfavorite_count\r\rretweet_count\r\rfollowers_count\r\rfriends_count\r\rverified\r\r\r\r\r\rCivisAnalytics\r\r3\r\r1\r\r6880\r\r658\r\rTRUE\r\r\r\rhadleywickham\r\r61\r\r0\r\r76469\r\r288\r\rTRUE\r\r\r\rgrspur\r\r14\r\r6\r\r857\r\r623\r\rTRUE\r\r\r\r\r\rSource of Tweets\rClose to 1050 tweets are done by the web client and other clients such as Android and Iphone have tweet counts of respectively 106 and 233. Other sources include oddly Instagram, Facebook, WordPress and LinkedIn, which I am naming because of their popularity.\nggplot(tidytuesday_tweets,aes(fct_infreq(source)))+\rgeom_bar()+coord_flip()+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),hjust=-0.25)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Types of Sources\u0026quot;)+\rggtitle(\u0026quot;Source of Tweets\u0026quot;)\r\rTweets Per Month\rBeginning of #TidyTuesday we have 293 tweets on the month of April. Even though over the next months the number of tweets are decreasing this is not the case in October. Lowest number of tweets are recorded in September with 115 tweets.\nggplot(tidytuesday_tweets,\raes(x=month(tidytuesday_tweets$created_at)))+\rgeom_bar()+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),vjust=-0.15)+\rscale_x_continuous(breaks = seq(1,12),labels = seq(1,12))+\rylab(\u0026quot;Frequency\u0026quot;)+ xlab(\u0026quot;Months\u0026quot;)+\rggtitle(\u0026quot;Tweet Counts By Month\u0026quot;)\r\rMost Tweets By Screen Name\rThere are 30 twitter users if we consider the accounts that have tweeted more than or equal to 10 tweets under the hashtag “TidyTuesday”. Thomas Mock has tweeted most which is 172 including retweets, and the second place goes to R4DScommunity with 92 tweets. All the other users have individually less than 40 tweets.\ntidytuesday_tweets %\u0026gt;%\rgroup_by(screen_name) %\u0026gt;%\rfilter(n() \u0026gt;= 10) %\u0026gt;%\rggplot(aes(x=fct_infreq(screen_name)))+\rgeom_bar()+ coord_flip()+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),hjust=-0.15)+\rylab(\u0026quot;Frequency\u0026quot;)+ xlab(\u0026quot;Screen Name\u0026quot;)+\rggtitle(\u0026quot;Screen Name with Most Tweets\u0026quot;)\rMost Tweets By Screen Name and their Source\rFor the same plot if we consider the source for the tweets, it is clear that only seven sources were used. Mostly all of these users are using the web client, but some are using the iPhone as well. R4DS community does more tweeting through iPhone than TweetDeck. TweetDeck is a simple way of handling multiple twitter accounts at the same time. Tidyyourworld account only uses Android and WeAreRLadies uses only TweetDeck.\ntidytuesday_tweets %\u0026gt;%\rgroup_by(screen_name) %\u0026gt;%\rfilter(n() \u0026gt;= 10) %\u0026gt;%\rggplot(aes(x=fct_infreq(screen_name),fill=source))+\rgeom_bar(position = \u0026quot;stack\u0026quot;,stat=\u0026quot;count\u0026quot;)+ coord_flip()+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),hjust=1,\rposition = position_stack())+\rylab(\u0026quot;Frequency\u0026quot;)+ xlab(\u0026quot;Screen Name\u0026quot;)+\rggtitle(\u0026quot;Screen Name with Most Tweets and their Source\u0026quot;)\r\rMost Tweets By Screen Name with their Retweet Counts\rOf the Top 30 users with most amount of tweets the highest amount of retweets is to a tweet from WeAreRLadies and it is 95. There are more outliers from Thomas Mock. and the highest range is to the user drob. Most from this top 30 users have the range between 0 and 10.\ntidytuesday_tweets[,c(\u0026quot;screen_name\u0026quot;,\u0026quot;retweet_count\u0026quot;)] %\u0026gt;%\rgroup_by(screen_name) %\u0026gt;%\rfilter(n() \u0026gt;= 10) %\u0026gt;%\rggplot(.,aes(x=fct_infreq(screen_name),y=retweet_count))+\rgeom_boxplot()+ coord_flip()+\rscale_y_continuous(breaks = seq(0,100,5),labels = seq(0,100,5))+\rylab(\u0026quot;Retweets\u0026quot;)+ xlab(\u0026quot;Screen Name\u0026quot;)+\rggtitle(\u0026quot;Screen Name with Most Tweets and their Retweets Count\u0026quot;)\r\rMost Tweets By Screen Name with their Favorite Counts\rSimilarly Thomas Mock has more outliers, and the highest range is to the user drob. Second place for outliers goes to R4DScommunity user. Close to 500 favorites are counted to a tweet by drob and second place is to a tweet by WeAreRladies with favorite counts slightly above 450.\ntidytuesday_tweets[,c(\u0026quot;screen_name\u0026quot;,\u0026quot;favorite_count\u0026quot;)] %\u0026gt;%\rgroup_by(screen_name) %\u0026gt;%\rfilter(n() \u0026gt;= 10) %\u0026gt;%\rggplot(.,aes(x=fct_infreq(screen_name),y=favorite_count))+\rgeom_boxplot()+ coord_flip()+\rscale_y_continuous(breaks = seq(0,500,25),labels = seq(0,500,25))+\rylab(\u0026quot;Favourites Count\u0026quot;)+ xlab(\u0026quot;Screen Name\u0026quot;)+\rggtitle(\u0026quot;Screen Name with Most Tweets and their Favourties Count\u0026quot;)\r\r\rRelationship between Favorite Counts vs Retweet Counts ?\rVery clear positive correlation. Y scale ranges from 0 to 500, where x scale range is from 0 to 100 and most of the data points are centered around the range of 0 to 12 retweets and 0 to 60 Favorites. A Few data data points are out of the above mentioned range.\nggplot(tidytuesday_tweets, aes(x=retweet_count,y=favorite_count))+\rgeom_point()+geom_smooth()+\rscale_x_continuous(breaks =seq(0,100,2) ,labels =seq(0,100,2))+\rscale_y_continuous(breaks =seq(0,500,10),labels =seq(0,500,10))+\rxlab(\u0026quot;Retweets\u0026quot;)+ylab(\u0026quot;Likes\u0026quot;)+\rggtitle(\u0026quot;Retweets Versus Likes\u0026quot;)\r\rRelationship between Followers Count vs Friends Count ?\rHere I have considered only twitter profiles which has followers count less than 5000 with friends count also less than 5000. The reason is to explain the relationship more clearly. Clearly most of the twitter profiles are having followers less than 2000 with friends also less than 2000. Clearly there are some profiles with Followers count above 1000 but friends count less than 1000. Even though there are few profiles with less than 1000 followers but more than 1000 Friends.\nggplot(subset(tidytuesday_tweets,\rfollowers_count \u0026lt; 5000 \u0026amp; friends_count \u0026lt; 5000), aes(x=followers_count,y=friends_count))+\rgeom_point()+geom_smooth()+\rscale_x_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+\rscale_y_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+\rxlab(\u0026quot;Followers Count\u0026quot;)+ylab(\u0026quot;Friends Count\u0026quot;)+\rggtitle(\u0026quot;Followers Count Versus Friends Count\u0026quot;)\r\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rUsing tidyverse as usual is fun.\n\rThe Box plots for several variables in the same plot is easy for the use of comparison.\n\rThe Scatter plots are nice to understand the relationship among two continuous variables.\n\rThe geom_smooth function is also very useful in modelling the data.\n\r\r\rFurther Analysis\r\rWe can focus on the text variable which could be used for a word cloud.\n\rFurther we can try to understand the hashtags with favorites and retweets.\n\r\rTHANK YOU\n\r","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"594ab394cfcd43564816d08e7109ecb7","permalink":"/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/post/tidytuesday2019/week1/week-1-tidytuesday-tweets/","section":"post","summary":"2019 Week 1 of TidyTuesday: #TidyTuesday and #rstats.","tags":["R","R package","TidyTuesday"],"title":"2019 Week 1 : #TidyTuesday Tweets ","type":"post"},{"authors":null,"categories":["R"],"content":"\rIntroduction\rmle\rmle2\r\rConclusion\r\r\rIntroduction\rmle and mle2 are my favorite functions, because they provide extensive amount of outputs for the optimization process. Even though there is no difference in analytical methods used in both of these functions. Further, these analytical methods are the same ones used by optim function. To be honest mle and mle2 functions are wrapper functions of optim. It means both mle and mle2 are using the optim function inside but with some additional inputs, which would generate extended outputs.\nEven if I do Benchmark the analytical methods for the mle function it would be very similar to optim function tables but with additional time taken, because of the extra outputs. This would similarly occur when we benchmark analytical methods from the mle2 function as well.\nTherefore, I figure why do we need to benchmark them at all. So this blog post is to simply reiterate the initial things which I said in my earlier post on the blog post Benchmarking optimization functions in R.\nmle\rmle function is from the stats4 package. If we intend to use this function for the estimation of shape parameters a and b of the Beta-Binomial distribution wtih Binomial Outcome Data, then we need to use the EstMLEBetaBin function from the fitODBOD package. This is not enough because for limitations in the mle we need to make changes in our EstMLEBetaBin function as mentioned below.\nlibrary(stats4)\rlibrary(fitODBOD)\r#new function to facilitate mle criteria formle\u0026lt;-function(a,b)\r{\rEstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)\r}\r# optimizing values for a,b using default analytial method\rmle_answer\u0026lt;-mle(minuslogl = formle,start = list(a=0.1,b=0.2))\rWe are going to use the Alcohol Consumption data of week 1. In the above code chunk we are using the mle function for our task of finding the optimum shape parameter values for a and b while using the given Binomial Outcome data. Also If you wish you study about the mle function by referring this link from my previous post.\n\rmle2\rbbmle package holds the mle2 function. It is simply an updated version for the mle function. Although there need to be no changes in the EstMLEBetaBin function to satisfy the mle2 function’s criteria. Now it will be possible to use it.\nlibrary(bbmle)\r# optimizing values for a,b using default analytical method\rmle2_answer\u0026lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),\rdata = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))\rStill if someone needs a brief introduction to mle2 function they can refer my previous brief through this link.\n\r\rConclusion\rMy personal opinion is to use the mle2 function, but moving towards what should be the analytical method. It would be wise to choose it based on your needs as these methods completely depend on the data, function that needs to be estimated, complexity of the function and finally the number estimators that needs to be estimated.\nThis is the link to the article which is for Benchmarking optim function. It might be useful while understanding the analytical methods.\nTHANK YOU\n\r","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546041600,"objectID":"6cd03ea51d629db87819ce08453566d3","permalink":"/post/mleand2/benchmarking-the-mle-and-mle2-function/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/post/mleand2/benchmarking-the-mle-and-mle2-function/","section":"post","summary":"Using different analytical methods from the mle and mle2 optimizing function.","tags":["bbmle","fitODBOD","mle","mle2","R","R package"],"title":"Benchmarking the mle and mle2 function ","type":"post"},{"authors":null,"categories":["R"],"content":"\rIntroduction\rMost Essential Packages\rdevtools\rpkgbuild\rpkgload\rrcmdcheck\rusethis\r\rroxygen2\rknitr\rmarkdown, rmarkdown, rmdformats\rspelling\rtrackmd\r\rtestthat\r\rEssential Packages\rgit2r and gh\rdesc\rcovr\rbadgecreatr and badger\rhexSticker\rpkgdown\r\rStill I have not Used\rpackrat\rpkgconfig\rpkginspector\rrvcheck\rrversions\rformatR\rwhoami\r\rConclusion\r\r\rIntroduction\rPackage development is a sense of accomplishment for any statistical programmer who needs self satisfaction. I developed the R package fitODBOD for the purpose of fitting Over dispersed Binomial Outcome Data using Binomial Mixture Distributions and Alternate Binomial Distributions. It was a an amazing journey learning how to develop an R package, which took me around 6 months while understanding the theoretical aspects of my research project and doing my 4th year courses.\nI am still learning new things related to R, which is helpful for this R package development. Making package version updates regularly is for the benefit of the user. I have learned new ways to express the theoretical concepts in the simplest form of functions, classes and methods. Currently, I am exploring the possibility of using Rshiny dashboard and GUI.\nIn the beginning, R package developers have used manual techniques (which mean difficult techniques)\nto develop R functions, documentation and examples for their packages. Over time it has changed rapidly, where currently we are using R packages to develop our own R package. In this post I shall briefly mention these packages which you can use. Using these packages it is possible to make package development stress free, time efficient and objective effective. Simultaneously we can make our R packages more attractive for the users, which would lead to lot of attention in the R community.\nThere are three types of packages in my perspective, first “Most Essential Packages” which cannot be ignored, second “Essential packages” it is your choice to ignore and finally, “Still I have not Used” packages.\n\rMost Essential Packages\rThere are three packages in my main interest list and they are devtools, roxygen2 and testhat.\ndevtools\rCollection of packages which would significantly help the package development process. Functions such as dev_mode, check_failures, check_win and check_man.\nLink for the package\npkgbuild\rLocates compilers needed to build R packages on various platforms.\nLink for the package\n\rpkgload\rSimulate the process of installing a package and then attacking it.\nLink for the package\n\rrcmdcheck\rRun “R CMD check” from R programmaticallly, and capture the results of the individual checks.\nLink for the package\n\rusethis\rAutomating few tasks related to package building.\nLink for the package\n\r\rroxygen2\rGenerate Rd documentation, and Namespace file with simplicity which would save time, when package update occurs.\nLink for the package\nknitr\rUseful to develop vignettes related to R package development.\nLink for the package\n\rmarkdown, rmarkdown, rmdformats\rHtml formats to vignettes in R package development and special template styles for the vignettes.\nmarkdown rmarkdown rmdformats\n\rspelling\rChecking for spelling issues in Rd documentation files.\nLink for the package\n\rtrackmd\rTracking changes in markdown files for vignette.\nLink for the package\n\r\rtestthat\rChecking if functions work properly by testing them in multiple ways for errors, outputs and inputs.\nLink for the package\n\r\rEssential Packages\rWe would not necessarily need these packages to develop our package, but it would make things more official if we choose to use them. Creating official badges, using GitHub for version control, checking for code coverage, distinct logo and having a website to explore the functions and vignettes of the package.\ngit2r and gh\rAccess to GitHub so that version control would occur smoothly with integration in Rstudio.\ngit2r gh\n\rdesc\rEditing the Description file using package rather than manually editing the file.\nLink to package\n\rcovr\rChecking code coverage, which means does all functions have examples and are there tests for error messages, etc.\nLink to package\n\rbadgecreatr and badger\rAdding badges to GitHub repository, for example Download, CRAN status, code coverage, Release date, version and much more.\nbadgecreatr badger\n\rhexSticker\rCreating a hexagon sticker for your package. Mostly just for the fun, but in a while its like promoting a brand.\nLink to package\n\rpkgdown\rUsing man files, vignette of your package to develop a static website. Further, it is possible to promote this site to get more people interested in the package.\nLink to package\n\r\rStill I have not Used\rIn this list I will explore packages which can be used to make package development more simple and elegant. Mostly using functions for the tasks of proper code spaces, indent and necessary R versions of dependency packages.\npackrat\rManage the R packages in an isolated, portable and reproducible way.\nLink to package\n\rpkgconfig\rSet configuration options on a per-package basis.\nLink to package\n\rpkginspector\rUnderstand internal structure of an R package.\nLink to package\n\rrvcheck\rCheck latest release version of R and R packages.\nLink to package\n\rrversions\rFocusing on R version ‘r-release’ and ‘r-oldrel’. Further all previous R versions.\nLink to package\n\rformatR\rSpaces and Indent for the code automatically added\nLink to package\n\rwhoami\rUsername and full-name of current user, also email address and GitHub username.\nLink to package\n\r\rConclusion\rHopefully we would have more packages and awareness towards R package development with more simplicity in the coming years. Even though we have more than 15,000 packages in CRAN it would still rapidly increase in the coming years, not only in CRAN but also in GitHub as well.\nSo far this post has only word content and links, therefore I am adding a screenshot of my fitODBOD package GitHub ReadME.md file. This screen shot includes badges for downloads, R version, published date, package version and a hexagon logo sticker.\n\r\rI hope this blog post is useful for anyone who has intentions to develop their R package or in the process of development or version controlling. I developed fitODBOD as a research project for my final year and it included a thesis report as well. Over time I wrote a Journal article as well and it is still under review. Also I have seen R package development as a PhD submission as well. Therefore it would be worthwhile developing an R package as a way to keep an active status regarding your field of interest in Statistics.\nTHANK YOU\n\r","date":1545523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545523200,"objectID":"39bcdc5bbd8466e5a1174e5bb2d074d7","permalink":"/post/newpackage/build-a-new-package-with-existing-r-packages/","publishdate":"2018-12-23T00:00:00Z","relpermalink":"/post/newpackage/build-a-new-package-with-existing-r-packages/","section":"post","summary":"Detailed list of packages to help package development.","tags":["R","R package"],"title":"Build a New Package with Existing R packages","type":"post"},{"authors":null,"categories":["R"],"content":"\r1. Google\r2. CRAN\r3. Bio - Conductor\r4. GitHub pages\r5. Rdocumentation\r6. Crantastic\r7. rpackages\r8. R - Opensci\r9. Rseek\r10. R Site Search\r11. R-forge\r12. AwesomeR\r13. CRAN Task View\r14. Rstudio - Rpackages\r15. stack overflow - r\r16. CRANalerts\r\r\rHow to find your R package is simply a blog post helping people to provide a list of websites where they can find R packages. These websites were useful for me while developing my own R package fitODBOD. So that I would be sure that fitODBOD is a unique package and what its functions should be able to do.\nThis is a list with 16 items\n1. Google\rWhen you have no idea to find a package first thing is to “Google”.\nLink \r2. CRAN\rOfficial website to find standard packages. The packages downloaded here will have documentation manuals, vignettes and sometimes journal articles which would simplify work for people who use them.\nLink \r3. Bio - Conductor\rAnother standard location to publish your R package, but only related to the field of Biology.\nLink \r4. GitHub pages\rIf CRAN or Bio - Conductor is with high standards or too much work for your package you can still publish it and the ideal place for this is GitHub.\nLink \r5. Rdocumentation\rA place to find interactive documentation for the packages in CRAN, Bio - Conductor and GitHub. They simply include everything in the manual of a package but in html format.\nLink \r6. Crantastic\rAll packages which are a part of CRAN is in this website. We can search packages based on Authors, package name, reviews and tags.\nLink \r7. rpackages\rSimilar to crantastic this website also provides information to CRAN packages, but it is better because package related statistics is also shown here.\nLink \r8. R - Opensci\rSearch range for R packages in this website has more categories which is informative. I would say better than above mentioned ones.\nLink \r9. Rseek\rThis is like a google search engine for R packages.\nLink \r10. R Site Search\rWebsite dedicated to search R functions, package vignettes and task views.\nLink \r11. R-forge\rProjects related to R are mentioned in this website and how progress has been made on them is also here. Most of these projects will be published as packages later with significant importance.\nLink \r12. AwesomeR\rThis is a website which has R packages based on topics related to statistics. Some of these topics are Machine Learning, Bayesian, Optimization, Bio statistics and much more.\nLink \r13. CRAN Task View\rTopic related R packages are bundled together in this website. Further, the topics give a brief explanation, but the webpages give an extensive amount of information about what is unique is these packages. Also you do not need internet to use this because it is part of Rstudio help.\nLink \r14. Rstudio - Rpackages\rSeveral crucial packages which would be very useful are considered here. All of these packages are projects. Further they are very popular in the R community.\nLink \r15. stack overflow - r\rIf you cannot achieve something very specific related to R coding it is possible to use the website. It provides answers from other R users, sometimes even blogs related to the issues with solutions.\nLink \r16. CRANalerts\rThis is an email service which would alert us regarding specific packages accordance to our request. Whenever there is an update for a chosen package we would receive an email alert.\nLink This is my list of places for reaching out to help with related to R packages and R programming. I use them constantly and they are very much helpful to me. Finally, I hope this post would be useful to anyone who wants to find or use R packages.\nTHANK YOU\n\r","date":1545436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545436800,"objectID":"fc56f69128a20392885d9ebabbed3eb3","permalink":"/post/findrpackage/how-to-find-your-r-package/","publishdate":"2018-12-22T00:00:00Z","relpermalink":"/post/findrpackage/how-to-find-your-r-package/","section":"post","summary":"List of possible places where you can find R packages for your specific needs.","tags":["R","R package"],"title":"How To Find Your R package ?","type":"post"},{"authors":null,"categories":["fitODBOD"],"content":"\r\rEstimating the shape parameters of Beta-Binomial Distribution\rIntroduction\rBrief of maxLik Function\rNR method\rBFGS method\rBFGSR method\rBHHH method\rSANN method\rCG method\rNM method\r\rSumary of Time evalutation for different Analytical methods of maxLik function\rSummary of results after using the maxLik function for different analytical methods\rFinal Conclusion\r\r\rEstimating the shape parameters of Beta-Binomial Distribution\r\rIntroduction\rBeginning of this month I wrote a small section regarding maxLik function by comparing it to other optimization functions. Here, we will further study the analytical methods which can be used in this function and compare them to find suitability. maxLik function is from the package maxLik. Further, Documentation clearly indicates all things related to the function in detail.\nFocusing on the seven analytical methods is my intention from this blog post. So, we have the Beta-Binomial distribution and Binomial Outcome data, and need to estimate proper shape parameters which would Maximize the Log Likelihood value of the Beta-Binomial distribution for the Alcohol Consumption data. In this case Alcohol Consumption data from the fitODBOD package will be used.\nFurther we will focus on the process time to optimization, estimated shape parameters, maximized Log Likelihood value, expected frequencies, p-value and Over-dispersion with tables.\nBelow are the seven analytical methods in concern\nNR\rBFGS\rBFGSR\rBHHH\rSANN\rCG\rNM\r\rAlcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.\nlibrary(fitODBOD)\rkable(Alcohol_data,\u0026quot;html\u0026quot;,align=c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 14,full_width = F) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\rDays\r\rweek1\r\rweek2\r\r\r\r\r\r0\r\r47\r\r42\r\r\r\r1\r\r54\r\r47\r\r\r\r2\r\r43\r\r54\r\r\r\r3\r\r40\r\r40\r\r\r\r4\r\r40\r\r49\r\r\r\r5\r\r41\r\r40\r\r\r\r6\r\r39\r\r43\r\r\r\r7\r\r95\r\r84\r\r\r\r\rBrief of maxLik Function\rSmall section about the maxLik function will be very useful to understand this blog post.\nReference : Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood estimation in R Computational Statistics 26, 443–458 Marquardt, D.W., (1963)\nAn Algorithm for Least-Squares Estimation of Nonlinear Parameters, Journal of the Society for Industrial \u0026amp; Applied Mathematics 11, 2, 431–441\nSo for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would maximize the Log Likelihood value of the Beta-Binomial distribution.\nFirst we are transforming the given EstMLEBetaBin function to satisfy the maxLik function conditions.\n# new function to facilitate maxLik criteria\r# only one input but has two elements\rformaxLik\u0026lt;-function(a)\r{\r-EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\r}\rSo the formaxLik function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further the maxLik function can be scrutinized as below.\n\rpackage : maxLik\rNo of Inputs: 6\rMinimum required Inputs : 2\rClass of output : list or class of maxim or class of maxLik\rNo of outputs: 11\rNo of Analytical Methods : 7\rDefault Method : Automatically chosen\r\r\rNR method\rNR is an abbreviation for Unconstrained and equality-constrained maximization based on the quadratic approximation (Newton) method. The idea of the Newton method is to approximate the function at a given location by a multidimensional quadratic function, and use the estimated maximum as the start value for the next iteration.\nlibrary(maxLik)\r# optimizing values for a,b using NR analytical method\rNR_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;NR\u0026quot;)\r# obtaining class of output\rclass(NR_answer)\r# length of output\rlength(NR_answer)\r# the outputs\rNR_answer$estimate # estimated values for a, b\rNR_answer$maximum # minimized function value NR_answer$iterations # no of iterations to succeed\rNR_answer$gradient # last gradient value which was calculated\rNR_answer$message # additional information\rNR_answer$hessian # hessian matrix\rNR_answer$code # indicates successful completion\rNR_answer$fixed # logical vector indicating which parameters are constants\rNR_answer$type # type of maximization\rNR_answer$last.step # list describing the last unsuccessful step\rNR_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rNR_answer$estimate[1],NR_answer$estimate[2])\r\rBFGS method\rBFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\n# optimizing values for a,b using BFGS analytical method\rBFGS_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;BFGS\u0026quot;)\r# obtaining class of output\rclass(BFGS_answer)\r# length of output\rlength(BFGS_answer)\r# the outputs\rBFGS_answer$estimate # estimated values for a, b\rBFGS_answer$maximum # minimized function value BFGS_answer$iterations # no of iterations to succeed\rBFGS_answer$gradient # last gradient value which was calculated\rBFGS_answer$message # additional information\rBFGS_answer$hessian # hessian matrix\rBFGS_answer$code # indicates successful completion\rBFGS_answer$fixed # logical vector indicating which parameters are constants\rBFGS_answer$type # type of maximization\rBFGS_answer$last.step # list describing the last unsuccessful step\rBFGS_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rBFGS_answer$estimate[1],BFGS_answer$estimate[2])\r\rBFGSR method\rCombination of two methods which are Newton-Raphson, BFGS (Broyden 1970, Fletcher 1970, Goldfarb 1970, Shanno 1970).\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\n# optimizing values for a,b using BFGSR analytical method\rBFGSR_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;BFGSR\u0026quot;)\r# obtaining class of output\rclass(BFGSR_answer)\r# length of output\rlength(BFGSR_answer)\r# the outputs\rBFGSR_answer$estimate # estimated values for a, b\rBFGSR_answer$maximum # minimized function value BFGSR_answer$iterations # no of iterations to succeed\rBFGSR_answer$gradient # last gradient value which was calculated\rBFGSR_answer$message # additional information\rBFGSR_answer$hessian # hessian matrix\rBFGSR_answer$code # indicates successful completion\rBFGSR_answer$fixed # logical vector indicating which parameters are constants\rBFGSR_answer$type # type of maximization\rBFGSR_answer$last.step # list describing the last unsuccessful step\rBFGSR_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rBFGSR_answer$estimate[1],BFGSR_answer$estimate[2])\r\rBHHH method\rBHHH method (Berndt, Hall, Hall, Hausman 1974). The BHHH (information equality) approximation is only valid for log-likelihood functions. It requires the score (gradient) values by individual observations and hence those must be returned by individual observations by grad or fn. With the complexity of BHHH method I choose not to discuss it here, but a reference is mentioned to anyone who has interest in this analytical method.\nReference : Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974): Estimation and Inference in Nonlinear Structural Models, Annals of Social Measurement 3, 653–665.\n\rSANN method\rMethod SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.\nBy default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to \\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\\), where \\(t\\) is the current iteration step and temp and tmax are specifiable via control.\nNote that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.\nReference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.\n# optimizing values for a,b using SANN analytical method\rSANN_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;SANN\u0026quot;)\r# obtaining class of output\rclass(SANN_answer)\r# length of output\rlength(SANN_answer)\r# the outputs\rSANN_answer$estimate # estimated values for a, b\rSANN_answer$maximum # minimized function value SANN_answer$iterations # no of iterations to succeed\rSANN_answer$gradient # last gradient value which was calculated\rSANN_answer$message # additional information\rSANN_answer$hessian # hessian matrix\rSANN_answer$code # indicates successful completion\rSANN_answer$fixed # logical vector indicating which parameters are constants\rSANN_answer$type # type of maximization\rSANN_answer$last.step # list describing the last unsuccessful step\rSANN_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rSANN_answer$estimate[1],SANN_answer$estimate[2])\r\rCG method\rMethod CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak-Ribiere or Beale-Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.\nReference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.\n# optimizing values for a,b using CG analytical method\rCG_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;CG\u0026quot;)\r# obtaining class of output\rclass(CG_answer)\r# length of output\rlength(CG_answer)\r# the outputs\rCG_answer$estimate # estimated values for a, b\rCG_answer$maximum # minimized function value CG_answer$iterations # no of iterations to succeed\rCG_answer$gradient # last gradient value which was calculated\rCG_answer$message # additional information\rCG_answer$hessian # hessian matrix\rCG_answer$code # indicates successful completion\rCG_answer$fixed # logical vector indicating which parameters are constants\rCG_answer$type # type of maximization\rCG_answer$last.step # list describing the last unsuccessful step\rCG_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rCG_answer$estimate[1],CG_answer$estimate[2])\r\rNM method\rNM is the abbreviation to Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.\nReference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.\n# optimizing values for a,b using NM analytical method\rNM_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \u0026quot;NM\u0026quot;)\r# obtaining class of output\rclass(NM_answer)\r# length of output\rlength(NM_answer)\r# the outputs\rNM_answer$estimate # estimated values for a, b\rNM_answer$maximum # minimized function value NM_answer$iterations # no of iterations to succeed\rNM_answer$gradient # last gradient value which was calculated\rNM_answer$message # additional information\rNM_answer$hessian # hessian matrix\rNM_answer$code # indicates successful completion\rNM_answer$fixed # logical vector indicating which parameters are constants\rNM_answer$type # type of maximization\rNM_answer$last.step # list describing the last unsuccessful step\rNM_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rNM_answer$estimate[1],NM_answer$estimate[2])\r\r\rSumary of Time evalutation for different Analytical methods of maxLik function\rBelow table will compare the system process time for different analytical methods. In order to do this time comparison it is possible to use the benchmark function of rbenchmark package. Below mentioned code chunk provides the output in a table form which includes the analytical methods and their respective time values. The estimation process of the parameters where each method has been replicated 1000 times to receive a more accurate table for time values.\nTable is in the ascending order for elapsed time column. It is evidently clear that SANN analytical method has taken the most time. Before that the analytical method BFGSR is in 5th place. While NM or Nelder Mead method has taken the least time. This time is calculated for 1000 replications of the function being repeated under same conditions. These times does not only reflect based on analytical method, rather on the Log Likelihood function that needs to be maximized, the data provided, the number of estimated that needs to be estimated, the complexity of the function and finally the computer’s processing power.\nlibrary(rbenchmark)\rResults1\u0026lt;-benchmark(\r\u0026quot;NR\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;NR\u0026quot;)},\r\u0026quot;BFGS\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;BFGS\u0026quot;)},\r\u0026quot;BFGSR\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;BFGSR\u0026quot;)},\r\u0026quot;SANN\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;SANN\u0026quot;)},\r\u0026quot;CG\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;CG\u0026quot;)},\r\u0026quot;NM\u0026quot;={maxLik(logLik=formaxLik, start = c(0.1,0.2), method = \u0026quot;NM\u0026quot;)},\rreplications = 1000,\rcolumns = c(\u0026quot;test\u0026quot;,\u0026quot;replications\u0026quot;,\u0026quot;elapsed\u0026quot;,\r\u0026quot;relative\u0026quot;,\u0026quot;user.self\u0026quot;,\u0026quot;sys.self\u0026quot;),\rorder = \u0026#39;elapsed\u0026#39;\r)\rkable(Results1,\u0026quot;html\u0026quot;,align = c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(full_width=T,bootstrap_options=c(\u0026quot;striped\u0026quot;),font_size = 14)%\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\r\rtest\r\rreplications\r\relapsed\r\rrelative\r\ruser.self\r\rsys.self\r\r\r\r\r\r6\r\rNM\r\r1000\r\r16.65\r\r1.000\r\r16.65\r\r0.00\r\r\r\r2\r\rBFGS\r\r1000\r\r29.75\r\r1.787\r\r29.74\r\r0.00\r\r\r\r1\r\rNR\r\r1000\r\r36.69\r\r2.204\r\r36.58\r\r0.03\r\r\r\r5\r\rCG\r\r1000\r\r64.42\r\r3.869\r\r64.39\r\r0.00\r\r\r\r3\r\rBFGSR\r\r1000\r\r629.26\r\r37.793\r\r628.61\r\r0.06\r\r\r\r4\r\rSANN\r\r1000\r\r1767.41\r\r106.151\r\r1754.92\r\r0.53\r\r\r\r\r\rSummary of results after using the maxLik function for different analytical methods\rEstimated the shape parameters a,b pair wise from the analytical methods NR, BFGS, BFGSR, SANN, CG and NM. These estimated parameters will be now used in the fitBetaBin function to find the expected frequencies, p-values and over-dispersion. The above measurements can be used to compare each analytical method for any significance difference.\nComparing p-values it is clear that all analytical methods generate the same value up-to third decimal point. This is not the case in Maximum Log Likelihood value where analytical methods NR, BFGS, CG and NM have obtained the value -813.4571, while BFGSR and SANN have shown -813.4576. Further, Over-dispersion values are similar until third decimal point, but after that there is a clear difference among all six methods.\nAll of the analytical methods have produced distinct values for estimated shape parameters of a and b. For the shape parameter a, similarity is only until second decimal point, and for shape parameter b, similarity of value is only on first decimal point.\n\r\rBinomialRandomVariable\r\rFrequency\r\rNR\r\rBFGS\r\rBFGSR\r\rSANN\r\rCG\r\rNM\r\r\r\r\r\r0\r\r47\r\r54.62\r\r54.62\r\r54.72\r\r54.78\r\r54.62\r\r54.61\r\r\r\r1\r\r54\r\r42\r\r42\r\r41.98\r\r42.06\r\r42\r\r42\r\r\r\r2\r\r43\r\r38.9\r\r38.9\r\r38.85\r\r38.93\r\r38.9\r\r38.91\r\r\r\r3\r\r40\r\r38.54\r\r38.54\r\r38.47\r\r38.55\r\r38.54\r\r38.54\r\r\r\r4\r\r40\r\r40.07\r\r40.07\r\r40\r\r40.06\r\r40.07\r\r40.07\r\r\r\r5\r\r41\r\r44\r\r43.99\r\r43.93\r\r43.97\r\r44\r\r44\r\r\r\r6\r\r39\r\r53.09\r\r53.09\r\r53.06\r\r53.03\r\r53.09\r\r53.09\r\r\r\r7\r\r95\r\r87.78\r\r87.8\r\r87.98\r\r87.76\r\r87.78\r\r87.77\r\r\r\rTotal No of Observations\r\r399\r\r399\r\r399.01\r\r398.99\r\r399.14\r\r399\r\r398.99\r\r\r\rp-value\r\r\r0.0901\r\r0.0903\r\r0.0902\r\r0.0903\r\r0.0901\r\r0.0902\r\r\r\rEstimated a and b\r\r\ra=0.7229428 b=0.5808488\r\ra=0.7228919 b=0.5807283\r\ra=0.7209896 b=0.5790360\r\ra=0.7219066 b=0.5813484\r\ra=0.7229403 b=0.5808469\r\ra=0.7230707 b=0.5809894\r\r\r\rMaximum Log Likelihood\r\r\r-813.4571\r\r-813.4571\r\r-813.4576\r\r-813.4576\r\r-813.4571\r\r-813.4571\r\r\r\rOver Dispersion\r\r\r0.434067\r\r0.4340993\r\r0.4347778\r\r0.4341682\r\r0.4340679\r\r0.4340165\r\r\r\r\r\rFinal Conclusion\rNow we can conclude the above findings using the tables provided, and it is clear that there is no strong change in expected frequencies, maximum Log Likelihood value or p-value if we use any one of the methods mentioned above. If time is crucial it is best to avoid BFGSR and SANN methods as they take considerable amount of time. I would recommend choose the analytical method from maxLik function based on your needs of output and research objective.\nTHANK YOU\n\r","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"5b4fca2860576a2b19329c0bf37d66f3","permalink":"/post/maxlik/benchmarking-maxlik-function/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/post/maxlik/benchmarking-maxlik-function/","section":"post","summary":"Comparing the analytical methods of the maxLik optimizing function.","tags":["Beta-Binomial","fitODBOD","maxLik"],"title":"Benchmarking the maxLik function","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rIOS slide Presentation\rIntroduction\rPackages Used\rSpecies vs Sex vs BirthYear (code)\rSpecies vs Sex vs BirthYear (plot)\rStatus vs Sex vs BirthYear (code)\rStatus vs Sex vs BirthYear (plot)\rSpecies vs Sex vs Status (code)\rSpecies vs Sex vs Status (plot)\rBirth Year and Sex of the Acquisitioned (code)\rBirth Year and Sex of the Acquisitioned (plot)\rSpecies and their sex over current location (code)\rSpecies and their sex over current location (plot)\rAcquisitioned ones and thier Sex with Status (code)\rAcquisitioned ones and thier Sex with Status (plot)\rConclusion\r\r\rOver the weeks I have only done blog posts for TidyTuesday. Today for week 38, I am going to present my blog post in a presentation. This presentation does not include plots, but the code only. Therefore, I am putting the plots here.\nBlog post : https://t.co/QhXIzSRuit\nGitHub code for presentation: https://t.co/GERv0GziFL #week38 #TidyTuesday pic.twitter.com/x2aDqjhbnl\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) December 18, 2018  IOS slide Presentation\r\rPresentation\rRmarkdown for the same presentation\r\rBelow is the content from the presentation, but I have included the plots.\n#load the packages\rlibrary(readr)\rlibrary(lubridate)\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(ggthemr)\rlibrary(stringr)\r\rIntroduction\r\r2194 observations.\r21 variables.\rData : Cetacean Data\rRead : The Pudding Article\rAbout : Big Fish in the Sea.\r\r\rPackages Used\r\rreadr\rlubridate\rtidyverse\rmagrittr\rggthemr\rstringr\r\r#load the data\rSeaCreature \u0026lt;- read_csv(\u0026quot;allCetaceanData.csv\u0026quot;, col_types = cols(X1 = col_skip()))\rattach(SeaCreature)\r# loading theme\rggthemr(\u0026quot;flat dark\u0026quot;)\r\rSpecies vs Sex vs BirthYear (code)\rPlot1\u0026lt;-ggplot(SeaCreature,aes(x=species,y=birthYear,color=sex))+\rgeom_jitter()+\rcoord_flip()+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Species and Sex over their BirthYear\u0026quot;)+\rylab(\u0026quot;Birth Year\u0026quot;)+\rxlab(\u0026quot;Species\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_1.png\u0026quot;,width = 12,height = 12)\r\rSpecies vs Sex vs BirthYear (plot)\r\rPlot 1\rAlot of Bottle-nose type species from early years.\rMore missing values for Birth Year.\rSecond most goes to Killer Whale Orca.\rThird place is in with Beluga type Species.\rHere and there few of them without knowledge of Gender.\r\r\r\rStatus vs Sex vs BirthYear (code)\rPlot2\u0026lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),\ry=birthYear,color=sex))+\rgeom_jitter()+\rcoord_flip()+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Status and Sex over their BirthYear\u0026quot;)+\rylab(\u0026quot;Birth Year\u0026quot;)+\rxlab(\u0026quot;Status\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_2.png\u0026quot;,width = 12,height = 12)\r\rStatus vs Sex vs BirthYear (plot)\r\rPlot 2\rDead Sea Creatures from the beginning of time itself.\rMostly dead, but from 1960 alot of them are alive.\rBirth Year unknown for most of the Dead and few of the Released.\rQuite a few with status unknown.\rOnly one escaped and it is a male in 1981.\r\r\r\rSpecies vs Sex vs Status (code)\rPlot3\u0026lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),\ry=str_wrap(species,12),color=sex))+\rgeom_jitter()+\rcoord_flip()+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Species and Sex over their status\u0026quot;)+\rylab(\u0026quot;Species\u0026quot;)+\rxlab(\u0026quot;Status\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_3.png\u0026quot;,width = 14,height = 12)\r\rSpecies vs Sex vs Status (plot)\r\rPlot 3\rOne male Bottle-nose species escaped.\rMore Killer whale orca’s and White-sided Pacific Species are dead than alive\rAround 15 Species have dead creatures and non alive.\rOne male Bottle-nose species Escaped but found dead.\rThere are 4 miscarriaged Bottle-nose species and three are female.\r\r\r\rBirth Year and Sex of the Acquisitioned (code)\rPlot4\u0026lt;-ggplot(SeaCreature,aes(x=acquisition,\ry=birthYear,color=sex))+\rgeom_jitter()+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Acquisitioned ones with their and BirthYear\u0026quot;)+\rylab(\u0026quot;Birth Year\u0026quot;)+\rxlab(\u0026quot;Acquisition\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_4.png\u0026quot;,width = 12,height = 12)\r\rBirth Year and Sex of the Acquisitioned (plot)\r\rPlot 4\rWith early Birth Year to until 1990 the creatures were captured.\rFrom Birth Year 1971 to 2017 only the creatures are born.\rAfter 1965 around 30 creatures have been rescued.\rClose to 40 creatures with unknown status with Birth Year known.\rMost of the rescued ones are of Male gender.\r\r\r\rSpecies and their sex over current location (code)\rPlot5\u0026lt;-ggplot(SeaCreature,aes(x=str_wrap(species,12),\ry=currently,color=sex))+\rgeom_jitter()+\rtheme(axis.text.x =element_text(angle = 90, hjust = 1))+\rggtitle(\u0026quot;Species and Sex over their Current Location\u0026quot;)+\rylab(\u0026quot;Current Location\u0026quot;)+\rxlab(\u0026quot;Species\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_5.png\u0026quot;,width = 14,height = 14)\r\rSpecies and their sex over current location (plot)\r\rPlot 5\rClose to 50 current locations.\rThere are few locations with only one type of species.\rBottle-nose creatures in most of these locations.\rSea Life park in Hawaii has a diverse amount of Species.\rSea world in San Diego is second when it comes to diversity.\r\r\r\rAcquisitioned ones and thier Sex with Status (code)\rPlot6\u0026lt;-ggplot(SeaCreature,aes(x=status,y=acquisition,color=sex))+\rgeom_jitter()+\rggtitle(\u0026quot;Acquisitioned with Sex and Status\u0026quot;)+\rxlab(\u0026quot;Status\u0026quot;)+\rylab(\u0026quot;Acquisition\u0026quot;)+ legend_bottom() #ggsave(\u0026quot;Plot_6.png\u0026quot;,width = 12,height = 12)\r\rAcquisitioned ones and thier Sex with Status (plot)\r\rPlot 6\rMost of the Captured creatures are Dead, but few of them Released.\rMost of the Rescued creatures are Dead, few alive and some Released.\rIn Unknown acquisition-ed type alot of them are Dead.\rOne rescued creature with unknown status.\r6 creatures which were born have been released and 50% are male.\r\r\r\rConclusion\r\rIos slides are NICE.\rJitter plots useful for categorical data.\rPlots are too complex when using Location, Currently and Birth Year, but manageable.\rBottle-nose species is holding a special place in this data-set.\rAlot of unknown data points when it comes to Birth Year.\r\rTHANK YOU\n\r","date":1545091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545091200,"objectID":"8fbb6a6374eacf48cad997c8fd65066f","permalink":"/post/week_38/week-38-sea-creatures/","publishdate":"2018-12-18T00:00:00Z","relpermalink":"/post/week_38/week-38-sea-creatures/","section":"post","summary":"2018 Week 38 TidyTuesday: Sea creatures(Mostly Whales).","tags":["TidyTuesday","R"],"title":"Week 38: Sea Creatures","type":"post"},{"authors":null,"categories":["fitODBOD"],"content":"\r\rEstimating the shape parameters of Beta-Binomial Distribution\rIntroduction\rBrief of optim Function\rNelder and Mead method\rBFGS method\rCG method\rL-BFGS-B method\rSANN method\rBrent method\r\rSummary of Time evaluation for different Analytical methods of optim function\rSummary of results after using the optim function for different analytical methods\rFinal Conclusion\r\r\rEstimating the shape parameters of Beta-Binomial Distribution\r\rIntroduction\rI wrote a blog post earlier this month to understand the optimization functions in R and compare them. Here I am taking my time to go through one function at a time, only when there are more than one analytical method to use.\nToday I will scrutinize the optim function which has six analytical methods. Setting the stage, we have the Beta-Binomial distribution and Binomial Outcome data, and we need to estimate proper shape parameters which would minimize the Negative Log Likelihood value or Maximize the Log Likelihood value of the Beta-Binomial distribution for the above Binomial Outcome data. In this case Alcohol Consumption data from the fitODBOD package will be used.\nIn this blog post we focus on the process time to optimization, estimated shape parameters, minimized Negative Log Likelihood value, expected frequencies, p-value and Over-dispersion in tables.\nBelow are the six analytical methods in concern\nNelder Mead\rBFGS\rCG\rL-BFGS-B\rSANN\rBrent\r\rAlcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.\nlibrary(fitODBOD)\rkable(Alcohol_data,\u0026quot;html\u0026quot;,align=c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 14,full_width = F) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\rDays\r\rweek1\r\rweek2\r\r\r\r\r\r0\r\r47\r\r42\r\r\r\r1\r\r54\r\r47\r\r\r\r2\r\r43\r\r54\r\r\r\r3\r\r40\r\r40\r\r\r\r4\r\r40\r\r49\r\r\r\r5\r\r41\r\r40\r\r\r\r6\r\r39\r\r43\r\r\r\r7\r\r95\r\r84\r\r\r\r\rBrief of optim Function\rReading through the optim function brief from the previous post it will help the reader regarding operational questions of the function.\nSo for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would minimize the Negative Log Likelihood value of the Beta-Binomial distribution.\nFirst we are transforming the given EstMLEBetaBin function to satisfy the optim function conditions.\n# new function to facilitate optim criteria\r# only one input but has two elements\rforoptim\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\r}\rSo the foroptim function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further the optim function can be scrutinized as below.\n\rpackage : stats\rNo of Inputs: 7\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 5\rNo of Analytical Methods : 6\rDefault Method : Nelder-Mead\r\r\rNelder and Mead method\rDefault analytical method is Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.\nReference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.\nBelow is the code of using optim function with Nelder and Mead analytical method.\n# optimizing values for a,b using default analytical method or Nelder and Mead\rNM_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim)\r# the outputs\rNM_answer$par # estimated values for a, b\rNM_answer$value # minimized function value NM_answer$counts # see the documentation to understand\rNM_answer$convergence # indicates successful completion\rNM_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,NM_answer$par[1],NM_answer$par[2])\r\rBFGS method\rThe documentation indicates that BFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\nBelow is the code for using optim function with BFGS analytical method\n# optimizing values for a,b using BFGS inputs\rBFGS_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \u0026quot;BFGS\u0026quot;)\r# the outputs\rBFGS_answer$par # estimated values for a, b\rBFGS_answer$value # minimized function value BFGS_answer$counts # see the documentation to understand\rBFGS_answer$convergence # indicates successful completion\rBFGS_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rBFGS_answer$par[1],BFGS_answer$par[2])\r\rCG method\rThe documentation indicates the Method CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak–Ribiere or Beale–Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.\nReference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.\nUsing CG method with optim function is explained below\n# optimizing values for a,b using CG inputs\rCG_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \u0026quot;CG\u0026quot;)\r# the outputs\rCG_answer$par # estimated values for a, b\rCG_answer$value # minimized function value CG_answer$counts # see the documentation to understand\rCG_answer$convergence # indicates successful completion\rCG_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,CG_answer$par[1],CG_answer$par[2])\r\rL-BFGS-B method\rMethod L-BFGS-B is that of Byrd et. al. (1995) which allows box constraints, that is each variable can be given a lower and/or upper bound. The initial value must satisfy the constraints. This uses a limited-memory modification of the BFGS quasi-Newton method. If non-trivial bounds are supplied, this method will be selected, with a warning.\nReference : Byrd, R.H., Lu, P., Nocedal, J. and Zhu, C., 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5), pp.1190-1208.\nRefer the below code chunk to under the L-BFGS-B method from optim function\n# optimizing values for a,b using L-BFGS-B inputs\rL_BFGS_B_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \u0026quot;L-BFGS-B\u0026quot;)\r# the outputs\rL_BFGS_B_answer$par # estimated values for a, b\rL_BFGS_B_answer$value # minimized function value L_BFGS_B_answer$counts # see the documentation to understand\rL_BFGS_B_answer$convergence # indicates successful completion\rL_BFGS_B_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rL_BFGS_B_answer$par[1],L_BFGS_B_answer$par[2])\r\rSANN method\rMethod SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.\nBy default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to \\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\\), where \\(t\\) is the current iteration step and temp and tmax are specifiable via control.\nNote that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.\nReference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.\nBelow mentioned code chunk is simply using SANN method for optim function\n# optimizing values for a,b using default inputs\rSANN_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \u0026quot;SANN\u0026quot;)\r# the outputs\rSANN_answer$par # estimated values for a, b\rSANN_answer$value # minimized function value SANN_answer$counts # see the documentation to understand\rSANN_answer$convergence # indicates successful completion\rSANN_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rSANN_answer$par[1],SANN_answer$par[2])\r\rBrent method\rBrent Method is for one-dimensional problems only, using optimize(). It can be useful in cases where optim() is used inside other functions where only method can be specified, such as in mle from package stats4. Brent method does not work for our situation.\nReference : Brent, R.P., 2013. Algorithms for minimization without derivatives. Courier Corporation.\n\r\rSummary of Time evaluation for different Analytical methods of optim function\rBelow considered table will compare the system process time for different analytical methods. In order to do this time comparison it is necessary to use the benchmark function of rbenchmark package. Below written code chunk provides the output in a table form which includes the analytical methods and their respective time values. The estimation process of the parameters where each method has been replicated 1000 times to receive a more accurate table for time values.\nThe table is in accordance to the elapsed time value column in the ascending order. According to this we can see that least time takes to the Nelder and Mead method and most time is taken to the SANN method. These times completely depends on the Negative Log Likelihood function you need to minimize, the data you provided, the number of estimators that needs to be estimated, the complexity of the function and finally computer’s processing power.\nlibrary(rbenchmark)\rResults1\u0026lt;-benchmark(\r\u0026quot;NelderMead\u0026quot;={ optim(par = c(0.1,0.2), fn = foroptim)},\r\u0026quot;BFGS\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim,method = \u0026quot;BFGS\u0026quot;)},\r\u0026quot;CG\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim,method = \u0026quot;CG\u0026quot;)},\r\u0026quot;L-BFGS-B\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim,method = \u0026quot;L-BFGS-B\u0026quot;)},\r\u0026quot;SANN\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim,method = \u0026quot;SANN\u0026quot;)},\rreplications = 1000,\rcolumns = c(\u0026quot;test\u0026quot;,\u0026quot;replications\u0026quot;,\u0026quot;elapsed\u0026quot;,\r\u0026quot;relative\u0026quot;,\u0026quot;user.self\u0026quot;,\u0026quot;sys.self\u0026quot;),\rorder = \u0026#39;elapsed\u0026#39;\r)\rkable(Results1,\u0026quot;html\u0026quot;,align = c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(full_width = T,bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 14) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\r\rtest\r\rreplications\r\relapsed\r\rrelative\r\ruser.self\r\rsys.self\r\r\r\r\r\r1\r\rNelderMead\r\r1000\r\r15.49\r\r1.000\r\r12.44\r\r0.07\r\r\r\r4\r\rL-BFGS-B\r\r1000\r\r23.46\r\r1.515\r\r17.53\r\r0.05\r\r\r\r2\r\rBFGS\r\r1000\r\r42.87\r\r2.768\r\r30.25\r\r0.11\r\r\r\r3\r\rCG\r\r1000\r\r93.99\r\r6.068\r\r63.95\r\r0.11\r\r\r\r5\r\rSANN\r\r1000\r\r2526.70\r\r163.118\r\r2071.07\r\r2.44\r\r\r\r\r\rSummary of results after using the optim function for different analytical methods\rAfter using the methods Nelder Mead, BFGS, CG, L-BFGS-B and SANN to estimate the shape parameters a, b we can use the estimated parameters in the function fitBetaBin. Using this function we can find expected frequencies for each of these analytical methods and compare p-values and over-dispersion. Further, understand if using different analytical methods had any effect on them.\nAccording to the below table there is no significant changes between the expected frequencies, except while using SANN method. All five methods generate different Over-dispersion values after the first three decimal places. Negative Log Likelihood values and p values are same for all 5 methods until first three decimal places. This is a clear indication of it does not matter what analytical method we use the estimation will occur effectively but only efficiency will be affected.\n\r\rBinomialRandomVariable\r\rFrequency\r\rNelderMead\r\rBFGS\r\rCG\r\rLBFGSB\r\rSANN\r\r\r\r\r\r0\r\r47\r\r54.61\r\r54.62\r\r54.62\r\r54.62\r\r54.75\r\r\r\r1\r\r54\r\r42\r\r42\r\r42\r\r42\r\r42.02\r\r\r\r2\r\r43\r\r38.91\r\r38.9\r\r38.9\r\r38.9\r\r38.89\r\r\r\r3\r\r40\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.52\r\r\r\r4\r\r40\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.04\r\r\r\r5\r\r41\r\r44\r\r43.99\r\r44\r\r44\r\r43.96\r\r\r\r6\r\r39\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.05\r\r\r\r7\r\r95\r\r87.77\r\r87.8\r\r87.78\r\r87.78\r\r87.78\r\r\r\rTotal No of Observations\r\r399\r\r398.99\r\r399.01\r\r399\r\r399\r\r399.01\r\r\r\rp-value\r\r\r0.0902\r\r0.0903\r\r0.0901\r\r0.0901\r\r0.0901\r\r\r\rEstimated a and b\r\r\ra=0.7230707 b=0.5809894\r\ra=0.7228930 b=0.5807279\r\ra=0.7229414 b=0.5808477\r\ra=0.7229432 b=0.5808496\r\ra=0.7215669 b=0.5802982\r\r\r\rNegative Log Likelihood\r\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4573\r\r\r\rOver Dispersion\r\r\r0.4340165\r\r0.4340992\r\r0.4340675\r\r0.4340668\r\r0.4344303\r\r\r\r\r\rFinal Conclusion\rWe had 6 methods to compare but choosing one over the other is completely harmless to the final result of estimation as seen by our tables. And our situation forces us to not use the Brent method. The only issue is time, therefore I would recommend choose the best analytical method from the optim function based on your needs of output and research objective.\nTHANK YOU\n\r","date":1544745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544745600,"objectID":"96374b42aeff237101e45033d4c029ed","permalink":"/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/","publishdate":"2018-12-14T00:00:00Z","relpermalink":"/post/optim/optim-estimating-the-shape-parameters-of-beta-binomial-distribution/","section":"post","summary":"Opimizing using 'optim' function and comparing analytical methods.","tags":["Beta-Binomial","fitODBOD","optim","R"],"title":"Benchmarking the optim function","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rInspection Type\rCritical Flag\rInspection Type and Critical Flag over the years\rCycle Inspection\rPre-permit Operational\rAdministrative Miscellaneous\r\rMost Inspected Restaurants\rDunkin Donuts\rSubway\rMcDonalds\rStarbucks\rKennedy Fried Chicken\r\rConclusion\rFurther Analysis\r\r\r# load the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(magrittr)\rlibrary(ggthemr)\rlibrary(lubridate)\rlibrary(stringr)\rlibrary(kableExtra)\r#using theme\rggthemr(\u0026quot;fresh\u0026quot;)\r#load data\rNYC\u0026lt;-read_csv(\u0026quot;nyc_restaurants.csv\u0026quot;, col_types = cols(inspection_date = col_date(format = \u0026quot;%m/%d/%Y\u0026quot;)))\rattach(NYC)\rData set is completed with more than 300000 records and several important variables such as inspection date, violation code, critical flag, score and violation description. In this article I will mainly focus on Inspection Type, boro, Inspection year, cuisine type and Critical Flag.\nSo NYC Restaurants. Ha. My take on this topic. The code in GitHub: https://t.co/vbTREkq32Y #TidyTuesday #rstats pic.twitter.com/AFDjiOfk9o\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) December 13, 2018  GitHub Code\nFirst see what type of inspections have occurred.\nInspection Type\rThis ordered bar plot for inspection type clearly indicates that cycle inspection / initial inspection has the highest amount of counts with 171,657. while second place goes to cycle inspection/ re-inspection with 73207. Other types of inspection hold less than 21,000 counts, where there are more than 10 types of inspections which hold counts less than 1000.\n#summary.factor(inspection_type) %\u0026gt;%\r# sort()\r# Bar plot for Insepction type\rggplot(NYC,aes(x=fct_infreq(str_wrap(inspection_type,35))))+\rgeom_bar()+coord_flip()+\rscale_y_continuous(breaks = seq(0,200000,25000),labels = seq(0,200000,25000))+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),hjust=-0.005)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Type of Insepction\u0026quot;)+\rggtitle(\u0026quot;Types of Inspection\u0026quot;)\r\rCritical Flag\rCritical flag has three types of categories which are Critical, Not Critical and Not Applicable. It is represented as a pie chart where 54.9% are critical(164,623) and second place goes to Not Critical(129,348) with 43.1%. Finally, only 6029 inspections have lead to Not Applicable which is represented by 2.0%.\n#summary for critical flag\r#NYC$critical_flag %\u0026gt;%\r# summary.factor() %\u0026gt;%\r# sum()\rvalue=c(164623,6029,129348)\r# creating data frame for Critical flag\rCF\u0026lt;-data.frame(\rgroup=c(\u0026quot;Critical\u0026quot;,\u0026quot;Not Applicable\u0026quot;,\u0026quot;Not Critical\u0026quot;),\rvalue=c(164623,6029,129348),\rper=round(value*100/300000,4)\r)\r# pie chart for percentages\rP1\u0026lt;-ggplot(CF,aes(x=\u0026quot;\u0026quot;,y=per,fill=group))+\rgeom_col()+ theme_void()+\rgeom_text(aes(label=scales::percent(per/100)),position=position_stack(vjust=0.5))+\rlabs(title = \u0026quot;Critical Flag \\nDistribution\u0026quot;,fill=\u0026quot;Type\u0026quot;)+\rcoord_polar(theta = \u0026quot;y\u0026quot;,start = 0)\r# pie chart for counts\rP2\u0026lt;-ggplot(CF,aes(x=\u0026quot;\u0026quot;,y=value,fill=group))+\rgeom_col()+theme_void()+\rlabs(title = \u0026quot;Critical Flag\\n Distribution\u0026quot;,fill=\u0026quot;Type\u0026quot;)+ geom_text(aes(label = value), position = position_stack(vjust = 0.5)) +\rcoord_polar(theta = \u0026quot;y\u0026quot;,start = 0)\rgridExtra::grid.arrange(P1,P2,nrow=2)\r\rInspection Type and Critical Flag over the years\rNow facing the issue of inspection year, inspection type and critical flag, I have generated bar plots for identifying the behavior. Basically what we have is bar plots for years from 2012 to 2018 how the counts of Critical flag types have changed for different paired types of inspections.\nInspection types considered\n\rCycle Inspection / Initial Inspection and Cycle Inspection / Re-inspection.\rPre-permit (Operational) / Initial Inspection and Pre-permit (Operational) / Re-inspection.\rAdministrative Miscellaneous / Initial Inspection and Administrative Miscellaneous / Re-inspection.\r\rCycle Inspection\rInitial Inspection is always high for all the years while assigning Critical. The years 2012, 2013 and 2014 has very low amount of counts while the succeeding years have increasing counts. Initial inspection over the years from 2015 is increasing for the gap between Critical and Not Critical. In 2015 the above gap is close to 4000 but by 2018 it has increased to 10000. If we consider Re-inspection the gap for Critical and Not Critical is close 1000 in year 2015 but by year 2018 it is 4000. Not applicable is increasing over the years for both initial inspection and re-inspection, even though the amounts are in hundreds.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Cycle Inspection / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Cycle Inspection / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Cycle Inspection\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Cycle Inspection over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\rPre-permit Operational\rSecond most considered type of inspection is Pre-permit Operational, where it begins from 2014. Even though in year 2014 the amounts for initial inspection and re-inspection are less than 20 in all three critical flag categories. Considering the gap between Critical and Not Critical for initial inspection over the years there is a clear increase. Where as in year 2015 the gap is slightly less than 400, next year it is close to 1000, but by year 2018 this gap is above than 2000.\nIn re-inspection for the year 2015 the gap is almost 150, yet with more inspections by 2018 the gap increases to 600. For, Not Applicable the counts do not have a clear increasing or decreasing pattern over the years.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Pre-permit (Operational) / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Pre-permit (Operational) / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Pre-permit (Operational)\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Pre-permit (Operational) over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\rAdministrative Miscellaneous\rVery odd bar plot here than previous two plots for inspection types. There is no Critical type restaurants in the Administrative Miscellaneous inspection type. The counts begin from year 2014 but the amounts are less than 10. Clearly for initial inspections over the years from 2015 to 2018 the count for Not Applicable are increasing, but this is not the case for Not Critical.\nIn year 2015 the amount for the type Not Critical for initial inspection was 528, but in years 2016 and 2018 the amounts increased respectively to 1038 and 1486. Even though the amount decreased to 1015 in year 2017. The same pattern of increase and decrease behavior occurs for Re-inspection type as well.\n# subsetting data # Specific insepction type, critical flag and year with bar plot subset(NYC,inspection_type==\u0026quot;Administrative Miscellaneous / Initial Inspection\u0026quot; |\rinspection_type==\u0026quot;Administrative Miscellaneous / Re-inspection\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rxlab(\u0026quot;Administrative Miscellaneous\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Administrative Miscellaneous over the years for Critical Flag\u0026quot;)+\rlabs(fill=\u0026quot;Critical Flag\u0026quot;)+\rfacet_wrap(~year(inspection_date)) +\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..),\rposition = position_dodge(width = 1), vjust = -0.05)\r\r\rMost Inspected Restaurants\rFirst look at the top 5 restaurants which were inspected and clearly Dunkin’ Donuts has the highest amount with 3136, while second place goes to Subway with 2419 and third place goes to McDonald’s with 1783.\n# Most 5 restaurants which were inspected\rkable(summary.factor(dba) %\u0026gt;%\rsort() %\u0026gt;%\rtail(5)\r,col.names = c(\u0026quot;Frequency\u0026quot;),align = \u0026#39;c\u0026#39;) \r\r\r\r\rFrequency\r\r\r\r\r\rKENNEDY FRIED CHICKEN\r\r1031\r\r\r\rSTARBUCKS\r\r1636\r\r\r\rMCDONALD’S\r\r1783\r\r\r\rSUBWAY\r\r2419\r\r\r\rDUNKIN’ DONUTS\r\r3136\r\r\r\r\rDunkin Donuts\rQueens has close to 1000 observations of Dunkin Donuts and lowest amount goes to Staten Island with 308. Other three boros have counts in between 550 and 710.\n# subsetting Dunkin Donuts with boro\rsubset(NYC, dba==\u0026quot;DUNKIN\u0026#39; DONUTS\u0026quot;) %\u0026gt;%\rggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Dunkin Donuts in a Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rThere are only 5 cuisine types and prominent ones are American, Donuts and drinks(Cafe/Coffee/Tea). Year 2014 is very low in amounts even for cuisine Donuts, but this is not the case over the next few years and the scores are mostly centered between 5 to 15. Bagels/Pretzels and Jewish/ Kosher have least amount of data where none of scores are above 25. It is safe to to say we have more Critical type data and they are mostly close to the score of 10.\n# Dunkin Donuts and scores with critical flag\rsubset(NYC, dba==\u0026quot;DUNKIN\u0026#39; DONUTS\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Dunkin Donuts score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,60,5),labels =seq(0,60,5))+\rfacet_wrap(~cuisine_description) \r\rSubway\r912 points from Manhattan with the highest count and lowest count goes to Staten Island with 141 counts. Bronx and Brooklyn has counts in between 320 and 365, but Queens boro has an amount of 683.\n# subsetting Subway with boro\rsubset(NYC, dba==\u0026quot;SUBWAY\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Subways in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rSubway has only 5 cuisines which are American, Other, Sandwiches, Sandwiches/Salads/Mixed Buffet and Soups \u0026amp; Sandwiches. In these five categories only Sandwiches has significant amount of data points. Oddly, we can see the year 1900 in the x axis and which means error.\nIn Sandwiches category there are more points which are centered in between 5 to 15 in scores, while most of them are Not Critical. Oddly in 2018 there are Critical data points with scores above 55 in Sandwiches cuisine type.\nFor the Other category there are only 4 observations which are in 2018 and 75% of them are Not Critical. Considering American Cuisines there are points in all 4 years and most of them are less than 25% and Not Critical.\n# Subway and scores with critical flag\rsubset(NYC, dba==\u0026quot;SUBWAY\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Subway score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rMcDonalds\rClose to 550 data points are from Manhattan boro, but Staten Island boro has points close to 75. Second place of 487 counts goes to Queens boro. Other two boros, which are Bronx and Brooklyn have counts in the range of 320 and 360.\n# subsetting McDonalds with boro\rsubset(NYC, dba==\u0026quot;MCDONALD\u0026#39;S\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many McDonalds in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rOnly the cuisines Other and Sandwiches/Salads/Mixed Buffet has data points in the years 2017 and 2018 and these points have an amount less than 15 in both cases. Considering the other two cuisines which are American and Hamburgers, there are more points in the latter than in the former. It should be noted that Hamburgers cuisine has mostly points centered close to the score range of 5 to 10, and these points are mostly Not Critical.\nIn American cuisine for the year 2016 there are 5 points which have scores close to 70, even though in any other year this has not occurred. To be more precise, except those 5 points all the others have scores less than 45 for American Cuisine.\n# McDonalds and scores with critical flag\rsubset(NYC, dba==\u0026quot;MCDONALD\u0026#39;S\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;McDonalds score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rStarbucks\rThere are more than 1100 Starbucks in Manhattan only while other boros have less than 220 and lowest amount goes to Bronx with 31. Second lowest goes to Staten Island with 41.\n#subsetting Starbucks with boro\rsubset(NYC, dba==\u0026quot;STARBUCKS\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Starbucks in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rStarbucks is mainly focused on Cafe/Coffee/Tea rather than cuisines such as Pizza, Sandwiches, American and other. Clearly there are negligible amount of data points in those four categories, except American cuisine.\nIf we focus on Drinks(Coffee/Cafe/Tea), evidently most of them are Not Critical and they are centered around the score of 0 to 10. Even though scores higher than 15 do occur they are spread out widely. This is a common occurrence for all four years, which is from 2015 to 2018.\n# Starbucks and scores with critical flag\rsubset(NYC, dba==\u0026quot;STARBUCKS\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+\rlabs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Starbucks score changes with Critical Flag for Cuisines\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\rKennedy Fried Chicken\rBronx the has most amount observations to Kennedy Fried Chicken’s with 626 while lowest count of 5 is from Staten Island. Other three boros have counts in the range of 75 to 190.\n#subsetting Kennedy Fried Chicken with boro\rsubset(NYC, dba==\u0026quot;KENNEDY FRIED CHICKEN\u0026quot;) %\u0026gt;% ggplot(.,aes(x=boro))+\rgeom_bar(position = \u0026quot;dodge\u0026quot;,stat = \u0026quot;count\u0026quot;)+\rgeom_text(stat = \u0026quot;count\u0026quot;,aes(label=..count..), vjust = -0.05)+\rggtitle(\u0026quot;How many Kennedy Fried Chicken\u0026#39;s in Boro\u0026quot;)+\rxlab(\u0026quot;Boro\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)\rThere are only four cuisines which are related to Kennedy Fried Chicken, and they are American, Chicken, Hamburgers and other. The categories Hamburgers and Other have very less amount of counts and the year 1900 is also mentioned. For Hamburgers cuisine only the year 2017 has significant amount of data points, where the year 2015 has only one and year 2018 has only two points.\nFurther, the scores for these points are always less than 20 and mostly Critical. Cuisines of Chicken has more data points than American but in both types there is no clear centering of data around a certain score.\n# Kennedy Fried Chicken and scores with critical flag\rsubset(NYC, dba==\u0026quot;KENNEDY FRIED CHICKEN\u0026quot;) %\u0026gt;%\rggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\rgeom_jitter(alpha=0.3)+ labs(color=\u0026quot;Critical Flag\u0026quot;)+ ggtitle(\u0026quot;Kennedy Fried Chicken score changes with Critical Flag\u0026quot;)+\rxlab(\u0026quot;Critical Flag\u0026quot;)+ylab(\u0026quot;Score\u0026quot;)+\rscale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\rfacet_wrap(~cuisine_description) \r\r\rConclusion\rMy conclusion of the above plots and a table in point form\n\rfacet wrap is very useful.\n\rAdding colors makes it more useful for the plots.\n\rGenerating pie chart using bar chart looks good.\n\r\r\rFurther Analysis\r\rIt is possible to focus on violation codes.\n\rFurther looking at the cuisines we can divide it based on the boros as well.\n\r\rTHANK YOU\n\r","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"abcec5a1f72983b2624e600077e24359","permalink":"/post/week_37/week-37-nyc-restaurants/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/post/week_37/week-37-nyc-restaurants/","section":"post","summary":"2018 Week 37 TidyTuesday: New York Restaurants.","tags":["R","TidyTuesday"],"title":"Week 37: NYC Restaurants","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r\rClaps\rTop 10 Authors and Claps for their posts\rTop 10 Authors with most posts\rTop 10 Authors who have posts with Images\rTop 10 Authors who have posts without Images\rTop 10 Authors and Reading time for their posts\rTop 5 Publications with most posts\rWord Cloud for the Titles from Top 10 Authors\rWord Cloud for the Titles from Top 5 publications\rConclusion\rFurther Analysis\r\r\r#loading packages\r#load data\rlibrary(readr)\r#manipulate data\rlibrary(dplyr)\rlibrary(magrittr)\r# format table with expense\rlibrary(formattable)\r# knitting the document\rlibrary(knitr)\r# another type of table\rlibrary(kableExtra)\r# playing with strings\rlibrary(stringr)\r# combining two plots\rlibrary(grid)\rlibrary(gridExtra)\r# that theme you wanted\rlibrary(ggthemr)\r# text analysis\rlibrary(tm)\rlibrary(SnowballC)\rlibrary(RColorBrewer)\rlibrary(wordcloud)\r# adding theme called fresh for plots\rggthemr(\u0026quot;fresh\u0026quot;)\r#loading the data\rmedium \u0026lt;- read_csv(\u0026quot;medium_datasci.csv\u0026quot;)\rattach(medium)\rFocusing on Claps with Authors and publications, where does writing alot of posts will get you with popularity and claps. The code will focus on Top 10 Authors with most of the posts and Claps they have received. Further, does having an image in the post matter ?. Finally, word clouds for Top 10 authors and Top 5 publications with their titles.\nmy take medium posts on week 36 #tidytuesday https://t.co/FTAZh0vKI8\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) December 8, 2018  GitHub Code\nClaps\rTable indicates that 25,729 posts have 0 claps, while 7,093 posts with only one clap, and finally 3044 posts with 2 claps. The only odd one is posts with 50 claps where the count is 970.\n# extracting the top 15 with most claps\rclaps_table\u0026lt;-table(claps) %\u0026gt;%\rsort() %\u0026gt;%\rtail(15)\r# table it up kable(t(claps_table),\u0026quot;html\u0026quot;) %\u0026gt;%\rkable_styling(bootstrap_options = \u0026quot;striped\u0026quot;,full_width = T) %\u0026gt;%\rrow_spec(0,bold = T,font_size = 13,color = \u0026quot;grey\u0026quot;)\r\r\r13\r\r12\r\r9\r\r8\r\r11\r\r7\r\r50\r\r10\r\r6\r\r5\r\r4\r\r3\r\r2\r\r1\r\r0\r\r\r\r\r\r602\r\r634\r\r661\r\r721\r\r759\r\r864\r\r970\r\r989\r\r1124\r\r1389\r\r1402\r\r2150\r\r3044\r\r7093\r\r25729\r\r\r\r\r\rTop 10 Authors and Claps for their posts\rThere are only two posts which do not have Images in their content. The highest number of claps is 60,000, a post written by Sophia Ciocca under the title “How Does Spotify Know You So Well?”. Second Place is for the article “Blockchain is not only crappy technology but a bad vision for the future” which was written by Kai Stinchombe with 53,000 claps. De Xun is the only author who has two articles which are in this list on the places 8 and 9 with claps respectively 37,000 and 36,000.\n# seperate medium with author, titles, claps and image\rclaps_A_I\u0026lt;-medium[,c(\u0026quot;author\u0026quot;,\u0026quot;title\u0026quot;,\u0026quot;subtitle\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)] %\u0026gt;%\rarrange(claps) %\u0026gt;%\rtail(10)\rnames(claps_A_I)\u0026lt;-c(\u0026quot;Author\u0026quot;,\u0026quot;Title\u0026quot;,\u0026quot;Subtitle\u0026quot;,\u0026quot;Claps\u0026quot;,\u0026quot;Image\u0026quot;)\r# table it\rformattable(claps_A_I[,-3],align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;r\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rClaps=color_tile(\u0026quot;lightblue\u0026quot;,\u0026quot;blue\u0026quot;),\rImage=color_tile(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;)\r))\r\r\rAuthor\r\rTitle\r\rClaps\r\rImage\r\r\r\r\r\rVishal Maini\r\rA Beginners Guide to AI/ML\r\r36000\r\r1\r\r\r\rDe Xun\r\rSWIPE Bi-Weekly Update, 16th-27th July\r\r36000\r\r1\r\r\r\rDe Xun\r\r[PARTNERSHIP] SWIPE-Bluzelle: Building SWIPEs decentralized database\r\r37000\r\r1\r\r\r\rAndrej Karpathy\r\rSoftware 2.0\r\r38000\r\r0\r\r\r\rRadu Raicea\r\rWant to know how Deep Learning works? Heres a quick guide for everyone.\r\r39000\r\r1\r\r\r\rAnything App\r\rFast-forward twenty years with Anything App.\r\r42000\r\r0\r\r\r\rMichael Jordan\r\rArtificial IntelligenceThe Revolution Hasnt Happened Yet\r\r46000\r\r1\r\r\r\rXiaohan Zeng\r\rI interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers\r\r49000\r\r1\r\r\r\rKai Stinchcombe\r\rBlockchain is not only crappy technology but a bad vision for the future\r\r53000\r\r1\r\r\r\rSophia Ciocca\r\rHow Does Spotify Know You So Well?\r\r60000\r\r1\r\r\r\r\r\rTop 10 Authors with most posts\rYves Mulkers has most amount of posts with 487 but only 3,779 claps. This is not the case for Corsairs publishing where for 156 posts the number of claps are 111,501. Even looking at other authors names we can see that writing alot of posts does not create claps.\n# finding who are the top 10 authors with claps\r# summary.factor(medium$author) %\u0026gt;%\r# sort() %\u0026gt;%\r# tail(11)\r# extracting posts only from the top 10 authors with most posts\rTop10_author\u0026lt;-subset(medium,\rauthor ==\u0026quot;C Gavilanes\u0026quot; | author == \u0026quot;Jae Duk Seo\u0026quot; |\rauthor == \u0026quot;Corsair\u0026#39;s Publishing\u0026quot; | author==\u0026quot;Alibaba Cloud\u0026quot; |\rauthor == \u0026quot;Ilexa Yardley\u0026quot; | author == \u0026quot;Peter Marshall\u0026quot; |\rauthor == \u0026quot;AI Hawk\u0026quot; | author == \u0026quot;DEEP AERO DRONES\u0026quot; |\rauthor == \u0026quot;Synced\u0026quot; | author == \u0026quot;Yves Mulkers\u0026quot;)\r# plotting the top 10 authors\rg1_Top10_a\u0026lt;-ggplot(Top10_author,aes(x=author))+\rcoord_flip()+ geom_bar()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;Top 10 Authors and number of posts they have written\u0026quot;)+\rgeom_text(stat = \u0026#39;count\u0026#39;,aes(label=..count..),hjust=0.5)\r# plotting the top 10 authors and their claps\rg2_Top10_a\u0026lt;-Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;)] %\u0026gt;%\rgroup_by(author) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rggtitle(\u0026quot;Total number of Claps they got\u0026quot;)+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+\rcoord_flip()+geom_text(aes(label=claps),hjust =0.5 )\r# plotting two plots at same grid\rgrid.arrange(g1_Top10_a,g2_Top10_a,ncol=1)\r\rTop 10 Authors who have posts with Images\rExtracting the top 10 authors with posts which have images it is clear most of the posts do have Images and they do generate claps. This is true for Corsairs’s Publishing. With 154 posts it generates 109,906 claps. There are authors who have written more posts than totally received claps . It should be noted that two Authors did not add any Images for their post and they are Peter Marshall and C Gavilanes.\n# plotting top 10 authors with Image\rI1_g1_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;image\u0026quot;)],image==1) %\u0026gt;%\rggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 10 Authors and posts which has Images\u0026quot;)+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(label=..count..),hjust =0.5 )\r# plotting the claps for top 10 authors with Image I1_g2_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)],image==1) %\u0026gt;%\rgroup_by(author,image) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rcoord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+\rggtitle( \u0026quot;Total number of Claps they got\u0026quot;)+\rgeom_text(aes(label=claps),hjust =0.5 ) # top plots one grid\rgrid.arrange(I1_g1_Top10_a,I1_g2_Top10_a,ncol=1)\r\rTop 10 Authors who have posts without Images\rPosts without images have very low amount of total claps. To be specific 14 posts by Jae Duk Seo have 1833 claps but 2 posts by Corsair’s publishing has 1595 claps. That is very Impressive. Further there are even posts which have claps less than 10 where the number of posts is less than 5.\n# plotting top 10 authors with No Image\rI0_g1_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;image\u0026quot;)],image==0) %\u0026gt;%\rggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 10 Authors and posts without Images\u0026quot;)+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(label=..count..),hjust =0.5 )\r# plotting the claps for top 10 authors with No Image I0_g2_Top10_a\u0026lt;-subset(Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;claps\u0026quot;,\u0026quot;image\u0026quot;)],image==0) %\u0026gt;%\rgroup_by(author,image) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rcoord_flip()+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+ ggtitle(\u0026quot;Total number of claps they got\u0026quot;)+\rgeom_text(aes(label=claps),hjust =0.5 )\r# top plots one grid\rgrid.arrange(I0_g1_Top10_a,I0_g2_Top10_a,ncol=1)\r\rTop 10 Authors and Reading time for their posts\rReading in minutes, does it has anything to do with number of posts?. Looking at the plot it is clear that posts from Synced has more total reading time than Yves Mulkers with highest number posts. The difference between posts is close 150, while difference between reading times is above 150 for Yves Mulkers and Synced.\n# plotting top 10 authors with reading times\rRT_g1_Top10_a\u0026lt;-Top10_author[,c(\u0026quot;author\u0026quot;,\u0026quot;reading_time\u0026quot;)] %\u0026gt;%\rgroup_by(author) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=author,y=reading_time))+ geom_bar(stat = \u0026quot;identity\u0026quot;)+\rggtitle(\u0026quot;Reading Time\u0026quot;)+\rxlab(\u0026quot;Author\u0026quot;)+ylab(\u0026quot;Reading Time in minutes\u0026quot;)+ coord_flip()+geom_text(aes(label=reading_time),hjust =0.5 )\r# printing the above plot with number of posts grid.arrange(g1_Top10_a,RT_g1_Top10_a,ncol=1)\r\rTop 5 Publications with most posts\rPublications with most number of posts has the highest number of claps and order achieved\nin the “Top 5 publications and number of posts they have written” plot is maintained in the “Total number of Claps they got” plot as well. This simply refers, when you write alot of posts under a publication you will receive alot of claps because of the foundation these specific publications holds in Medium.\n# finding who are the top 5 publications with claps\r# summary.factor(medium$publication) %\u0026gt;%\r# sort() %\u0026gt;%\r# tail(11)\r# extracting posts only from the top 5 publications with most posts\rTop5_pub\u0026lt;-subset(medium,\rpublication ==\u0026quot;Towards Data Science\u0026quot; | publication == \u0026quot;Hacker Noon\u0026quot; |\rpublication == \u0026quot;Becoming Human: Artificial Intelligence Magazine\u0026quot; |\rpublication ==\u0026quot;Chatbots Life\u0026quot; |\rpublication == \u0026quot;Data Driven Investor\u0026quot; )\r# plotting the top 5 publications\rg1_Top5_p\u0026lt;-ggplot(Top5_pub,aes(x=str_wrap(publication,15)))+\rcoord_flip()+ geom_bar()+\rxlab(\u0026quot;Publication\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+ ggtitle(\u0026quot;Top 5 Publications and number of posts they have written\u0026quot;)+\rgeom_text(stat = \u0026#39;count\u0026#39;,aes(label=..count..),hjust=0.5)\r# plotting the top 5 publications and their claps\rg2_Top5_p\u0026lt;-Top5_pub[,c(\u0026quot;publication\u0026quot;,\u0026quot;claps\u0026quot;)] %\u0026gt;%\rgroup_by(publication) %\u0026gt;%\rsummarise_each(funs(sum)) %\u0026gt;%\rggplot(.,aes(x=str_wrap(publication,15),y=claps))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rxlab(\u0026quot;Publication\u0026quot;)+ylab(\u0026quot;Claps\u0026quot;)+ ggtitle(\u0026quot;Total number of Claps they got\u0026quot;)+\rcoord_flip()+geom_text(aes(label=claps),hjust =0.5 )\r# plotting two plots at same grid\rgrid.arrange(g1_Top5_p,g2_Top5_p,ncol=1)\r\rWord Cloud for the Titles from Top 10 Authors\rWord cloud from the titles of the posts by Top 10 authors of most number of posts is below. The words thing, read, data, drone and new are with highest mentions. Where words such as big, telecom and tech are next in the line with higher amount of posts. In restrictions I have considered that this word cloud will have 1500 words and only if a word atleast holds the frequency of 10.\nWell, I could clearly see that there cannot be 1500 words here.\n#convert into data frame\rTop10_author\u0026lt;-data.frame(Top10_author)\r# Calculate Corpus\rTop10_author.Corpus\u0026lt;-Corpus(VectorSource(Top10_author$title))\r# clean the data\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Corpus,PlainTextDocument)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Corpus,tolower)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removeNumbers)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removeWords,stopwords(\u0026quot;english\u0026quot;))\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,removePunctuation)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,stripWhitespace)\rTop10_author.Clean\u0026lt;-tm_map(Top10_author.Clean,stemDocument)\r# save as png\r#png(filename = \u0026quot;wordcloud1.png\u0026quot;,width = 1024,height = 768)\r# plot the word cloud\rwordcloud(Top10_author.Clean,max.words = 1500,min.freq = 10,\rcolors = brewer.pal(11,\u0026quot;Spectral\u0026quot;),random.color = FALSE,\rrandom.order = TRUE)\r\rWord Cloud for the Titles from Top 5 publications\rThis word cloud also has similar restrictions for number of words and minimum frequency for a word. Words such as data, learn, use, machin, network, deep, scienc and artifici have most amount of frequency. Further, words such as neural, intellig, chatbot, part and python are also with significant amount of frequency. Here we can see clearly see there can be more than 1000 words.\n#convert into data frame\rTop5_pub\u0026lt;-data.frame(Top5_pub)\r# Calculate Corpus\rTop5_pub.Corpus\u0026lt;-Corpus(VectorSource(Top5_pub$title))\r#clean the data\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Corpus,PlainTextDocument)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Corpus,tolower)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removeNumbers)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removeWords,stopwords(\u0026quot;english\u0026quot;))\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,removePunctuation)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,stripWhitespace)\rTop5_pub.Clean\u0026lt;-tm_map(Top5_pub.Clean,stemDocument)\r# save as png\r#png(filename = \u0026quot;wordcloud2.png\u0026quot;,width = 1024,height = 768)\r# plot the word cloud\rwordcloud(Top5_pub.Clean,max.words = 1500,min.freq = 10,\rcolors = brewer.pal(11,\u0026quot;Spectral\u0026quot;),random.color = FALSE,\rrandom.order = TRUE)\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rUsing dplyr to manipulate the data-set was useful when there is complication.\n\rgrid and gridExtra packages provide a safe way of combining multiple plots into one plot.\n\rformattable and kableExtra were crucial in generating tables which are informative.\n\rWord cloud or analyzing text is very useful and flexible when we use the above packages.\n\r\r\rFurther Analysis\r\rSimilarly we can do the above analysis for Top 5 publications and other variables.\n\rWord clouds for subtitles also will be interesting to see, specially focusing on authors and publications.\n\r\rPlease see that\nThis is my sixth post on the internet so my mistakes in grammar and spellings should be very less than previous posts. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1544227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544227200,"objectID":"c4511375c23b0d3eae37508889743d15","permalink":"/post/week_36/week-36-medium-posts/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/post/week_36/week-36-medium-posts/","section":"post","summary":"2018 Week 36 TidyTuesday: Medium Posts.","tags":["R package","TidyTuesday","Medium"],"title":"Week 36: Medium Posts","type":"post"},{"authors":null,"categories":["fitODBOD"],"content":"\r\rEstimating the shape parameters by Maximizing the Log Likelihood value of Beta-Binomial Distribution.\rIntroduction\roptim Function\rnlm Function\rnlminb Function\rucminf Function\rmaxLik Function\rmle Function\rmle2 Function\r\rSummary of the seven optimization functions in R\rSummary of Time evaluation for the seven optimization R functions\rSummary of Results after estimating parameters using the optimization R functions\rFinal Conclusion\r\r\rEstimating the shape parameters by Maximizing the Log Likelihood value of Beta-Binomial Distribution.\r\rIntroduction\rWhen we need to estimate parameters from a discrete distribution or continuous distribution or a function we can use the below mentioned R functions. We will be using the technique of maximizing the Log Likelihood function or minimizing the Negative Log Likelihood function. Based on this technique we will compare these R functions because it might benefit people who are struggling which one to choose. We have 7 functions in total by my knowledge when I was writing this blog post.\nUsing the fitODBOD package, I will use the Alcohol Consumption data to try and model it for the Beta-Binomial Distribution, which has two shape parameters to estimate. The process is that we find values for shape parameters a and b (or \\(\\alpha\\) and \\(\\beta\\)) which will maximize the Log Likelihood function of Beta Binomial Distribution or in our case minimize the Negative Log Likelihood function of Beta Binomial Distribution.\nAbove mentioned Beta-Binomial distribution’s Probability Mass Function(PMF) is denoted as\n\\[P_{BetaBin}(x)= {n \\choose x} \\frac{B(\\alpha+x,n+\\beta-x)}{B(\\alpha,\\beta)} \\]\nwhere \\(n=0,1,2,...\\), \\(x=0,1,2,...,n\\) and \\(\\alpha,\\beta \u0026gt; 0\\). Further \\(x\\) is the Binomial Random variable, a,b(or \\(\\alpha\\), \\(\\beta\\)) are shape parameters and \\(n\\) is the binomial trial value. Also B(\\(\\alpha\\),\\(\\beta\\)) is the Beta function. In this distribution we have to estimate the values for a and b.\nFurther, using the PMF we can construct the Likelihood function for \\(\\Omega_{BB}=(\\alpha,\\beta)^T\\) as given below:\n\\[L(\\Omega_{BB}|x)=\\prod_{i=1}^{N} \\binom{n}{x_i} \\frac{B(\\alpha+x_i,n+\\beta-x_i)}{B(\\alpha,\\beta)}\\]\nwhere N is the Number of observations. Then Negative Log Likelihood function is given as\n\\[l(\\Omega_{BB}|x)=-\\sum_{i=1}^{N} log\\binom{n}{x_i} + \\sum_{i=1}^{N} log(B(\\alpha+x_i,n+\\beta-x_i)) - Nlog(B(\\alpha,\\beta))\\]\nIn the package fitODBOD we have the function EstMLEBetaBin which is constructed based on the above Negative Log Likelihood function, and we will use it.\nWe take Log to transform the Likelihood function values, which will simplify the computation process and save time. The optimization functions we need to compare will use specific mathematical methods to find the most appropriate shape parameter values.\nThe functions in question are\noptim\rnlm\rnlminb\rucminf\rmaxLik\rmle\rmle2\r\rFurther, I will focus on the attributes of the above functions. Focusing from which package, Number of Inputs, Number of Outputs, Time to complete optimization and Analytical Methods used with the assistance in two tables. Alcohol Consumption data has two sets of frequency values but only values from week 1 will be used. Below is the the Alcohol Consumption data, where number of observations is 399 and the Binomial Random variable is a vector of values from zero to seven.\nlibrary(fitODBOD)\rkable(Alcohol_data,\u0026quot;html\u0026quot;,align=c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 14,full_width = F) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\rDays\r\rweek1\r\rweek2\r\r\r\r\r\r0\r\r47\r\r42\r\r\r\r1\r\r54\r\r47\r\r\r\r2\r\r43\r\r54\r\r\r\r3\r\r40\r\r40\r\r\r\r4\r\r40\r\r49\r\r\r\r5\r\r41\r\r40\r\r\r\r6\r\r39\r\r43\r\r\r\r7\r\r95\r\r84\r\r\r\r\roptim Function\roptim is the first function in concern. Documentation of the optim function is useful and it indicates that this function is only used on one input situations only. This means our EstMLEBetaBin function has to be modified. Reason for this is only the parameters that should be estimated need to be input values but our EstMLEBetaBin function has four parameters which are a,b,x(Binomial Random Variable) and freq(corresponding frequency values).\nWhile using optim function first index refers to shape parameter a and second index refers to shape parameter b. Further, we have to input the observations or in our case the Binomial random variable values and their respective frequencies. I think it is inconvenient to modify the EstMLEBetaBin function, because if we want to estimate parameters for different data-sets it would become tedious. After modification we have a new function foroptim which can be used for demonstration and comparison.\nBelow is the code to estimation and going through the outputs. It should be noted that we have to provide initial parameter values as an input to the optim function, and it is best to provide values in the domain of shape parameter values which we want to estimate.\nHere the shape parameters a and b are in the region of greater than zero but less than positive infinity (\\(+\\infty \u0026gt;a,b\u0026gt;0\\)). So for the initial parameters of a=0.1 and b=0.2 we are finding parameters which would minimize the Negative Log Likelihood function of Beta-Binomial distribution with the Alcohol Consumption data.\n# new function to facilitate optim criteria\r# only one input but has two elements\rforoptim\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\r}\r# optimizing values for a,b using default mathematical method\roptim_answer\u0026lt;-optim(par=c(0.1,0.2),fn=foroptim)\r# obtaining class of output\rclass(optim_answer)\r#length of output\rlength(optim_answer)\r# the outputs\roptim_answer$par # estimated values for a, b\roptim_answer$value # minimized function value optim_answer$counts # see the documentation to understand\roptim_answer$convergence # indicates successful completion\roptim_answer$message # additional information\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\roptim_answer$par[1],optim_answer$par[2])\rSo the foroptim function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further we have scrutinized the function as below.\n\rpackage : stats\rNo of Inputs: 7\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 5\rNo of Analytical Methods : 6\rDefault Method : Nelder-Mead\r\r\rnlm Function\rnlm function is also similar to optim but only one analytical method will be used, which is a Newton-type Algorithm. Here also there needs to be changes made to our EstMLEBetaBin function as previously. After making those changes we have called the new Negative Log-likelihood function of Beta-Binomial distribution as fornlm. Then we can use the nlm function and estimate a and b for the initial shape parameter values of 0.1 and 0.2 respectively. Documentation of nlm function is very useful so that we can understand how it works.\nBelow is the code for using nlm function appropriately and fiddling with the results.\n#new function to facilitate nlm criteria\rfornlm\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\r}\r#optimizing values for a,b using the only analytical method\rnlm_answer\u0026lt;-nlm(f=fornlm,p=c(0.1,0.2))\r#obtaining class of output\rclass(nlm_answer)\r#length of output\rlength(nlm_answer)\r# the outputs\rnlm_answer$estimate # estimated values for a, b\rnlm_answer$minimum # minimized function value nlm_answer$gradient # gradient at the estimated minimum of given funciton\rnlm_answer$code # indicates successful completion\rnlm_answer$iterations # number of iterations performed\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rnlm_answer$estimate[1],nlm_answer$estimate[2])\rSimilarly for the fornlm function we have estimated values for a and b which would fit the Alcohol consumption data of week 1. Below is a point form summary of nlm function.\n\rpackage : stats\rNo of Inputs: 12\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 5\rNo of Analytical Methods : 1\rDefault Method : Newton-type Algorithm\r\r\rnlminb Function\rnlminb is also similar and requires the EstMLEBetaBin function to be restructured as similar to previous situations. After this task we now have a function called fornlminb. nlminb function is based on analytical method of unconstrained and box-constrained optimization using PORT routines.\nAfter choosing initial parameter values for a and b, which are respectively 0.1 and 0.2 the estimation was done following the process below\n# new function to facilitate nlminb criteria\rfornlminb\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default analytical method\rnlminb_answer\u0026lt;-nlminb(start=c(0.1,0.2), objective=fornlminb)\r# obtaining class of output\rclass(nlminb_answer)\r# length of output\rlength(nlminb_answer)\r# the outputs\rnlminb_answer$par # estimated values for a, b\rnlminb_answer$objective # minimized function value nlminb_answer$evaluations # see the documentation to understand\rnlminb_answer$convergence # indicates successful completion\rnlminb_answer$message # additional information\rnlminb_answer$iterations # number of iterations performed\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rnlminb_answer$par[1],nlminb_answer$par[2])\rDocumentation includes the information related to the function therefore referring it will be useful. After estimating values for a and b using nlminb function these were noticed regarding the function in concern\n\rpackage : stats\rNo of Inputs: 8\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 6\rNo of Analytical Methods : 1\rDefault Method : Unconstrained and box-constrained optimization using PORT routines\r\r\rucminf Function\rPackage ucminf produces the ucminf function, as previously mentioned functions here also we have to change the EstMLEBetaBin function. After making the changes we will be using the forucminf function to the estimation process of shape parameters a and b.\nWhen initial parameter values are set to a=0.1 and b=0.2 we will obtain results from ucminf function, which will minimize the Negative Log-likelihood value of Beta-Binomial distribution. Below is the code for estimation and using the results to understand the function ucminf.\nlibrary(ucminf)\r# new function to facilitate ucminf criteria\rforucminf\u0026lt;-function(a)\r{\rEstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default analytical method\rucminf_answer\u0026lt;-ucminf(par=c(0.1,0.2),fn=forucminf)\r#obtaining class\rclass(ucminf_answer)\r# length of output\rlength(ucminf_answer)\r# the outputs\rucminf_answer$par # estimated values for a, b\rucminf_answer$value # minimized function value ucminf_answer$invhessian.lt # see the documentation understand\rucminf_answer$convergence # indicates successful completion\rucminf_answer$message # additional information\rucminf_answer$info\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rucminf_answer$par[1],ucminf_answer$par[2])\rWith the help of R Documentation the estimation was done for shape parameter values a and b using the ucminf function. Below is the initial understanding of the ucminf function\n\rpackage : ucminf\rNo of Inputs: 5\rMinimum required Inputs : 2\rClass of output : list\rNo of outputs: 6\rNo of Analytical Methods : 1\rDefault Method : Quasi-Newton Algorithm type with BFGS updating\r\r\rmaxLik Function\rmaxLik function is from the maxLik package, which only maximizes the Log Likelihood function. Therefore we have to restructure EstMLEBetaBin as previously mentioned, but as an addition a negative sign is added for the output. This new function will be called as formaxLik.\nFor the initial parameter values where a=0.1 and b=0.2 the maxLik function will be used and results will be evaluated as below.\nlibrary(maxLik)\r# new function to facilitate maxLik criteria\rformaxLik\u0026lt;-function(a)\r{\r-EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\r}\r# optimizing values for a,b using default analytical method\rmaxLik_answer\u0026lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2))\r# obtaining class of output\rclass(maxLik_answer)\r# length of output\rlength(maxLik_answer)\r# the outputs\rmaxLik_answer$estimate # estimated values for a, b\rmaxLik_answer$maximum # minimized function value maxLik_answer$iterations # no of iterations to succeed\rmaxLik_answer$gradient # last gradient value which was calculated\rmaxLik_answer$message # additional information\rmaxLik_answer$hessian # hessian matrix\rmaxLik_answer$code # indicates successful completion\rmaxLik_answer$fixed # logical vector indicating which parameters are constants\rmaxLik_answer$type # type of maximization\rmaxLik_answer$last.step # list describing the last unsuccessful step\rmaxLik_answer$control # see the documentation understand\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmaxLik_answer$estimate[1],maxLik_answer$estimate[2])\rUsing the Documentation of maxLik and the Documentation of maxNR function the above analysis was done for the maxLik function. According to the above code, below are the findings from the maxLik function in point form\n\rpackage : maxLik\rNo of Inputs: 6\rMinimum required Inputs : 2\rClass of output : list or class of maxim or class of maxLik\rNo of outputs: 11\rNo of Analytical Methods : 7\rDefault Method : Automatically chosen\r\r\rmle Function\rstats4 package is installed so that mle function can be operated for the purpose of estimating a and b parameters. Here also as previously we need to make some changes as below and create a new Negative Log Likelihood function called formle.\nFor the initial parameter values of a=0.1 and b=0.2 the Negative Log Likelihood value of Beta-Binomial distribution has been minimized using mle function. Below is the code for estimation and investigation from the outputs of mle after estimation.\nlibrary(stats4)\r# new function to facilitate mle criteria\rformle\u0026lt;-function(a,b)\r{\rEstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)\r}\r# optimizing values for a,b using default analytical method\rmle_answer\u0026lt;-mle(minuslogl=formle,start = list(a=0.1,b=0.2))\r# obtaining class\rclass(mle_answer)\r# length of output\rlength(mle_answer)\r# the outputs\rmle_answer@call # inputs i have used mle_answer@coef # estimated values for a,b\rmle_answer@fullcoef # all values, even the fixed values we did not want to estimate\rmle_answer@vcov # variance covariance matrix for a,b\rmle_answer@min # minimized function value\rmle_answer@details # details after estimation process\rmle_answer@nobs # number of observations to be used for computing only if given mle_answer@method # optimization methods used\r# Methods used\rconfint(mle_answer) # confidence intervals for estimated values\rlogLik(mle_answer) # Negative loglikelihood value for estimated values profile(mle_answer) # Likelihood profile generation.\rnobs(mle_answer) # number of observations to be used for computing only if given\rshow(mle_answer) # display object briefly\rsummary(mle_answer) # generate a summary\r#update() # updating if we have new data and need to estimate new values\rvcov(mle_answer) # variance covariance matrix\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmle_answer@coef[1],mle_answer@coef[2])\rIt should be noted that in all 5 previous functions we had only outputs in the form of lists, but through mle we are seeing a class of mle output.The Documention explains the inputs and outputs of mle. Documentation for mle-class explains further about the methods that can be used. Below is a list of findings based on the outputs of the mle function.\n\rpackage : stats4\rNo of Inputs: 5\rMinimum required Inputs : 2\rClass of output : class of mle\rNo of outputs: 9\rMethods for output: 8\rNo of Analytical Methods :6\rDefault Method : Nelder and Mead\r\r\rmle2 Function\rmle2 function is advanced than mle function and it is from the package bbmle. Here, there is no need to modify the EstMLEBetabin function from the fitODBOD package to estimation as all previous situations .\nFor the initial parameter values of a=0.1 and b=0.2 Negative Log Likelihood function of Beta-Binomial distribution will be minimized where outputs will be investigated and methods related to output of class mle2 will be used.\nBelow is the code for using mle2 function and scrutinizing the output and methods related to it.\nlibrary(bbmle)\r# optimizing values for a,b using default analytical method\rmle2_answer\u0026lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),\rdata = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))\r# obtaining class\rclass(mle2_answer)\r# length of output\rlength(mle2_answer)\r# the outputs\rmle2_answer@call # inputs generally considered mle2_answer@call.orig # inputs i have given\rmle2_answer@coef # estimated values for a,b\rmle2_answer@fullcoef # all values, even the fixed values we did not want to estimate\rmle2_answer@vcov # variance covariance matrix for a,b\rmle2_answer@min # minimized function value\rmle2_answer@details # details after estimation process\rmle2_answer@method # optimization methods used\rmle2_answer@data # data used for estimation mle2_answer@formula # if a formula was specified in the input mle2_answer@optimizer # function used for optimizing\r# Methods used\rcoef(mle2_answer) # extrat the estimated values\rconfint(mle2_answer) # confidence intervals for estimated values\rshow(mle2_answer) # display object briefly\rsummary(mle2_answer) # generate a summary\r#update() #updating if we have new data and need to estimate new values\rvcov(mle2_answer) # variance covariance matrix\r#formula(mle2_answer) # if a formula was specified in the input #plot(mle2_answer) # plot the profile\rlogLik(mle2_answer) # Negative loglikelihood value for estimated values profile(mle2_answer) # profile of estimated values\r# fitting the Beta-Binomial distribution with estimated shape parameter values\rfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\rmle2_answer@coef[1],mle2_answer@coef[2])\rDocumention of mle2 function and Documentation of mle2-class\nprovide a few findings as mentioned below in point form.\n\rpackage : bbmle\rNo of Inputs: 22\rMinimum required Inputs : 3\rClass of output : class of mle2\rNo of outputs: 12\rMethods for output: 9\rNo of Analytical Methods : 6\rDefault Method : Nelder and Mead\r\r\r\rSummary of the seven optimization functions in R\rAfter completing a brief fact finding of the R functions optim, nlm, nlminb, ucminf, maxLik, mle and mle2. We can record the facts and results in two tables. Using them we can choose the best suitable function for our needs of estimation from optimization.\nMostly I prefer mle2 function than others because it provides special methods to handle the mle2 outputs, and the output themselves are very thorough than other R function outputs.\nIt can be seen that estimated a and b shape parameter values are different only from the third decimal point in all outputs. But this does not effect the minimized Negative Log Likelihood value of Beta-Binomial distribution for our Alcohol Consumption data, which is same for all outputs.\n\r\rFunction\r\roptim\r\rnlm\r\rnlminb\r\rucminf\r\rmaxLik\r\rmle\r\rmle2\r\r\r\r\r\rPackage\r\rstats\r\rstats\r\rstats\r\rucminf\r\rmaxLik\r\rstats4\r\rbbmle\r\r\r\rNo of Inputs\r\r7\r\r12\r\r8\r\r5\r\r6\r\r5\r\r22\r\r\r\rMinimum No of Inputs\r\r2\r\r2\r\r2\r\r2\r\r2\r\r2\r\r3\r\r\r\rAnalytical Methods\r\r6\r\r1\r\r1\r\r1\r\r7\r\r6\r\r6\r\r\r\rOutput Type\r\rList\r\rList\r\rList\r\rList\r\rList class maxLik class maxim\r\rclass mle\r\rclass mle2\r\r\r\rNo of Outputs\r\r5\r\r5\r\r6\r\r6\r\r11\r\r9\r\r12\r\r\r\rNo of Methods\r\rNone\r\rNone\r\rNone\r\rNone\r\rNone\r\r8\r\r9\r\r\r\rInitial value(a,b)\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\ra=0.1 b=0.2\r\r\r\rEstimated value(a,b)\r\ra=0.7230707 b=0.5809894\r\ra=0.7229384 b=0.5808448\r\ra=0.7229404 b=0.5808469\r\ra=0.7229390 b=0.5808458\r\ra=0.7229428 b=0.5808488\r\ra=0.7228930 b=0.5807279\r\ra=0.7228930 b=0.5807279\r\r\r\rNegative Log Likelihood value\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r813.4571\r\r\r\r\r\rSummary of Time evaluation for the seven optimization R functions\rFurther at the beginning I have mentioned to evaluate the system process time for the seven functions. In order to do this time comparison it is possible to use the benchmark function of rbenchmark package, and below mentioned code chunk provides the output in a table form. It includes the functions and their respective time values. The estimation process of the parameters where each function has been replicated 1000 times to receive a more accurate table of time values.\nThe table is in ascending order according to the elapsed time values column. According to this we can see that least time takes to the nlminb function and most time is taken to the mle2 function. These times completely depends on the Negative Log Likelihood function you need to minimize, the data you provided, the number of estimators that needs to be estimated, the complexity of the function and finally your computer’s processing power.\nTherefore based on the needs of your output and the function which needs estimation choose the best function out of the above seven. Because as I said related to the earlier table the estimated values are slightly different but does not make any strong influence on the results.\nlibrary(rbenchmark)\rResults2\u0026lt;-benchmark(\r\u0026quot;optim\u0026quot;={optim(par = c(0.1,0.2), fn = foroptim)},\r\u0026quot;nlm\u0026quot;={nlm(f = fornlm, p = c(0.1,0.2))},\r\u0026quot;nlminb\u0026quot;={nlminb(start = c(0.1,0.2), objective = fornlminb)},\r\u0026quot;ucminf\u0026quot;={ucminf(par = c(0.1,0.2), fn = forucminf)},\r\u0026quot;maxLik\u0026quot;={maxLik(logLik = formaxLik, start = c(0.1,0.2))},\r\u0026quot;mle\u0026quot;={mle(minuslogl = formle, start = list(a=0.1,b=0.2))},\r\u0026quot;mle2\u0026quot;={mle2(minuslogl = EstMLEBetaBin, start = list(a=0.1, b=0.2),\rdata = list(x=Alcohol_data$Days, freq=Alcohol_data$week1))},\rreplications = 1000,\rcolumns = c(\u0026quot;test\u0026quot;,\u0026quot;replications\u0026quot;,\u0026quot;elapsed\u0026quot;,\r\u0026quot;relative\u0026quot;,\u0026quot;user.self\u0026quot;,\u0026quot;sys.self\u0026quot;),\rorder = \u0026#39;elapsed\u0026#39;\r)\rkable(Results2,\u0026quot;html\u0026quot;,align = c(\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;)) %\u0026gt;%\rkable_styling(full_width = F,bootstrap_options = c(\u0026quot;striped\u0026quot;),font_size = 10) %\u0026gt;%\rrow_spec(0,color = \u0026quot;blue\u0026quot;) %\u0026gt;%\rcolumn_spec(1,color = \u0026quot;red\u0026quot;)\r\r\r\rtest\r\rreplications\r\relapsed\r\rrelative\r\ruser.self\r\rsys.self\r\r\r\r\r\r3\r\rnlminb\r\r1000\r\r6.34\r\r1.000\r\r6.34\r\r0.00\r\r\r\r4\r\rucminf\r\r1000\r\r7.67\r\r1.210\r\r7.67\r\r0.00\r\r\r\r2\r\rnlm\r\r1000\r\r8.38\r\r1.322\r\r8.37\r\r0.00\r\r\r\r1\r\roptim\r\r1000\r\r10.70\r\r1.688\r\r10.64\r\r0.00\r\r\r\r6\r\rmle\r\r1000\r\r23.14\r\r3.650\r\r23.11\r\r0.01\r\r\r\r5\r\rmaxLik\r\r1000\r\r36.28\r\r5.722\r\r36.25\r\r0.00\r\r\r\r7\r\rmle2\r\r1000\r\r41.29\r\r6.513\r\r41.23\r\r0.00\r\r\r\r\r\rSummary of Results after estimating parameters using the optimization R functions\rAfter using the functions optim, nlm, nlminb, ucminf, maxLik, mle and mle2 to estimate the shape parameters a, b we can use the estimated parameters in the function fitBetaBin. Using this function we can find expected frequencies for each of the estimated parameters a,b and compare p-values and over-dispersion and understand if using different optimization functions had any effect on them.\nAccording to the below table there is no significant changes between the expected frequencies, p-values or the over dispersion values. This is a clear indication that it does not matter what function we use for the optimization. Because the process will occur effectively only efficiency(time) will be affected.\n\r\rBinomialRandomVariable\r\rFrequency\r\roptim\r\rnlm\r\rnlminb\r\rucminf\r\rmaxLik\r\rmle\r\rmle2\r\r\r\r\r\r0\r\r47\r\r54.61\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r54.62\r\r\r\r1\r\r54\r\r42\r\r42\r\r42\r\r42\r\r42\r\r42\r\r42\r\r\r\r2\r\r43\r\r38.91\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r38.9\r\r\r\r3\r\r40\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r38.54\r\r\r\r4\r\r40\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r40.07\r\r\r\r5\r\r41\r\r44\r\r44\r\r44\r\r44\r\r44\r\r43.99\r\r43.99\r\r\r\r6\r\r39\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r53.09\r\r\r\r7\r\r95\r\r87.77\r\r87.78\r\r87.78\r\r87.78\r\r87.78\r\r87.8\r\r87.8\r\r\r\rTotal No of Observations\r\r399\r\r398.99\r\r399\r\r399\r\r399.01\r\r399\r\r399.01\r\r399.01\r\r\r\rp-value\r\r\r0.0902\r\r0.0901\r\r0.0901\r\r0.0901\r\r0.0901\r\r0.0903\r\r0.0903\r\r\r\rOver Dispersion\r\r\r0.4340165\r\r0.4340686\r\r0.4340679\r\r0.4340683\r\r0.434067\r\r0.4340992\r\r0.4340992\r\r\r\r\r\rFinal Conclusion\rWe had 7 functions to compare but choosing one over the other is completely harmless to the final result of estimation as seen by our tables. The only issue is time, therefore I would recommend choose the best function based on your needs of output and research objective.\nTHANK YOU\n\r","date":1543881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543881600,"objectID":"4d5c6f53598f1fd0c54e06c65bec9ee8","permalink":"/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/","publishdate":"2018-12-04T00:00:00Z","relpermalink":"/post/fitodbod_1/benchmarking-maximum-liklihood-functions-from-r/","section":"post","summary":"Comparing optimization functions for the estimation of shape parameters from the Beta-Binomial distribution.","tags":["bbmle","Beta-Binomial","fitODBOD","maxLik","mle","mle2","nlm","nlminb","optim","ucminf"],"title":"Benchmarking optimization functions in R","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rBridge Data and Baltimore\rCounties which have bridges owned by State Highway Agency\rCounties which have bridges owned by County Highway Agency\rCounties which have bridges owned by State Toll Authority\rMost amount of bridges Built based on Year\rAverage Traffic Less than or equal to 100,000 for Counties with Bridge Condition\rAverage Traffic More than 100,000 for Counties with Bridge Condition\rImprovement and Bridge Conditions with Counties\r\rConclusion\rFurther Analysis\r\r\r# loading the packages\rlibrary(readr)\rlibrary(tidyverse)\rlibrary(stringr)\rlibrary(ggthemr)\rlibrary(gganimate)\rlibrary(formattable)\r# load the theme flat dark\rggthemr(\u0026quot;flat dark\u0026quot;)\r# reading the data\rbridges \u0026lt;- read_csv(\u0026quot;baltimore_bridges.csv\u0026quot;)\r#View(bridges)\r# naming the columns\rnames(bridges)\u0026lt;-c(\u0026quot;lat\u0026quot;,\u0026quot;long\u0026quot;,\u0026quot;County\u0026quot;,\u0026quot;Carries\u0026quot;,\u0026quot;Year Built\u0026quot;,\r\u0026quot;Condition\u0026quot;,\u0026quot;Average Daily Traffic\u0026quot;,\u0026quot;Total Improvement\u0026quot;,\r\u0026quot;Month\u0026quot;,\u0026quot;Year\u0026quot;,\u0026quot;Owner\u0026quot;,\u0026quot;Responsibility\u0026quot;,\u0026quot;Vehicles\u0026quot;)\rattach(bridges)\rBridge Data and Baltimore\rData for the analysis and description about the Baltimore bridges are hyper-linked.\nBridges and Jitter plots. Ownership, Counties, and Condition with Average Daily Traffic. My take of the Bridge Data. #TidyTuesday The further code here https://t.co/L6zjRP8OKA pic.twitter.com/9RrBFSyAPD\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) November 28, 2018  GitHub Code\nData on bridges is of week 35 from TidyTuesday. Trying to explain the data using maps is obvious, yet I will use animated jitter plots. There are 13 variables and 2079 observations. Brave choice of limiting my self to less than 10 variables, where latitude, longitude and Vehicles will not be taken into account.\nSo with the help of packages tidyverse, ggthemr, gganimate,formattable and readr I will complete this analysis. Most of the bridges are owned by several agencies, but I will only focus on the top three ownership holders.\nCounties which have bridges owned by State Highway Agency\rClose to 1000 bridges are owned by State Highway Agency, where most of them are in Baltimore County. High amount of bridges are in good condition, further more bridges are in Fair condition and only around 10 bridges in Poor condition.\nConsidering the Average Daily Traffic only one bridge in Poor condition has the amount of close to 110,000, while all the other poor condition bridges have Average Daily Traffic less than 30,000. Counties Anne Arundel and Hartford have no Poor condition bridges at all.\nMost of the bridges are from Baltimore County and around 20 bridges have count of more than 150,000 Average Daily Traffic for both Fair and Good conditions. Hartford and Carroll Counties have their Average Daily Traffic which does not exceed 80,000 at any condition of the bridge.\n# jitter plot to State Highway Agency\rggplot(subset(bridges,Owner==\u0026quot;State Highway Agency\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+\rggtitle(\u0026quot;Condition of Bridges owned by State Highway Agency \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,230000,10000) ,breaks = seq(0,230000,10000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)+geom_jitter()\r\rCounties which have bridges owned by County Highway Agency\rCounty Highway Agency owns the second most amount of bridges in this data-set. Therefore using jitter plots we are going to check how the condition of the bridges and counties are explained in the simplest manner.\nLess amount of poor condition bridges in all counties except Anne Roundel County. All bridges owned by County Highway Agency have a limited Average Daily Traffic less than 50,000. Clearly we have more Fair bridges than Good ones. In the Poor condition category only two have Average Daily Traffic more than 20,000, while other two have more than 10 bridges.\nMost of these bridges are in Baltimore County even it is in any one of three conditions. There are few bridges which have values more than 40,000 Average Daily Traffic and they are also in Baltimore County.\nThere are bridges which have Zero Average Daily Traffic. In all three Conditions only Hartford County has bridges which has Average Daily Traffic less than 10,000.\n# jitter plot to County Highway Agency\rggplot(subset(bridges,Owner==\u0026quot;County Highway Agency\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+ geom_jitter()+\rggtitle(\u0026quot;Condition of Bridges owned by County Highway Agency \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,40000,5000) ,breaks = seq(0,40000,5000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rCounties which have bridges owned by State Toll Authority\rThere is only one bridge which is in Poor condition and it is in Baltimore County, while counties Howard and Anne Arundel have no Good condition bridges. Further there is only 3 Fair condition bridges in Howard County while they have values for Average Daily Traffic less than 10,000.\nThe highest Average Daily Traffic is close to 170,000 which are only 4 and are in Good and Fair conditions. Further, Anne Arundel County has only one Good bridge and in Hartford it is six bridges. Only few of the bridges have Average Daily Traffic close to zero.\n# jitter plot to State tolll authority\rggplot(subset(bridges,Owner==\u0026quot;State Toll Authority\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\rxlab(\u0026quot;County\u0026quot;)+geom_jitter()+\rggtitle(\u0026quot;Condition of Bridges owned by State Toll Authority \\nand their Average Daily Traffic\u0026quot;)+\rscale_y_continuous(labels =seq(0,170000,10000) ,breaks = seq(0,170000,10000))+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rMost amount of bridges Built based on Year\rYears 1957, 1970, 1975, 1991, 1963 and 1961 have the top 6 spots for building more than 50 bridges in those years. If we consider the conditions of Fair and Good only the year 1991 is suitable to be mentioned, while all other years has at-least one Poor condition bridge. Further There are more Poor condition bridges in 1961 than in 1957. While all Poor condition bridges has Average Daily Traffic less than 50,000.\nFinally, there are only few bridges which have Average Daily Traffic above 100,000 and only 3 are in Good condition. There are Bridges which can have Average Daily Traffic close to zero in all 6 years and all conditions.\n# jitter plot to years based on Most amount of bridges built\rggplot(subset(bridges,`Year Built`==\u0026quot;1957\u0026quot; | `Year Built`==\u0026quot;1970\u0026quot; | `Year Built`==\u0026quot;1975\u0026quot; | `Year Built`==\u0026quot;1991\u0026quot; |\r`Year Built`==\u0026quot;1963\u0026quot; | `Year Built`==\u0026quot;1961\u0026quot;)\r,aes(color=Condition,y=`Average Daily Traffic`,x=factor(`Year Built`)))+\rxlab(\u0026quot;Year Built\u0026quot;)+ylab(\u0026quot;Average Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Most amount of Bridges built based on Years \\nand their Conditions\u0026quot;)+\rgeom_jitter()+legend_bottom()+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rAverage Traffic Less than or equal to 100,000 for Counties with Bridge Condition\rWhile obtaining summary for county variable there is one issue because there are two observations which say “Baltimore city” than “Baltimore City” and I don’t want to change them.\nIf we focus on Average Daily Traffic less than or equal to 100,000 based on County and Condition. It is clear that Poor condition bridges are part of this criteria and mostly Average Daily Traffic is less than 5000 for Counties Howard, Hartford and Carroll. While Baltimore County has highest amount up-to 75,000, but Baltimore County has highest amount close to 40,000 for Average Daily Traffic. Finally Anne Arundel County has only one Poor condition bridge which has Average Daily Traffic close to zero.\nWe can see that there are more Fair Condition bridges than Good ones. In Baltimore County most of the Fair condition bridges have Average Daily Traffic less than 15000. Similarly Carroll county and Hartford county also behave under such criteria. But for Good condition bridges this is not the case where there is no certain strong dense region as similar to Fair condition bridges.\nPreviously when we looked into county we did not see Baltimore City often as a factor, but here that is not the case.\n# jitter plot to average daily Traffic less than or equal 1000000\rggplot(subset(bridges,`Average Daily Traffic`\u0026lt;=100000 \u0026amp; County!=\u0026quot;Baltimore city\u0026quot;),\raes(x=County,y=`Average Daily Traffic`,color=Condition))+\rxlab(\u0026quot;County\u0026quot;)+ylab(\u0026quot;Averag Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Average Daily Traffic Less than 100,000 \\nFor Counties\u0026quot;)+\rscale_y_continuous(labels = seq(0,100000,5000),breaks = seq(0,100000,5000))+\rtheme(axis.text.x = element_text(angle = -90))+coord_flip()+ geom_jitter()+\rtransition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rAverage Traffic More than 100,000 for Counties with Bridge Condition\rThis Jitter plot is completely different than previous one, because there are no clear dense regions for any counties and conditions of the bridge. There is only one Poor condition bridge in Baltimore County where the Average Daily Traffic is close to 115,000. In Fair condition bridges also Baltimore County holds the most, while they are slightly dense in the region of 175,000 to 190,000. while for Howard County similar density occurs between 190,000 to 205,000. Bridges in Good condition have more higher values in Baltimore County than Anne Arundel County.\n# jitter plot to average daily Traffic more than 1000000\rggplot(subset(bridges, `Average Daily Traffic` \u0026gt; 100000 \u0026amp; County != \u0026quot;Baltimore city\u0026quot;),\raes(x=County,y=`Average Daily Traffic`,color=Condition))+\rxlab(\u0026quot;County\u0026quot;)+ylab(\u0026quot;Average Daily Traffic\u0026quot;)+\rggtitle(\u0026quot;Average Daily Traffic More than 100,000 \\nFor Counties\u0026quot;)+\rscale_y_continuous(labels=seq(100000,230000,5000),breaks=seq(100000,230000,5000))+\rcoord_flip()+ theme(axis.text.x = element_text(angle = -90))+\rgeom_jitter()+transition_states(Condition,transition_length = 2,state_length = 3)+\renter_fade()+exit_shrink()+ease_aes(\u0026quot;back-in\u0026quot;)\r\rImprovement and Bridge Conditions with Counties\rIn the variable of Total Improvement there are 1438 missing values, 42 values are zero and the rest are actual values. I am going to look at Total Improvement in two tables. First table will include where Bridges have Total Improvement higher than 9,999,000 USD and less than 30,000,000 USD. Second table is for Bridges which have Total Improvement higher than or equal to 30,000,000 USD.\nFurther to make these tables interesting I will be using the package formattable, and colors and tiles for numerical values. In the first table there are 7 bridges while only Anne Arundel County holds 3 and Baltimore City holds 4. One bridge is from 1953, and others are from the period of 1977 to 1983. Conditions of these bridges are mostly Fair and two bridges are in Good condition. Lowest Average Daily Traffic is 11760, while highest is 124193, where both bridges are in Fair Condition, and the amount spent on them for Total Improvement are respectively 18,163,000 USD and 16,264,000 USD. The bridge with Highest amount of Average Daily traffic is built in 1953.\n# removing unnecessary columns and setting restriction to # Total Improvement\rTop10\u0026lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` \u0026gt; 9999 \u0026amp; `Total Improvement` \u0026lt; 30000)\r# setting colours\rcustomRed0 = \u0026quot;#FF8080\u0026quot;\rcustomRed = \u0026quot;#7F0000\u0026quot;\rcustomyellow0 = \u0026quot;#FFFF80\u0026quot;\rcustomyellow = \u0026quot;#BFBF00\u0026quot;\rcustomblue0 = \u0026quot;#6060BF\u0026quot;\rcustomblue = \u0026quot;#00007F\u0026quot;\r# creating the table for above data set\rformattable(Top10,align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rCounty =formatter(\u0026quot;span\u0026quot;,style= ~style(color=\u0026quot;grey\u0026quot;)),\r`Total Improvement`=color_tile(customblue0,customblue),\r`Average Daily Traffic`=color_tile(customyellow0,customyellow),\r`Year Built`=color_tile(customRed0,customRed)\r))\r\r\rCounty\r\rCarries\r\rYear Built\r\rCondition\r\rAverage Daily Traffic\r\rTotal Improvement\r\r\r\r\r\rAnne Arundel County\r\rMD 2\r\r1983\r\rFair\r\r53221\r\r13504\r\r\r\rAnne Arundel County\r\rUS 50\r\r1953\r\rFair\r\r124193\r\r16264\r\r\r\rAnne Arundel County\r\rUPPER LEVEL ROADWA\r\r1977\r\rFair\r\r11760\r\r18163\r\r\r\rBaltimore City \r\rIS 95 SB\r\r1977\r\rFair\r\r94765\r\r15785\r\r\r\rBaltimore City \r\rIS 95 VIADUCT SB\r\r1980\r\rGood\r\r63650\r\r16051\r\r\r\rBaltimore City \r\rIS 95 VIADUCT NB\r\r1980\r\rFair\r\r52850\r\r20484\r\r\r\rBaltimore City \r\rIS 95 SB\r\r1980\r\rGood\r\r55621\r\r20484\r\r\r\r\rWhen I did try to plot the top ten bridges with most Total improvement values there was one issue, which is the distance between first two values and the next 8 values. Therefore I divided the table into two.\nIn this second table We can see there are two bridges which are from Baltimore City and are built in 1980 and 1971, but the amount spent on Total Improvement is 300,000,000 USD each. But their Average Daily Traffic is respectively 56280 and 30600.\nWhile we have another bridge from Baltimore City and built in 1907, but Total Improvement amount is 35,026,000 USD. Here, the Average Daily Traffic is 3900.\n# removing unnecessary columns and setting restriction to # Total Improvement\rTop3\u0026lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` \u0026gt;= 30000)\r# setting colours\rcustomRed0 = \u0026quot;#FF8080\u0026quot;\rcustomRed = \u0026quot;#7F0000\u0026quot;\rcustomyellow0 = \u0026quot;#FFFF80\u0026quot;\rcustomyellow = \u0026quot;#BFBF00\u0026quot;\rcustomblue0 = \u0026quot;#6060BF\u0026quot;\rcustomblue = \u0026quot;#00007F\u0026quot;\r# creating the table for above data set\rformattable(Top3,align=c(\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;c\u0026quot;),\rlist(\rCounty =formatter(\u0026quot;span\u0026quot;,style= ~style(color=\u0026quot;black\u0026quot;)),\r`Total Improvement`=color_tile(customblue0,customblue),\r`Average Daily Traffic`=color_tile(customyellow0,customyellow),\r`Year Built`=color_tile(customRed0,customRed)\r))\r\r\rCounty\r\rCarries\r\rYear Built\r\rCondition\r\rAverage Daily Traffic\r\rTotal Improvement\r\r\r\r\r\rBaltimore City\r\rUS 40 EDMONDSON AV\r\r1907\r\rPoor\r\r3900\r\r35026\r\r\r\rBaltimore City\r\rIS 95 VIADUCT SB\r\r1980\r\rFair\r\r56280\r\r300000\r\r\r\rBaltimore City\r\rEASTERN AVENUE\r\r1971\r\rFair\r\r30600\r\r300000\r\r\r\r\r\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rJitter plots and animation are useful in explaining one continuous variable with multiple categorical variables.\n\rSub-setting the data set and applying formattable package is useful to explain different continuous values with in a table.\n\r\r\rFurther Analysis\r\rSimilarly we can use mapping to point out the locations of the bridges and use animation to make it more clear.\r\rPlease see that\nThis is my fifth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1543363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543363200,"objectID":"aeaecbd69b88f61c656058c5c9514117","permalink":"/post/week_35/week-35-baltimore-bridges/","publishdate":"2018-11-28T00:00:00Z","relpermalink":"/post/week_35/week-35-baltimore-bridges/","section":"post","summary":"2018 Week 35 TidyTuesday: Baltimore bridges.","tags":["TidyTuesday","R"],"title":"Week 35: Baltimore Bridges","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r\rPeople who do and do not celebrate Thanksgiving\rAge Distribution\rAge with Other Factors\rGender Distribution\rGender with Other Factors\rFamily Income Distribution\rFamily Income with Other Factors\rUS Region Distribution\r\rConclusion\rFurther Analysis\r\r\r# Load the packges\rlibrary(ggplot2)\rlibrary(ggthemr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(tidyverse)\rlibrary(tweenr)\rlibrary(gganimate)\rlibrary(kableExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(readr)\r#load the data\rThanksgiving\u0026lt;-read_csv(\u0026quot;thanksgiving_meals.csv\u0026quot;)\r# apply the theme grape\rggthemr(\u0026quot;grape\u0026quot;)\r#subset the people who said yes for celebrating thanksgiving\rThanksgiving_Yes\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;Yes\u0026quot;)\r#subset the people who said no for celebrating thanksgiving\rThanksgiving_No\u0026lt;-subset(Thanksgiving,celebrate==\u0026quot;No\u0026quot;)\rData set was provided on week 34 for TidyTuesday analysis. As it is Thanksgiving week this is understandable. You can receive the data set here. There are more than 65 variables and 1058 observations. The data was acquired buy a survey conducted online and information about them are here.\nMy take on Thanksgiving, Praying and Celebrating based on US regions from this data set. #Tidytuesday https://t.co/dhCJEYWOlO pic.twitter.com/4Kij5PF4oV\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) November 22, 2018  GitHub Code\nPeople who do and do not celebrate Thanksgiving\rIn this ThanksGiving data set 980 are celebrating, and 78 are not celebrating Thanksgiving. I will use plots to understand their composition and tables to explain them further.\nVariables in consideration for this task is none other than Age, Gender, Family Income and US regions. Finally my aim is to create animated plots and interactive tables for the above variables through the help of packages gganimate and kable.\nAge Distribution\rFirst the age distribution has only 4 groups, where people who celebrate Thanksgiving in the age category of 18-29 is the very least. Highest count goes to the age category of 45-59 with 269. There are 33 missing observations and they were removed.\nConsidering the people who do not celebrate Thanksgiving the least count of 6 goes to category of 60+, but here the category of 18-29 has the highest count of 31. No missing observations were recorded here.\nBelow is an animated bar plot where the counts change for their respective 4 categories. As 90% of respondents have answered Yes for celebrating Thanksgiving and rest have answered No we can clearly see the count differences\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate dont_age\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$age))\r# people who do celebrate\rdo_age\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$age)))\r# people who do celebrate\rdata_do_age\u0026lt;-data.frame(group=c(\u0026quot;18-29\u0026quot;,\u0026quot;30-44\u0026quot;,\u0026quot;45-59\u0026quot;,\u0026quot;60+\u0026quot;),\rvalues=do_age$`summary.factor(na.omit(Thanksgiving_Yes$age))`,\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,4))\r# people who do not celebrate\rdata_dont_age\u0026lt;-data.frame(group=c(\u0026quot;18-29\u0026quot;,\u0026quot;30-44\u0026quot;,\u0026quot;45-59\u0026quot;,\u0026quot;60+\u0026quot;),\rvalues=dont_age$`summary.factor(Thanksgiving_No$age)`,\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,4))\r# combining both\rdata_age\u0026lt;-rbind(data_do_age,data_dont_age)\r# animated bar plot for people who do celebrate and who do not celebrate ggplot(data_age,aes(x=factor(group),values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Age Group\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Age\u0026quot;)+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;cubic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rAge with Other Factors\rFirst table is Age vs Gender for people who celebrate Thanksgiving. All age categories have a percentage range in between 19 and 29. Highest percentage of 28.4055 is for Age category 45 - 59. Female have a higher percentage of 54.3823.\nFemale who are 60+ have the highest percentage of 15.2059, while lowest percentage of 8.7645 is for male in the age category of 18-29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,gender))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,gender)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\rcolumn_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rFemale\r\rMale\r\rSum\r\r\r\r\r\r18 - 29\r\r10.7709\r\r8.7645\r\r19.5354\r\r\r\r30 - 44\r\r13.4108\r\r11.4044\r\r24.8152\r\r\r\r45 - 59\r\r14.9947\r\r13.4108\r\r28.4055\r\r\r\r60+\r\r15.2059\r\r12.0380\r\r27.2439\r\r\r\rSum\r\r54.3823\r\r45.6177\r\r100.0000\r\r\r\r\rdetach(Thanksgiving_Yes)\rWhen considering the people who do not celebrate Thanksgiving, highest percentage of 62.8205 is for Male, while age category of 18-29 have the highest percentage of 39.7436.\nMale who are in between 18 and 29 have the highest percentage of 25.6410, while Female who are above 60 have the lowest percentage of 2.5641.\nattach(Thanksgiving_No)\r#kable(addmargins(table(age,gender))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,gender)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\rcolumn_spec(4,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rFemale\r\rMale\r\rSum\r\r\r\r\r\r18 - 29\r\r14.1026\r\r25.6410\r\r39.7436\r\r\r\r30 - 44\r\r11.5385\r\r19.2308\r\r30.7693\r\r\r\r45 - 59\r\r8.9744\r\r12.8205\r\r21.7949\r\r\r\r60+\r\r2.5641\r\r5.1282\r\r7.6923\r\r\r\rSum\r\r37.1796\r\r62.8205\r\r100.0001\r\r\r\r\rdetach(Thanksgiving_No)\rWith relative to people who celebrate Thanksgiving in the Family Income category highest percentage goes to USD 25,000 to 49,999.\nPeople who have Family Income USD 25,000 to 49,999 and age above 60 have the highest percentage of 4.96, while lowest percentage of 0.11 is for people who have Family Income in between USD 175,000 to 199,999 of age category of 18-29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,family_income)),\u0026quot;html\u0026quot;) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\r18 - 29\r\r3.4847\r\r2.1119\r\r0.9504\r\r0.2112\r\r0.5280\r\r0.1056\r\r0.6336\r\r3.6959\r\r2.1119\r\r2.0063\r\r3.6959\r\r19.5354\r\r\r\r30 - 44\r\r1.3728\r\r1.5839\r\r2.5343\r\r0.8448\r\r0.6336\r\r0.3168\r\r1.2672\r\r4.8574\r\r4.0127\r\r4.2239\r\r3.1679\r\r24.8153\r\r\r\r45 - 59\r\r0.3168\r\r1.3728\r\r4.1183\r\r2.5343\r\r2.0063\r\r1.1616\r\r3.1679\r\r4.0127\r\r3.1679\r\r3.6959\r\r2.8511\r\r28.4056\r\r\r\r60+\r\r0.3168\r\r1.2672\r\r3.9071\r\r1.4784\r\r0.8448\r\r1.1616\r\r2.9567\r\r4.9630\r\r4.1183\r\r3.4847\r\r2.7455\r\r27.2441\r\r\r\rSum\r\r5.4911\r\r6.3358\r\r11.5101\r\r5.0687\r\r4.0127\r\r2.7456\r\r8.0254\r\r17.5290\r\r13.4108\r\r13.4108\r\r12.4604\r\r100.0004\r\r\r\r\rdetach(Thanksgiving_Yes)\rOf people who do not celebrate Thanksgiving the Family Income category has the highest percentage which goes to People who prefer not to answer.\n15 cells in this table are zero which is the lowest percentage that can occur, while highest percentage goes to people who are in the age category 18 -29 while Family Income is USD 0 to 9,999 and prefer not to answer.\nattach(Thanksgiving_No) #kable(addmargins(table(age,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;) \r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\r18 - 29\r\r12.8205\r\r2.5641\r\r1.2821\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.5641\r\r5.1282\r\r2.5641\r\r12.8205\r\r39.7436\r\r\r\r30 - 44\r\r2.5641\r\r3.8462\r\r1.2821\r\r0.0000\r\r1.2821\r\r0.0000\r\r2.5641\r\r8.9744\r\r3.8462\r\r1.2821\r\r5.1282\r\r30.7695\r\r\r\r45 - 59\r\r2.5641\r\r2.5641\r\r0.0000\r\r1.2821\r\r1.2821\r\r1.2821\r\r2.5641\r\r5.1282\r\r0.0000\r\r3.8462\r\r1.2821\r\r21.7951\r\r\r\r60+\r\r0.0000\r\r1.2821\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.2821\r\r1.2821\r\r0.0000\r\r3.8462\r\r7.6925\r\r\r\rSum\r\r17.9487\r\r10.2565\r\r2.5642\r\r1.2821\r\r2.5642\r\r1.2821\r\r5.1282\r\r17.9488\r\r10.2565\r\r7.6924\r\r23.0770\r\r100.0007\r\r\r\r\rdetach(Thanksgiving_No)\rFor the people who celebrate Thanksgiving highest percentage of 21.80 goes to US region of South Atlantic. While lowest percentage goes to Mountain with 4.41.\nPeople who are from South Atlantic in the age categories of 45-59 and 60+ have the highest percentage of 6.55. While the lowest percentage of 0.64 goes to people who are in the age category of 18-29 and from East South Central.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(age,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(age,us_region)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r18 - 29\r\r2.79\r\r0.64\r\r2.26\r\r0.75\r\r0.97\r\r3.11\r\r3.76\r\r2.26\r\r2.47\r\r19.01\r\r\r\r30 - 44\r\r3.44\r\r1.18\r\r4.51\r\r0.97\r\r1.61\r\r3.87\r\r4.94\r\r1.72\r\r2.47\r\r24.71\r\r\r\r45 - 59\r\r4.40\r\r2.15\r\r4.94\r\r1.40\r\r1.72\r\r3.22\r\r6.55\r\r1.72\r\r2.69\r\r28.79\r\r\r\r60+\r\r4.94\r\r2.04\r\r3.87\r\r1.29\r\r1.61\r\r3.76\r\r6.55\r\r1.93\r\r1.50\r\r27.49\r\r\r\rSum\r\r15.57\r\r6.01\r\r15.58\r\r4.41\r\r5.91\r\r13.96\r\r21.80\r\r7.63\r\r9.13\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\rOf people who do not celebrate Thanksgiving 23.5294% are from Pacific, while lowest percentage is for people who are from New England and West North Central with 4.4118.\n10 cells have zero values which is the lowest percentage value. While highest percentage of 11.7647 occurs to people from Pacific and in the age category 30-44.\nattach(Thanksgiving_No)\r#kable(addmargins(table(age,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(5,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(age,us_region)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 9) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(5,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r18 - 29\r\r1.4706\r\r2.9412\r\r8.8235\r\r4.4118\r\r0.0000\r\r7.3529\r\r4.4118\r\r2.9412\r\r5.8824\r\r38.2354\r\r\r\r30 - 44\r\r2.9412\r\r0.0000\r\r5.8824\r\r1.4706\r\r1.4706\r\r11.7647\r\r5.8824\r\r0.0000\r\r2.9412\r\r32.3531\r\r\r\r45 - 59\r\r2.9412\r\r2.9412\r\r4.4118\r\r2.9412\r\r2.9412\r\r1.4706\r\r4.4118\r\r0.0000\r\r0.0000\r\r22.0590\r\r\r\r60+\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r2.9412\r\r1.4706\r\r1.4706\r\r0.0000\r\r7.3530\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5883\r\r8.8236\r\r4.4118\r\r23.5294\r\r16.1766\r\r4.4118\r\r8.8236\r\r100.0005\r\r\r\r\rdetach(Thanksgiving_No)\r\rGender Distribution\rWe have two types of gender categories in this data set which are male and female. According to the people who celebrate Thanksgiving 515 are Female, while only 432 are male. Here also there are 33 missing observations and they have been removed.\nBut this is not the case for those who do not celebrate Thanksgiving. Female have a count of only 29, where males have a count of 49. There were no missing observations.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_sex\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$gender))\r# people who do celebrate\rdo_sex\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$gender)))\r# people who do celebrate\rdata_do_sex\u0026lt;-data.frame(group=c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;),\rvalues=do_sex$`summary.factor(na.omit(Thanksgiving_Yes$gender))`,\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,2))\r# people who do not celebrate\rdata_dont_sex\u0026lt;-data.frame(group=c(\u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;),\rvalues=dont_sex$`summary.factor(Thanksgiving_No$gender)`,\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,2))\r# combining both data_sex\u0026lt;-rbind(data_do_sex,data_dont_sex)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_sex,aes(group,values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Gender\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Gender\u0026quot;)+\rscale_y_continuous(labels= seq(0,520,10),breaks = seq(0,520,10))+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;elastic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rGender with Other Factors\rOf people who do celebrate Thanksgiving highest percentage of 10.14 goes to Females where Family Income is USD 25,000 to 49,999. While lowest percentage of 1.27 goes to Males of Family Income category USD 175,000 to 199,999.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(gender,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(gender,family_income)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = T,font_size = 8.75) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\rFemale\r\r2.64\r\r3.80\r\r5.39\r\r2.11\r\r2.11\r\r1.48\r\r4.44\r\r10.14\r\r7.92\r\r6.97\r\r7.39\r\r54.39\r\r\r\rMale\r\r2.85\r\r2.53\r\r6.12\r\r2.96\r\r1.90\r\r1.27\r\r3.59\r\r7.39\r\r5.49\r\r6.44\r\r5.07\r\r45.61\r\r\r\rSum\r\r5.49\r\r6.33\r\r11.51\r\r5.07\r\r4.01\r\r2.75\r\r8.03\r\r17.53\r\r13.41\r\r13.41\r\r12.46\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\r3 cells in the below table are zero values, which is the lowest percentage value. Highest percentage of 14.1026 goes to Males who chose not to answer regarding Family Income where they do not celebrate Thanksgiving.\nattach(Thanksgiving_No)\r#kable(addmargins(table(gender,family_income))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(gender,family_income)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(13,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,angle = 270,align = \u0026#39;center\u0026#39;)\r\r\r\r$0 to $9,999\r\r$10,000 to $24,999\r\r$100,000 to $124,999\r\r$125,000 to $149,999\r\r$150,000 to $174,999\r\r$175,000 to $199,999\r\r$200,000 and up\r\r$25,000 to $49,999\r\r$50,000 to $74,999\r\r$75,000 to $99,999\r\rPrefer not to answer\r\rSum\r\r\r\r\r\rFemale\r\r6.4103\r\r3.8462\r\r1.2821\r\r1.2821\r\r0.0000\r\r0.0000\r\r1.2821\r\r5.1282\r\r3.8462\r\r5.1282\r\r8.9744\r\r37.1798\r\r\r\rMale\r\r11.5385\r\r6.4103\r\r1.2821\r\r0.0000\r\r2.5641\r\r1.2821\r\r3.8462\r\r12.8205\r\r6.4103\r\r2.5641\r\r14.1026\r\r62.8208\r\r\r\rSum\r\r17.9488\r\r10.2565\r\r2.5642\r\r1.2821\r\r2.5641\r\r1.2821\r\r5.1283\r\r17.9487\r\r10.2565\r\r7.6923\r\r23.0770\r\r100.0006\r\r\r\r\rdetach(Thanksgiving_No)\rFemale from South Atlantic who celebrate Thanksgiving have a highest percentage of 12.14. Where respondents from Mountain region and Males have the lowest percentage of 1.29.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(gender,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(gender,us_region)),4)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\rFemale\r\r8.16\r\r3.33\r\r8.59\r\r3.11\r\r3.33\r\r7.30\r\r12.14\r\r4.19\r\r4.40\r\r54.55\r\r\r\rMale\r\r7.41\r\r2.69\r\r6.98\r\r1.29\r\r2.58\r\r6.66\r\r9.67\r\r3.44\r\r4.73\r\r45.45\r\r\r\rSum\r\r15.57\r\r6.02\r\r15.57\r\r4.40\r\r5.91\r\r13.96\r\r21.81\r\r7.63\r\r9.13\r\r100.00\r\r\r\r\rdetach(Thanksgiving_Yes)\rMale respondents who do not celebrate Thanksgiving where they are from Middle Atlantic have a highest percentage of 16.1765. Even though Females of West South Central have the lowest percentage of 1.4706 and Males from West North Central also have the same percentage value.\nattach(Thanksgiving_No)\r#kable(addmargins(table(gender,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(3,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(gender,us_region)),6)*100),\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(3,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;1.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\rFemale\r\r4.4118\r\r2.9412\r\r4.4118\r\r4.4118\r\r1.4706\r\r8.8235\r\r7.3529\r\r2.9412\r\r1.4706\r\r38.2354\r\r\r\rMale\r\r2.9412\r\r2.9412\r\r16.1765\r\r4.4118\r\r2.9412\r\r14.7059\r\r8.8235\r\r1.4706\r\r7.3529\r\r61.7648\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5883\r\r8.8236\r\r4.4118\r\r23.5294\r\r16.1764\r\r4.4118\r\r8.8235\r\r100.0002\r\r\r\r\rdetach(Thanksgiving_No)\r\rFamily Income Distribution\rThere are 11 categories when it comes to Family Income. The option of Prefer Not to answer is given and has been chosen by people who celebrate and people who do not celebrate Thanksgiving.\nConsidering the people the who celebrate Thanksgiving, highest count of 166 goes to the category of 25,000 to 49,999 USD. While least count goes to 175,000 to 199,999 USD and the count is 26. Further, 118 people have chosen not to answer this question. 33 Missing observations were removed.\nWhere as in people who do not celebrate Thanksgiving, second highest count goes to the categories of 0 to 9,999 USD and 25,000 to 49,999 USD, where the count is 14. Similarly, for the least count of 1 also there are two Family Income categories, which are 125,000 to 149,999 USD and 175,000 to 199,999 USD. Prefer not to answer is the choice of 18 respondents who participated in this survey. No missing observations were recorded.\nAs before here also an animated bar plot is used to explain this.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_FI\u0026lt;-as.data.frame(summary.factor(Thanksgiving_No$family_income))\r# people who do celebrate\rdo_FI\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$family_income)))\r# people who do celebrate\rdata_do_FI\u0026lt;-data.frame(group=c(\u0026quot;0-9,999\u0026quot;,\u0026quot;10,000-24,999\u0026quot;,\u0026quot;25,000-49,999\u0026quot;,\r\u0026quot;50,000-74,999\u0026quot;,\u0026quot;75,000-99,999\u0026quot;,\u0026quot;100,000-124,999\u0026quot;,\r\u0026quot;125,000-149,999\u0026quot;,\u0026quot;150,000-174,999\u0026quot;,\r\u0026quot;175,000-199,999\u0026quot;,\u0026quot;200,000 and up\u0026quot;,\u0026quot;Not to answer\u0026quot;),\rvalues=c(52,60,166,127,127,109,48,38,26,76,118),\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,11))\r# people who do not celebrate\rdata_dont_FI\u0026lt;-data.frame(group=c(\u0026quot;0-9,999\u0026quot;,\u0026quot;10,000-24,999\u0026quot;,\u0026quot;25,000-49,999\u0026quot;,\r\u0026quot;50,000-74,999\u0026quot;,\u0026quot;75,000-99,999\u0026quot;,\u0026quot;100,000-124,999\u0026quot;,\r\u0026quot;125,000-149,999\u0026quot;,\u0026quot;150,000-174,999\u0026quot;,\u0026quot;175,000-199,999\u0026quot;,\r\u0026quot;200,000 and up\u0026quot;,\u0026quot;Not to answer\u0026quot;),\rvalues=c(14,8,14,8,6,2,1,2,1,4,18),\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,11))\r# combine the dataset\rdata_FI\u0026lt;-rbind(data_do_FI,data_dont_FI)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_FI,aes(group,values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Family Income in dollars\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to Family Income\u0026quot;)+\rscale_y_continuous(labels= seq(0,170,10),breaks = seq(0,170,10))+\rgeom_text(aes(label=values), vjust=1)+coord_flip()+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;bounce-in\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\rFamily Income with Other Factors\rOf people who do celebrate Thanksgiving the lowest percentage of zero is for people from West North Central with Family Income USD 175,000 to 199,999, people from West South Central with Family Income USD 175,000 to 199,999, people from Mountain with Family Income USD 150,000 to 174,999 and people from Mountain with Family Income USD 175,000 to 199,999. Highest percentage of 4.9409 goes to people from South Atlantic with Family Income USD 25,000 to 49,999.\nattach(Thanksgiving_Yes)\r#kable(addmargins(table(family_income,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(12,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do celebrate\rkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\u0026quot;html\u0026quot;)%\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(12,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r$0 to $9,999\r\r0.6445\r\r0.2148\r\r0.6445\r\r0.1074\r\r0.2148\r\r1.0741\r\r0.8593\r\r0.2148\r\r0.9667\r\r4.9409\r\r\r\r$10,000 to $24,999\r\r0.8593\r\r0.6445\r\r0.5371\r\r0.1074\r\r0.3222\r\r1.5038\r\r1.3963\r\r0.5371\r\r0.3222\r\r6.2299\r\r\r\r$100,000 to $124,999\r\r2.7927\r\r0.8593\r\r1.3963\r\r0.4296\r\r0.5371\r\r1.6112\r\r2.2556\r\r0.3222\r\r1.5038\r\r11.7078\r\r\r\r$125,000 to $149,999\r\r0.5371\r\r0.1074\r\r0.6445\r\r0.3222\r\r0.3222\r\r0.6445\r\r1.8260\r\r0.2148\r\r0.5371\r\r5.1558\r\r\r\r$150,000 to $174,999\r\r0.2148\r\r0.2148\r\r0.5371\r\r0.0000\r\r0.2148\r\r0.9667\r\r0.8593\r\r0.3222\r\r0.7519\r\r4.0816\r\r\r\r$175,000 to $199,999\r\r0.6445\r\r0.2148\r\r1.0741\r\r0.0000\r\r0.2148\r\r0.3222\r\r0.3222\r\r0.0000\r\r0.0000\r\r2.7926\r\r\r\r$200,000 and up\r\r0.7519\r\r0.4296\r\r2.0408\r\r0.6445\r\r0.7519\r\r1.0741\r\r1.1815\r\r0.5371\r\r0.6445\r\r8.0559\r\r\r\r$25,000 to $49,999\r\r2.2556\r\r1.1815\r\r2.5779\r\r0.6445\r\r1.0741\r\r2.1482\r\r4.9409\r\r1.5038\r\r1.0741\r\r17.4006\r\r\r\r$50,000 to $74,999\r\r2.5779\r\r0.9667\r\r2.0408\r\r0.5371\r\r0.5371\r\r1.8260\r\r2.7927\r\r1.3963\r\r0.8593\r\r13.5339\r\r\r\r$75,000 to $99,999\r\r2.7927\r\r0.8593\r\r1.9334\r\r0.8593\r\r0.5371\r\r1.2889\r\r2.4705\r\r1.2889\r\r1.5038\r\r13.5339\r\r\r\rPrefer not to answer\r\r1.5038\r\r0.3222\r\r2.1482\r\r0.7519\r\r1.1815\r\r1.5038\r\r2.9001\r\r1.2889\r\r0.9667\r\r12.5671\r\r\r\rSum\r\r15.5748\r\r6.0149\r\r15.5747\r\r4.4039\r\r5.9076\r\r13.9635\r\r21.8044\r\r7.6261\r\r9.1301\r\r100.0000\r\r\r\r\rdetach(Thanksgiving_Yes)\rThere are a lot of cell values which have zero therefore I am not going to state them. Further, Highest percentage value of 5.8824 is from people of Middle Atlantic and Family Income categories of USD 0 to 9,999 and USD 25,000 to 49,999.\nattach(Thanksgiving_No)\r#kable(addmargins(table(family_income,us_region))) %\u0026gt;% # kable_styling(\u0026quot;striped\u0026quot;,full_width = F) %\u0026gt;%\r# column_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\r# row_spec(12,bold = T,color = \u0026quot;red\u0026quot;)\r# table of percentages for people who do not celebrate\rkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\u0026quot;html\u0026quot;)%\u0026gt;% kable_styling(\u0026quot;striped\u0026quot;,full_width = F,font_size = 10) %\u0026gt;%\rcolumn_spec(11,bold = T,color = \u0026quot;red\u0026quot;) %\u0026gt;%\rrow_spec(12,bold = T,color = \u0026quot;red\u0026quot;)%\u0026gt;%\rcolumn_spec(1,bold = T,width = \u0026#39;2.5cm\u0026#39;) %\u0026gt;%\rrow_spec(0,bold = T,align = \u0026#39;center\u0026#39;)\r\r\r\rEast North Central\r\rEast South Central\r\rMiddle Atlantic\r\rMountain\r\rNew England\r\rPacific\r\rSouth Atlantic\r\rWest North Central\r\rWest South Central\r\rSum\r\r\r\r\r\r$0 to $9,999\r\r0.0000\r\r0.0000\r\r5.8824\r\r4.4118\r\r0.0000\r\r2.9412\r\r2.9412\r\r0.0000\r\r1.4706\r\r17.6472\r\r\r\r$10,000 to $24,999\r\r0.0000\r\r1.4706\r\r0.0000\r\r1.4706\r\r0.0000\r\r1.4706\r\r1.4706\r\r1.4706\r\r1.4706\r\r8.8236\r\r\r\r$100,000 to $124,999\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.9412\r\r\r\r$125,000 to $149,999\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r\r\r$150,000 to $174,999\r\r0.0000\r\r0.0000\r\r2.9412\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r2.9412\r\r\r\r$175,000 to $199,999\r\r0.0000\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r\r\r$200,000 and up\r\r1.4706\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r0.0000\r\r2.9412\r\r0.0000\r\r0.0000\r\r5.8824\r\r\r\r$25,000 to $49,999\r\r2.9412\r\r0.0000\r\r5.8824\r\r1.4706\r\r1.4706\r\r4.4118\r\r2.9412\r\r0.0000\r\r0.0000\r\r19.1178\r\r\r\r$50,000 to $74,999\r\r1.4706\r\r1.4706\r\r1.4706\r\r0.0000\r\r0.0000\r\r4.4118\r\r0.0000\r\r1.4706\r\r1.4706\r\r11.7648\r\r\r\r$75,000 to $99,999\r\r0.0000\r\r0.0000\r\r0.0000\r\r1.4706\r\r1.4706\r\r4.4118\r\r1.4706\r\r0.0000\r\r0.0000\r\r8.8236\r\r\r\rPrefer not to answer\r\r0.0000\r\r1.4706\r\r2.9412\r\r0.0000\r\r0.0000\r\r4.4118\r\r4.4118\r\r1.4706\r\r4.4118\r\r19.1178\r\r\r\rSum\r\r7.3530\r\r5.8824\r\r20.5884\r\r8.8236\r\r4.4118\r\r23.5296\r\r16.1766\r\r4.4118\r\r8.8236\r\r100.0008\r\r\r\r\rdetach(Thanksgiving_No)\r\rUS Region Distribution\rThere are 9 regions in both sides, and also both sides have missing values. People who do celebrate Thanksgiving have 49 missing values, while only 10 are missing values for people who do not celebrate Thanksgiving.\nattach(Thanksgiving_Yes)\rattach(Thanksgiving_No)\r# people who do not celebrate\rdont_USR\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_No$us_region)))\r# people who do celebrate\rdo_USR\u0026lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$us_region)))\r# people who do celebrate\rdata_do_USR\u0026lt;-data.frame(group=c(\u0026quot;East North Central\u0026quot;, \u0026quot;East South Central\u0026quot;,\r\u0026quot;West South Central\u0026quot;, \u0026quot;West North Central\u0026quot;,\r\u0026quot;Middle Atlantic\u0026quot;,\u0026quot;South Atlantic\u0026quot;, \u0026quot;Mountain\u0026quot;, \u0026quot;New England\u0026quot;, \u0026quot;Pacific\u0026quot;),\rvalues=c(145,56,85,71,145,203,41,55,130),\rframe=rep(\u0026quot;Do Celebrate\u0026quot;,9))\r# people who do not celebrate\rdata_dont_USR\u0026lt;-data.frame(group=c(\u0026quot;East North Central\u0026quot;, \u0026quot;East South Central\u0026quot;,\r\u0026quot;West South Central\u0026quot;, \u0026quot;West North Central\u0026quot;,\r\u0026quot;Middle Atlantic\u0026quot;,\u0026quot;South Atlantic\u0026quot;, \u0026quot;Mountain\u0026quot;, \u0026quot;New England\u0026quot;, \u0026quot;Pacific\u0026quot;),\rvalues=c(5,4,6,3,14,11,6,3,16),\rframe=rep(\u0026quot;Do not Celebrate\u0026quot;,9))\r# combine both datasets\rdata_USR\u0026lt;-rbind(data_do_USR,data_dont_USR)\r# animated plot for people who do celebrate and who do not celebrate\rggplot(data_USR,aes(x=str_wrap(group,7),values))+\rgeom_bar(stat = \u0026#39;identity\u0026#39;,position = \u0026quot;identity\u0026quot;)+\rylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;US Regions\u0026quot;)+\rggtitle(\u0026quot;Animated plot how Do and Do not people prefer \\naccording to US Regions\u0026quot;)+\rscale_y_continuous(labels= seq(0,210,10),breaks = seq(0,210,10))+\rgeom_text(aes(label=values), vjust=1)+\rtransition_states(frame,transition_length = 2,state_length = 3)+\renter_fade()+\rexit_shrink()+\rease_aes(\u0026#39;cubic-in-out\u0026#39;)\rdetach(Thanksgiving_Yes)\rdetach(Thanksgiving_No)\r\r\rConclusion\rI shall conclude my findings in point form\n\rWe can use gganimate to make bar plots interesting and useful.\n\rkable is very useful because of styling options.\n\r\r\rFurther Analysis\r\rThere are more than 50 variables therefore much more can be done than describing the data-set.\n\rWe can use advanced methods such as clustering and model fitting.\n\r\rPlease see that\nThis is my fourth post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1543190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543190400,"objectID":"257374ec038a4371c3f48f8d5d92ed26","permalink":"/post/week_34/week-34-thanksgiving/","publishdate":"2018-11-26T00:00:00Z","relpermalink":"/post/week_34/week-34-thanksgiving/","section":"post","summary":"2018 Week 34 TidyTuesday: Thanksgiving Survey.","tags":["TidyTuesday","R","R package"],"title":"Week 34 : Thanksgiving ","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\rWhat I posted in #TidyTuesday on 13th November 2018.\rSDI Countries with Malaria Death Count Rate\rSaharan Region with Malaria Death Count Rate\rEuropean Countries with Malaria Death Count Rate\rGreat Britain or United Kingdom with Malaria Death Count Rate\rAmerican Region with Malaria Death Count Rate\rAsian Countries with Malaria Death Count Rate\rNorth Africa and Middle East Region with Death Count Rate\rOceania Region with Death Count Rate\rCaribbean and Latin America and Caribbean Countries with Death Count Rate\rConclusion\rFurther Analysis\r\r\r# load the packages\rlibrary(ggplot2)\rlibrary(ggrepel)\rlibrary(ggthemr)\rlibrary(magrittr)\rlibrary(stringr)\rlibrary(gridExtra)\rlibrary(readr)\rlibrary(gganimate)\r# load the theme flat\rggthemr(\u0026quot;flat\u0026quot;)\r#load the data sets\rmalaria_deaths\u0026lt;-read_csv(\u0026quot;malaria_deaths.csv\u0026quot;)\rmalaria_deaths_age\u0026lt;-read_csv(\u0026quot;malaria_deaths_age.csv\u0026quot;)\rattach(malaria_deaths)\rattach(malaria_deaths_age)\r# disseminating data\rMalaria_deaths_Code_missing\u0026lt;-malaria_deaths[!complete.cases(Code),]\rMD_CM_SDI\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;High-middle SDI\u0026quot; | Entity == \u0026quot;High SDI\u0026quot; | Entity == \u0026quot;Low SDI\u0026quot; | Entity ==\u0026quot;Low-middle SDI\u0026quot; | Entity ==\u0026quot;Middle SDI\u0026quot;)\rMD_CM_Sahara\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Central Sub-Saharan Africa\u0026quot; |\rEntity == \u0026quot;Western Sub-Saharan Africa\u0026quot; | Entity == \u0026quot;Southern Sub-Saharan Africa\u0026quot; |\rEntity ==\u0026quot;Eastern Sub-Saharan Africa\u0026quot; | Entity ==\u0026quot;Sub-Saharan Africa\u0026quot;)\rMD_CM_EU\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Western Europe\u0026quot; | Entity == \u0026quot;Eastern Europe\u0026quot; | Entity == \u0026quot;Central Europe\u0026quot;)\rMD_CM_GB\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;England\u0026quot; | Entity == \u0026quot;Northern Ireland\u0026quot; | Entity == \u0026quot;Scotland\u0026quot; | Entity ==\u0026quot;Wales\u0026quot;)\rMD_CM_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Andean Latin America\u0026quot; | Entity == \u0026quot;North America\u0026quot; | Entity == \u0026quot;Southern Latin America\u0026quot; | Entity ==\u0026quot;Tropical Latin America\u0026quot; | Entity ==\u0026quot;Central Latin America\u0026quot; )\rMD_CM_A\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;North Africa and Middle East\u0026quot; | Entity == \u0026quot;Southeast Asia\u0026quot; | Entity == \u0026quot;Australasia\u0026quot; | Entity ==\u0026quot;East Asia\u0026quot; | Entity ==\u0026quot;Central Asia\u0026quot; | Entity ==\u0026quot;Oceania\u0026quot;| Entity ==\u0026quot;High-income Asia Pacific\u0026quot;| Entity ==\u0026quot;South Asia\u0026quot; )\rMD_CM_C_LA\u0026lt;-subset(Malaria_deaths_Code_missing,Entity == \u0026quot;Caribbean\u0026quot; | Entity == \u0026quot;Latin America and Caribbean\u0026quot;)\r# disseminating data\r#Malaria_deaths_age_Code_missing\u0026lt;-malaria_deaths_age[!complete.cases(code),]\r#MD_age_CM_SDI\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;High-middle SDI\u0026quot; | # entity == \u0026quot;High SDI\u0026quot; | entity == \u0026quot;Low SDI\u0026quot; |\r# entity ==\u0026quot;Low-middle SDI\u0026quot; | entity ==\u0026quot;Middle SDI\u0026quot;)\r#MD_age_CM_Sahara\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Central Sub-Saharan Africa\u0026quot; | # entity == \u0026quot;Western Sub-Saharan Africa\u0026quot; | entity == \u0026quot;Southern Sub-Saharan Africa\u0026quot; |\r# entity ==\u0026quot;Eastern Sub-Saharan Africa\u0026quot; | entity ==\u0026quot;Sub-Saharan Africa\u0026quot;)\r#MD_age_CM_EU\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Western Europe\u0026quot; | # entity == \u0026quot;Eastern Europe\u0026quot; | entity == \u0026quot;Central Europe\u0026quot;)\r#MD_age_CM_GB\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;England\u0026quot; | # entity == \u0026quot;Northern Ireland\u0026quot; | entity == \u0026quot;Scotland\u0026quot; |\r# entity ==\u0026quot;Wales\u0026quot; )\r#MD_age_CM_LA\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Andean Latin America\u0026quot; | # entity == \u0026quot;North America\u0026quot; | entity == \u0026quot;Southern Latin America\u0026quot; |\r# entity ==\u0026quot;Tropical Latin America\u0026quot; | entity ==\u0026quot;Central Latin America\u0026quot;)\r#MD_age_CM_A\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;North Africa and Middle East\u0026quot;| # entity == \u0026quot;Southeast Asia\u0026quot; | entity == \u0026quot;Australasia\u0026quot; | entity ==\u0026quot;East Asia\u0026quot;| # entity ==\u0026quot;Central Asia\u0026quot; | entity ==\u0026quot;Oceania\u0026quot;| # entity ==\u0026quot;High-income Asia Pacific\u0026quot;| entity ==\u0026quot;South Asia\u0026quot; )\r#MD_age_CM_C_LA\u0026lt;-subset(Malaria_deaths_age_Code_missing,entity == \u0026quot;Caribbean\u0026quot; | # entity == \u0026quot;Latin America and Caribbean\u0026quot;)\rWhat I posted in #TidyTuesday on 13th November 2018.\rBelow is the code and simple analysis I posted in relative to the Malaria data. I only focused on Sri Lanka and Malaria deaths over years. I could not find anything else at that time, but over the days I realized it should be worth to see how death rate counts change with different regions which do not have Code variable assigned for the data sets malaria deaths and malaria deaths of age.\n#TidyTuesday , First ever participation. How Malaria cases have drooped over the time in Sri Lanka #tidyverse #LKA . Link to code in github: https://t.co/mPSPZYFAkp pic.twitter.com/I2FKXAqjcT\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) November 13, 2018  GitHub Code\n\rPackages : ggplot2, ggrepel, ggthemr\rTidyTuesday : week 33\rData: malaria_deaths\rMalaria_deaths_plot: Plot shows the death rate per 100,000 people in Sri Lanka decreasing rapidly from 1996 to 2003 with a drop of 0.89 to 0.24, and by 2016 it reaches 0. While in 2015 this rate is 0.13. Highest death rate was in 1990 with 0.91.\r\r#data subset has been used\r#scales of x and y have been more scrutinized\r#labels have been added\r#x axis have been modified to accomodate the years Malaria_deaths_plot\u0026lt;-ggplot(subset.data.frame(malaria_deaths,Code==\u0026quot;LKA\u0026quot;),\raes(x=Year,\ry= `Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rgeom_point()+geom_line()+geom_text_repel()+\rggtitle(\u0026quot;Malaria Deaths for Both genders in a rate over the years in Sri Lanka\u0026quot;)+\rylab(\u0026quot;Deaths - Malaria - Sex: Both -\\n Age: Age-standardized (Rate) (per 100,000 people)\u0026quot;)+\rscale_y_continuous(breaks=seq(0.1,1,by=0.1) ,labels=seq(0.1,1,by=0.1))+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rtheme(axis.text.x = element_text(angle = 90))\r# print the plot\rprint(Malaria_deaths_plot)\r# save the plot\rggsave(Malaria_deaths_plot,width = 10,height = 10,dpi=300,\rfilename = \u0026quot;Malaria_Deaths_Sri Lanka.png\u0026quot;)\r\rData: malaria_deaths_age\rMalaria_deaths_age_plot: There are five categories in concern,where age category 15-49 has the most counts of in the range of 45-50. Second category is Under 5 close to 35 counts, while third category is 50-69 in-between 20-25. It should be noted that this order is for the year 1990. At the end of year 2015 this is not the case, where the categories and counts are 15-49 (close to 10), 50-69 (less than 10), 70 or order(close to 5), under 5(less than 5) and finally 5-14 (close to 0).\r\r#data subset has been used\r#according to age group colors are assigned\r#scales of x and y have been more scrutinized\r#labels have been added\r#x axis have been modified to accomodate the years Malaria_deaths_age_plot\u0026lt;-ggplot(subset.data.frame(malaria_deaths_age,code==\u0026quot;LKA\u0026quot;),\raes(x=year,y=deaths,color=factor(age_group)))+\rgeom_point()+geom_line()+\rggtitle(\u0026quot;Malaria Deaths by age category in Sri Lanka over the years\u0026quot;)+\rylab(\u0026quot;Deaths Count\u0026quot;)+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rscale_y_continuous(breaks=seq(0,60,by=5) ,labels=seq(0,60,by=5))+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_color_discrete(name=\u0026quot;Age Category\u0026quot;)\r# print the plot\rprint(Malaria_deaths_age_plot)\r# save the plot\rggsave(Malaria_deaths_age_plot,width=10,height = 10,dpi = 300,\rfilename = \u0026quot;Malaria Deaths Sri Lanka by Age.png\u0026quot;)\rMalaria Death count rate can be observed by countries or specific regions given in the data set. There are 9 regions in concern and each of these regions have sub parts or collections of countries. Variables included in this data-set are\n\rEntity - Full name of a country or region it is referred.\rCode - 3 letter ISO code for countries, but for regions there are no values.\rYear - Year range from 1990 to 2016.\rDeaths - Malaria - Sex : Both - Age : Age - Standardized (Rate) (per 100,000 people) - Number of people dead for both sexes per 100,000 people in a standardized age because of Malaria.\r\rThe Description of the data-set. I will focus on each region separately, because even inside the same region sub parts can behave differently. The 9 regions are\nSDI Countries\rSaharan Region\rEuropean Countries\rGreat Britain or United Kingdom\rAmerican Region\rAsian Countries\rNorth Africa and Middle East\rOceania Region\rCaribbean and Latin American Countries\r\r\rSDI Countries with Malaria Death Count Rate\rInitially there are 5 sub parts in SDI countries, but we can factor them into three based on their death rate behavior. Some regions are having higher death rate close to 75, while others have lower death rate close to 0.0002.\nFirst group is the sub regions which are having higher death rates than other SDI regions, which are Low SDI and Low-middle SDI countries. Low SDI countries are behaving poorly from 1990 with a 73.78 death rate which reaches it highest peek of 75.88 in 2001 after this death rate gradually declines until 2016 and reaches 37.87.\nThis is not the case for Low middle SDI countries where in 1990 death rate is 19.11 and reaches its peek of 22.01 in 2003 with small fluctuations. After this it gradually and slowly declines until 2016 while finishing in a rate of 15.7.\nattach(MD_CM_SDI)\r# scatter plot for Low and Low middle SDI\rggplot(subset(MD_CM_SDI,Entity == \u0026quot;Low SDI\u0026quot; | Entity == \u0026quot;Low-middle SDI\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Low-middle and Low SDI Countries\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ legend_bottom()+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(15,80,5),breaks =seq(15,80,5))\rSDI countries Middle and High-middle are performing better than the above two regions from 1990 itself. While Middle SDI countries have a very low death rate of close to 1 and periodically it decreases to 0.6 at 2016. To be exact in 1990 the death rate is 0.9274 and decrement occurs with small but unaffected fluctuations and reaches 0.6018 in 2016, which is the lowest point.\nFor High middle SDI countries the death rate in 1990 is 0.1168 and in 1996 it reaches the highest point of 0.1281. After 1996 there is clear decrease of death rate which reaches its minimum value of 0.0454 in 2013.\n# scatter plot for High middle and Middle\rggplot(subset(MD_CM_SDI,Entity == \u0026quot;Middle SDI\u0026quot;| Entity == \u0026quot;High-middle SDI\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Middle and High-middle SDI Countries\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,1,0.05),breaks =seq(0,1,0.05))\rHigh SDI countries are showing very strong decrease from 1990 it self. While in 1990 the death rate is 0.001387 and by polynomial decreasing by 2016 this reaches the least value of 0.000194.\n# scatter plot for High SDI\rggplot(subset(MD_CM_SDI, Entity ==\u0026quot;High SDI\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,6)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;High SDI Countries\u0026quot;)+ geom_text_repel()+ legend_bottom()+ geom_point()+ geom_line()+ scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.0014,0.0002),breaks =seq(0,0.0014,0.0002))\rdetach(MD_CM_SDI)\rIf we do plot all these 5 sub parts of SDI countries on one plot we would not have seen these differences and even if we do use facet grid this is not possible. Therefore, first I did plot all 5 regions together and then sub setted them based on their death rate change in values.\nIt is clear that High SDI values have the lowest death rate and Low, Low middle SDI have the highest death rates over the years 1990 to 2016.\n\rSaharan Region with Malaria Death Count Rate\rThere are 5 sub parts for Saharan region countries, which are Central Sub-Saharan, Eastern Sub-Saharan, Southern Sub-Saharan, Western Sub-Saharan and Sub-Saharan Africa countries. Here though it does not look like we need to separate these regions and plot them as groups. While plotting them as a whole they are clear and possible to interpret the differences in this line plot.\nCentral Sub-Saharan Africa and Western Sub-Saharan Africa countries behave similarly where they start with death rates respectively 108.67, 118.94. Further this death rate increases and reaches its highest peaks of 138.23 and 121.08 respectively for Western Sub-Saharan Africa and Central Sub-Saharan Africa countries for the year 2003. Finally they decrease into their lowest of 64.64 death rate for Central Sub-Saharan Africa and 87.54 death rate for Western Sub-Saharan Africa.\nSub-Saharan Africa Regions has a higher death rate performance through out the time line when it begins with 89.33 in 1990, and reaches its highest value of 96.28 in 2003. Further death rate decreases slowly up-to 51.93 in 2016, which is the least minimum value. Eastern Sub-Saharan Africa starts with 77.83 in 1990 and reaches its peak of 78.49 in 1991. In the next few years until 2000 there is small fluctuations. Finally there is a steep slope and reaches its lowest death rate value of 23.15 in 2016.\nDeath rate of Southern Sub-Saharan Africa is completely different than above four sub regions of Saharan Africa. In 1990 the death rate is 1.69 and it fluctuates until 2016, but it reaches the highest death rate of 2.57 in 2006. Further with this fluctuation in 2016 the death rate is 2.29 which is higher than the previous 7 years from 2016.\nattach(MD_CM_Sahara)\r# scatter plot for Sahara Region\rggplot(MD_CM_Sahara,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Saharan Africa Region\u0026quot;)+geom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ geom_text_repel()+\rtheme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\rdetach(MD_CM_Sahara)\rConsidering 1990 and 2016 the biggest decrease occurs for the region of Eastern Sub-Saharan Africa, while the lowest decrease is for Western Sub-Saharan Africa. While in the year gap of 1990 and 2016 only Southern Sub-Saharan Africa has an increase in death rate from 1.69 to 2.29.\n\rEuropean Countries with Malaria Death Count Rate\rI believe that European countries did not have any malaria related deaths even before 1990, because of their weather patterns. That is simply assured by here in this plot, where there is no deaths for the sub regions of Europe. Which are Central Europe, Eastern Europe and Western Europe.\nattach(MD_CM_EU)\r# scatter plot for European Region\rggplot(MD_CM_EU,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;European Region\u0026quot;)+ geom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+geom_text_repel()+\rscale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\rdetach(MD_CM_EU)\r\rGreat Britain or United Kingdom with Malaria Death Count Rate\rWe consider the collection of these countries as Great Britain or United Kingdom, which are England, Northern Ireland, Scotland and Wales. Here also weather pattern does have a high probability in causing a situation of no malaria related deaths. Further, these four countries are close to the European regions geographically therefore this assures us more that over the years from 1990 to 2016 the death rate is zero.\nattach(MD_CM_GB)\r# scatter plot for Great Britain or United Kingdom\rggplot(MD_CM_GB,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Great Britain or United Kingdom\u0026quot;)+ legend_bottom()+\rgeom_point()+geom_line()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,1,0.5),breaks =seq(0,1,0.5))\rdetach(MD_CM_GB)\r\rAmerican Region with Malaria Death Count Rate\rThere are 5 sub regions for American countries, yet we can divide them into two groups. One group will include Andean, Central and Tropical Latin American countries, and other group includes North America and Southern Latin American countries.\nIn the first group, Andean and Tropical Latin American countries begin with respectively 0.354, 0.351 death rates in 1990. This gradually decreases until 2016 for both regions, but Tropical Latin countries has a better decrement than Andean Latin American countries because they achieve death rate of respectively 0.038 and 0.048.\nIn this same time period of 1990 to 2016, Central Latin American begins with a death rate of 0.296 and reaches its lowest point of 0.053. In year 1994, Tropical Latin American region and Central Latin American region have the same death rate of 0.222. Further in the years 2004 and 2005 the death rates of Andean Latin American countries are 0.089, 0.071, but the same years Central Latin American region has death rates of 0.088, 0.080. Those are the two crucial changes which occur.\nattach(MD_CM_LA)\r# scatter plot for Andean, Central and Tropical Latin America\rggplot(subset(MD_CM_LA,Entity == \u0026quot;Andean Latin America\u0026quot;| Entity == \u0026quot;Central Latin America\u0026quot; | Entity == \u0026quot;Tropical Latin America\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,3)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Andean, Central and Tropical Latin American Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.40,0.05),breaks =seq(0,0.40,0.05))\rThe second group has North American region and Southern Latin American region. Clearly North America with its cold weather condition and developed status does not hold any deaths for malaria from 1990 to 2016. It is even possible to consider before this time range also there was no deaths for malaria. Yet Southern Latin American has death rate of 0.0208 in 1990 but decreases gradually and reaches its least minimum point of 0.0035 in 2016.\n# scatter plot for North and Southern Latin America\rggplot(subset(MD_CM_LA,Entity== \u0026quot;North America\u0026quot;| Entity ==\u0026quot;Southern Latin America\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;North America and Southern Latin American Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.021,0.001),breaks =seq(0,0.021,0.001))\rdetach(MD_CM_LA)\rComparing the American region only North American region is malaria free, while least amount progress has occurred in Central Latin American region. Tropical region has out performed Andean Latin American region. Next to North America, Southern Latin American has done more progress than other three regions.\n\rAsian Countries with Malaria Death Count Rate\rThere are 6 regions in the Asian region, which are South Asia, Southeast Asia, East Asia, High Income Asia Pacific, Australasia and Central Asia. Asia is a Large continent with wide variety of countries therefore we have more sub regions than any other continent here.\nSouth Asia and Southeast Asia are two regions which behave similarly where it begins in a higher rate in year 1990 and gradually decreasing until year 2016. South Asia has a death rate of 6.21 in 1990, but in 2016 it reaches to 3.61. For Southeast Asian region the death rate is 5.29 in year 1990 and reaches the death rate of 2.56 in year 2016.\nattach(MD_CM_A)\r# scatter plot for South Asia and Southeast Asia\rggplot(subset(MD_CM_A,Entity == \u0026quot;South Asia\u0026quot; | Entity == \u0026quot;Southeast Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rcolor=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;South Asia and Southeast Asia Region\u0026quot;)+ geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(2.4,6.4,0.2),breaks =seq(2.4,6.4,0.2))\rEast Asia region begins with a death rate of 0.0269 in year 1990 and it decreases over the coming years which will reach the lowest death rate of 0.0121 in year 2015. Further in year 2016 it slightly increases to 0.0122.\n# scatter plot for East Asia\rggplot(subset(MD_CM_A, Entity == \u0026quot;East Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;East Asia Region\u0026quot;)+ geom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.01,0.03,0.002),breaks =seq(0.01,0.03,0.002))\rCentral Asia region begins with a a death rate of 0.0304 in year 1990 and increases very slowly until year 1998 towards the death rate of 0.0314. After this the death rate decreases gradually until year 2013 where it becomes 0.0057. Until year 2016 this becomes the standard death rate for malaria in Central Asia Region.\n# scatter plot for Central Asia\rggplot(subset(MD_CM_A,Entity == \u0026quot;Central Asia\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Central Asia Region\u0026quot;)+ geom_text_repel()+\rgeom_point()+geom_line()+ legend_bottom()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0,0.05,0.005),breaks =seq(0,0.05,0.005))\rHigh Income Asia Pacific countries have a lower death rate than previous Asian region countries where in 1990 the death rate is 0.0073 but over the years it decreases gradually and reaches the lowest death rate value of 0.0008 in year 2016.\nAustralasia countries have no death rate over the year range of 1990 to 2016, which implicate that there is a possibility that before 1990 also there could have not been any deaths related to malaria\n# scatter plot for High Income Pacific and Australasia\rggplot(subset(MD_CM_A, Entity == \u0026quot;Australasia\u0026quot; | Entity == \u0026quot;High-income Asia Pacific\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;High Income Asia Pacific and Australasia Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.001,0.008,0.0005),breaks =seq(0.001,0.008,0.0005))\rdetach(MD_CM_A)\rAustralasia is the only region which does not have malaria over the years given in the data set. The lowest death rate occurs to High Income Asian Pacific region than other regions of Asia.\n\rNorth Africa and Middle East Region with Death Count Rate\rNorth Africa and Middle East region is the only odd region of all in this data set for malaria death rate. In year 1990 the death rate is 0.7968 but in the coming years it increases until 2005 and reaches 1.3444. Despite this increment after 2005 the death rate suddenly and rapidly drops until year 2011 and reaches its lowest point of 0.731. Again there is a steady but slow increase in death rate and reaches 0.8332 in year 2016.\nattach(MD_CM_A)\r# scatter plot to North Africa and Middle East\rggplot(subset(MD_CM_A, Entity == \u0026quot;North Africa and Middle East\u0026quot; ),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rggtitle(\u0026quot;North Africa and Middle East Region\u0026quot;)+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.5,1.5,0.05),breaks =seq(0.5,1.5,0.05))\rdetach(MD_CM_A)\rComparing all the other regions in this data set only North Africa and Middle East Region was very special with its odd fluctuations. At the end comparing the death rates of year 1990 and 2016 there is an increase close to 0.04, but it is far less than what was in year 2005 and 1990.\n\rOceania Region with Death Count Rate\rOceania region begins the malaria death rate of 14.78 in 1990, even though it oscillates over the next few years and reaches a death rate of 12.5 in 1998 and in year 2000 it reaches a staggering highest peak of 53.18 death rate. After reaching the highest peak it decreases slowly in the next few years and reaches its lowest point of 8.71 in 2016.\nattach(MD_CM_A)\r# Scatter plot to Oceania\rggplot(subset(MD_CM_A,Entity == \u0026quot;Oceania\u0026quot;),\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Oceania Region\u0026quot;)+\rgeom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+scale_y_continuous(labels = seq(6,54,2),breaks =seq(6,54,2))\rdetach(MD_CM_A)\rOceania region behaves very differently than other regions. By the year 2015 lowest death rate of 8.64 occurs for Oceania region.\n\rCaribbean and Latin America and Caribbean Countries with Death Count Rate\rCaribbean region countries in the 1990 has a malaria death rate of 0.1265, but next year it is 0.1519. Further it decreases over the years with some fluctuations. By year 2016 the death rate reaches 0.058.\nLatin American and Caribbean region countries begin with the death rate of 0.3043 in 1990, but it exponentially decreases until 2016 and reaches 0.0468. In the course it out performs Caribbean countries after year 1999 where Latin American and Caribbean countries occupy a death rate of 0.1282, Caribbean countries occupy a death rate of 0.1274.\nattach(MD_CM_C_LA)\r# Scatter plot to Caribbean and Latin America\rggplot(MD_CM_C_LA,\raes(x=Year,\ry=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\rlabel=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\rylab(\u0026quot;Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\u0026quot;)+\rggtitle(\u0026quot;Caribbean and Latin American and Caribbean Region\u0026quot;)+\rgeom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\rscale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ theme(axis.text.x = element_text(angle = 90))+\rscale_y_continuous(labels = seq(0.0,0.35,0.05),breaks =seq(0.0,0.35,0.05))\rdetach(MD_CM_C_LA)\rClearly Latin American and Caribbean countries perform better than Caribbean countries after year 1999 until year 2016. At the beginning of 1990 the death rate gap between both these regions are clearly high.\n\rConclusion\rI shall conclude my findings in point form\n\rDeath rate for malaria decreases over the years with small fluctuations over the years and reaches zero in 2016 for Sri Lanka. By 2016 all age categories reach a death count of zero, but in 1990 they are at different parts of the scale.\n\rHigh SDI countries have a low death rate. While Low and Low middle SDI countries have the highest death rates over the years from 1990 to 2016.\n\rIn the Saharan region lowest death rate is for Southern Sub-Saharan region and highest death rate is for Western Sub-Saharan region on the year 2016. This order is same for year 1990 as well in these regions. Highest death rate of all years and all sub regions occurs in year 2003 of 138.23 for Western Sub-Saharan Africa. While lowest death rate of all years and all sub regions is in year 1994 for Southern Sub-Saharan Africa with 1.55.\n\rEuropean and Great Britain related regions have no malaria related incidents over the years of 1990 to 2016, this could be because of their cold climate pattern and their developed status in the world.\n\rNorth America shows figures of no malaria related deaths over the years of 1990 to 2016. Southern Latin America has the lowest death rate of 0.0035 in 2016 and highest death rate is for Central Latin America. But in year 1990 highest death rate is for Andean Latin American region which is 0.354 and least death rate is for Southern Latin America with 0.0208.\n\rAustralasia has no death rate over the years of 1990 to 2016. Where in year 2016 lowest death rate occurs to High Income Asia Pacific of 0.0008 and highest in that year for Oceania region with a value of 8.71. In the year 1990 highest death rate is for Oceania region with 14.78 but lowest is for High Income Asia Pacific region with 0.0073. A special occurrence where comparing all the regions and years the highest death rate is for Oceania Region in year 2000.\n\rHighest death rate occurs in year 2005 for North Africa and Middle East region, but in 1990 the malaria death rate is 0.7968. Further in 2011 it hits the lowest value of 0.731 and finally moves up-to 0.8332 in year 2016.\n\rIn year 1990 Caribbean region has a lower death rate than Latin American and Caribbean region. But this gap gradually decreases over time and by year 2016 Caribbean region reaches a death rate of 0.058 while Latin America and Caribbean region occupies the rate of 0.0468.\n\r\r\rFurther Analysis\r\rThis article only focuses on the death rate but we can expand it to countries and compare them over continents.\n\rIt is possible to consider the age category and compare the death counts of different regions to understand further of these regions.\n\r\rPlease see that\nThis is my third post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1542758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542758400,"objectID":"3a7209ec23992bf87c2ebd13be15ad01","permalink":"/post/week_33/week-33-malaria-deaths/","publishdate":"2018-11-21T00:00:00Z","relpermalink":"/post/week_33/week-33-malaria-deaths/","section":"post","summary":"2018 Week 33 TidyTuesday: Malaria Deaths.","tags":["R","R package","TidyTuesday"],"title":"Week 33 : Malaria Deaths","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r\rIntroduction\rOperating Systems\rR versions\rDate versus Operating System\rDownload Size and IP ID\rConclusion\rFurther Analysis\r\r\r# load the packages\rlibrary(readr)\rlibrary(ggplot2)\rlibrary(lubridate)\rlibrary(ggthemr)\rlibrary(gridExtra)\rlibrary(magrittr)\rlibrary(knitr)\rlibrary(kableExtra)\rlibrary(readr)\r# load the data\rr_downloads_year \u0026lt;- read_csv(\u0026quot;r_downloads_year.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rr_downloads \u0026lt;- read_csv(\u0026quot;r-downloads.csv\u0026quot;, col_types = cols(X1 = col_skip(), date = col_date(format = \u0026quot;%Y-%m-%d\u0026quot;), time = col_time(format = \u0026quot;%H:%M:%S\u0026quot;)))\rWeek 31 : R and Package Downloads #TidyTuesday https://t.co/hALK5o4bGd\n\u0026mdash; Amalan Mahendran (@Amalan_Con_Stat) December 3, 2018  GitHub Code\nIntroduction\rTidy Tuesday is a very good move to improve R programming for anyone who is interested in statistics. Data-sets are uploaded every Tuesday, and plots are published under the #tidytuesday. This is just me presenting a few accumulated plots for the data-set r-downloads.csv and r_downloads_year.csv.\nI shall be focusing on the data set provided on 2018 October 30th, Which is R and Package download stats. My main objective is to understand how Sri Lankan users have behaved in this data-set.\nThe packages used in R are readr,ggplot2, lubridate, ggthemr, gridExtra, magrittr, knitr and kableExtra.\nThere are 701 downloads occurred in between the given time limit of 2017 October 20th to 2018 October 20th in Sri Lanka. Similarly, if we look at the downloads on the day of 2018 October 23rd, which is 3 observations. There are 7 variables to be concerned, which are\n\rdate - date of download (y-m-d)\rtime - time of download (in UTC)\rsize - size in bytes\rversion - R release version\ros - Operating System\rcountry - Two letter ISO country code\rip_id - Anonymized daily ip code(unique identifier)\r\r# extracting the observations only if the country is Sri Lanka\rr_downloads_year_LK\u0026lt;-subset.data.frame(r_downloads_year,country==\u0026quot;LK\u0026quot;)\rr_downloads_LK\u0026lt;-subset.data.frame(r_downloads,country==\u0026quot;LK\u0026quot;)\r# number of observations #dim(r_downloads_year_LK)\r#dim(r_downloads_LK)\r\rOperating Systems\rWindows is not a favorable operating system for open source programming was my myth. Well, No longer I shall believe that if it is considering Sri Lankans and R programming.\n#checking what type of operating systems are in use\rggthemr(\u0026quot;flat dark\u0026quot;)\rggplot(r_downloads_year_LK,aes(x=os))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;, aes(label=..count..), vjust=-0.5)+\rxlab(\u0026quot;Operating System\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks=seq(0,675,by=25))+\rggtitle(\u0026quot;Operating system preference of Sri lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\r# frequency table for Operating system\rtab1\u0026lt;-round(prop.table(table(r_downloads_year_LK$os)),4)\rtab1\u0026lt;-as.data.frame(tab1)\rnames(tab1)\u0026lt;-c(\u0026quot;Operating System\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab1) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to Operating system\u0026quot;=2))\r\r\rFrequency table to Operating system\r\r\r\r\rOperating System\r\rFrequency\r\r\r\r\r\rosx\r\r0.0371\r\r\r\rsrc\r\r0.0271\r\r\r\rwin\r\r0.9358\r\r\r\r\rMajority of users have used Windows which is 93.58%, while Mac users are represented with 3.71% and finally 2.71% from the source file. Next, focusing on the R versions downloaded.\n\rR versions\rVersions are updated regularly for R and a grand update occurred on 2018 April for the version 3.5.0. Further, versions 3.4.3 and 3.4.4 were updated in the time gap considered. There are versions from 3.0.0 and higher for Sri Lankan users. It is crucial to study this where we can understand how far does the user have knowledge about R and updating the software version.\n#checking what type of R versions were downloaded\rggplot(r_downloads_year_LK,aes(x=version))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;, aes(label=..count..), vjust=-0.5)+\rxlab(\u0026quot;R versions\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = seq(0,275,by=25))+\rggtitle(\u0026quot;R versions downloaded of Sri Lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rTable shows that version 3.5.1 represents a 36.52% followed by version 3.4.3 of 27.96% and in the third place version 3.5.0 with 15.98%. Further, all downloads are occurred for versions 3.0.0 or higher than it. People believed in 3.4.3 than 3.5.0, which could only mean that 3.4.3 was more stable for user and package requirements.\n# frequency table to R versions\rtab2\u0026lt;-sort(round(prop.table(table(r_downloads_year_LK$version)),4))\rtab2\u0026lt;-as.data.frame(tab2)\rnames(tab2)\u0026lt;-c(\u0026quot;R Version\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab2) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to R versions\u0026quot;=2))\r\r\rFrequency table to R versions\r\r\r\r\rR Version\r\rFrequency\r\r\r\r\r\r3.0.0\r\r0.0014\r\r\r\r3.2.2\r\r0.0014\r\r\r\r3.4.0\r\r0.0014\r\r\r\r3.3.3\r\r0.0029\r\r\r\r3.4.1\r\r0.0029\r\r\r\r3.4.4\r\r0.0899\r\r\r\r3.4.2\r\r0.0956\r\r\r\r3.5.0\r\r0.1598\r\r\r\r3.4.3\r\r0.2796\r\r\r\r3.5.1\r\r0.3652\r\r\r\r\rIf we further divide the operating systems bar plot with respective to R version it is clearly seen that only versions 3.5.1, 3.5.0, 3.4.4, 3.4.3 and 3.4.2 have maintained importance for the windows operating system.\n# Checking what type of operating system is used with R version\r#setting 10 colors becuase flat dark theme only has four originally\rset_swatch(c(\u0026quot;white\u0026quot;,\u0026quot;firebrick1\u0026quot;,\u0026quot;gold\u0026quot;,\u0026quot;darkorange\u0026quot;,\u0026quot;dodgerblue\u0026quot;,\u0026quot;darkblue\u0026quot;,\r\u0026quot;forestgreen\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;grey\u0026quot;,\u0026quot;grey44\u0026quot;,\u0026quot;black\u0026quot;))\rggplot(r_downloads_year_LK,aes(x=os,fill=version))+geom_bar()+\rgeom_text(stat=\u0026#39;count\u0026#39;,aes(y=..count..,label=..count..),position=\u0026quot;stack\u0026quot;,vjust=1)+\rxlab(\u0026quot;Operating System\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks=seq(0,675,by=25))+\rggtitle(\u0026quot;Operating system preference of Sri lankans for R\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\r#Contingency table to R version versus operating System\rkable(round(prop.table(table(r_downloads_year_LK$os,\rr_downloads_year_LK$version)),4)) %\u0026gt;%\rkable_styling(bootstrap_options = \u0026quot;striped\u0026quot;,full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Contingency table for R version vs Operating System\u0026quot;=11))\r\r\rContingency table for R version vs Operating System\r\r\r\r\r\r3.0.0\r\r3.2.2\r\r3.3.3\r\r3.4.0\r\r3.4.1\r\r3.4.2\r\r3.4.3\r\r3.4.4\r\r3.5.0\r\r3.5.1\r\r\r\r\r\rosx\r\r0.0000\r\r0.0000\r\r0.0029\r\r0.0000\r\r0.0000\r\r0.0100\r\r0.0086\r\r0.0000\r\r0.0071\r\r0.0086\r\r\r\rsrc\r\r0.0014\r\r0.0014\r\r0.0000\r\r0.0000\r\r0.0029\r\r0.0043\r\r0.0057\r\r0.0000\r\r0.0014\r\r0.0100\r\r\r\rwin\r\r0.0000\r\r0.0000\r\r0.0000\r\r0.0014\r\r0.0000\r\r0.0813\r\r0.2653\r\r0.0899\r\r0.1512\r\r0.3466\r\r\r\r\rWindows operating system percentages indicate that 34.66% of users have chosen version 3.5.1, 26.53% have chosen version 3.4.3 and finally version 3.5.0 with 15.12%. Close to 9% is represented by version 3.4.2 and 3.4.4.\n\rDate versus Operating System\rDate is a difficult variable in statistics therefore I have disseminated the date into 4 types, which are month(January to December), day(1-31), hour(0-23) and minutes(0-59). Further, I have tried to understand what type of operating systems were used in those time types.\n#checking which months the downloads occured inrespecitive to operating system\rggthemr(\u0026quot;flat dark\u0026quot;)\rggplot(r_downloads_year_LK,aes(x=month(date),fill=os))+\rgeom_bar()+ xlab(\u0026quot;Months\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks =seq(0,140,10))+\rscale_x_continuous(breaks=1:12) +\rggtitle(\u0026quot;Operating systems used in the months of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rSo, the first sub part of time is months. Here, we are considering 12 months of an year and months August and April reflects no Operating System is better than Windows property. While, August holding the least amount of downloads with slightly above frequency 20. Highest frequency occurs to October with count higher than 130 and significantly osx and src types of files also have higher amount than any-other month. Except August only the month of December has counts higher than 100.\n#checking which days the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=day(date),fill=os))+\rgeom_bar()+xlab(\u0026quot;Days\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks =seq(0,80,10))+\rscale_x_continuous(breaks=1:31)+\rggtitle(\u0026quot;Operating systems used in the days of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rNext, focusing on the days it is clear that 10th and 11th have most downloads respectively reaching more than 60 and 70 in counts, while in other days it is mostly less than 30. Further, clearly on the 31st it includes least frequency of 10 because 31st is not a common day of all 12 months. It would be very tiring to focus on operating systems individually, but to be fair there is clear sign of few days with only the use of windows, and a few days with combination of other operating systems with windows.\n#checking which hour the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=hour(time),fill=os))+\rgeom_bar()+xlab(\u0026quot;Hour\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = seq(0,100,5))+\rscale_x_continuous(breaks=0:23)+\rggtitle(\u0026quot;Operating systems used in the Hours of the day of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)\rI have been curious at this part because of wanting to know at which hour of the day did our Sri Lankan users download R and packages. Yet it should be noted that the hour of time could be local time of Sri Lanka or otherwise. Still according to the bar chart the hours 4th and 5th have most downloads with counts of above 80 and above 90 respectively. Where in the 21st hour it reaches the least amount of less than 5 counts. Most of the frequencies are in the range of 10 and 35.\n#checking which minute the downloads occured inrespecitive to operating system\rggplot(r_downloads_year_LK,aes(x=minute(time),fill=os))+\rgeom_bar()+xlab(\u0026quot;Minute\u0026quot;) +ylab(\u0026quot;Frequency\u0026quot;)+\rscale_y_continuous(breaks = 0:25)+\rscale_x_continuous(breaks=0:59)+\rggtitle(\u0026quot;Operating systems used in the minutes of the day of R download\u0026quot;,\rsubtitle = \u0026quot;2017 October 20th - 2018 October 20th\u0026quot;)+\rcoord_flip()\rLooking at the minutes it is very spread out. Focusing on special occasions only four minutes which are 59th, 46th, 12th and 0th have counts more than 20. While 51st minute has a count of 2. Rather than this nothing more significant occurs here. I think considering these counts in perspective of specific operating systems is tedious amount of work and waste of time.\n\rDownload Size and IP ID\rPackages were downloaded but none of their names were given in this data-set. Therefore we cannot know which package were downloaded. Yet we can identify the package sizes which were downloaded most. According to the table an R package with 82375220 bytes has most downloads of 50, while second place goes to to a size of 82375219 bytes and finally in third place is for 82375216 bytes with 39 counts.\n# table of frequency for sizes of download\rtab3\u0026lt;-as.data.frame(sort(table(r_downloads_year_LK$size))%\u0026gt;% tail(5))\rnames(tab3)\u0026lt;-c(\u0026quot;Size\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab3) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to Download size\u0026quot;=2))\r\r\rFrequency table to Download size\r\r\r\r\rSize\r\rFrequency\r\r\r\r\r\r82877772\r\r18\r\r\r\r78171328\r\r22\r\r\r\r82375216\r\r39\r\r\r\r82375219\r\r48\r\r\r\r82375220\r\r50\r\r\r\r\rLooking at the IP ID it is clear that 334 has the highest downloads of 55, while second place goes to 1060 with 46 downloads. Finally, ID number 1286 has 16 downloads with third place.\n# table of frequency to IP ID\rtab4\u0026lt;-as.data.frame(sort(table(r_downloads_year_LK$ip_id))%\u0026gt;% tail(5))\rnames(tab4)\u0026lt;-c(\u0026quot;IP ID\u0026quot;,\u0026quot;Frequency\u0026quot;)\rkable(tab4) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F,position=\u0026quot;left\u0026quot;) %\u0026gt;%\radd_header_above(c(\u0026quot;Frequency table to IP ID\u0026quot;=2))\r\r\rFrequency table to IP ID\r\r\r\r\rIP ID\r\rFrequency\r\r\r\r\r\r623\r\r9\r\r\r\r157\r\r12\r\r\r\r1286\r\r16\r\r\r\r1060\r\r46\r\r\r\r334\r\r55\r\r\r\r\r\rConclusion\rI shall conclude my findings in point form\n\rMost of Sri lankans (93.58%) use windows as a OS for R downloads\n\rTop three R versions are 3.5.1, 3.4.3 and 3.5.0 with percentages respectively 36.52% 27.96% and 15.98%.\n\rWindows users use versions 3.5.1, 3.4.3 and 3.5.0 with percentages 34.56%, 26.53% and 3.5.0.\n\rMost of the downloads occur in the months October and December, while days are 10th and 11th, while hours are 3rd and 4th and minutes of 59th, 46th, 12th and 0th.\n\rDownload size of 82375220 bytes happens with the highest count of 50, while the IP ID of 334 has most downloads of 55.\n\r\r\rFurther Analysis\r\rWe can do similar analysis for other countries and compare them.\n\rUsing Size it should be possible to understand what is being downloaded.\n\r\rPlease see that\nThis is my Second post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"31323fc02dbea618333dbe5dfe552575","permalink":"/post/week_31/week-31-r-and-package-downloads/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/post/week_31/week-31-r-and-package-downloads/","section":"post","summary":"2018 Week 31 TidyTuesday: R and R package Downloads.","tags":["TidyTuesday","R downloads","R","R package"],"title":"Week 31 : R and Package Downloads","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"\r\rMovie Profit, Not So Profit\rUnderstand Genre and Mpaa Rating on Movies\rLets Focus of movies which has zero domestic gross\rZero Domestic gross Point of View\rGenre and MPAA Rating Point of View\rFinding Outliers in Perspective of Genre and MPAA Rating\rProduction Budget and Worldwide Gross\rYears, Months and Days versus Production Budget\rConclusion\rFurther Analysis\r\r\rMovie Profit, Not So Profit\rThis is my first post on Tidy Tuesday and the data-set in question is Movie profit data-set. Even though the title of data says Movie profit I am going to focus on the movies which did not generate any revenue domestic and suggest on gross in worldwide.\nGitHub Code\nThe packages that I have used here are magrittr, tidyverse, scales, ggthemr, knitr, kableExtra, ggthemr and lubridate. The theme I am using for plots is “flat dark”.\n\rUnderstand Genre and Mpaa Rating on Movies\r3401 movies with 8 variables of information which include numeric and categorical. There are 202 distributors for movies of four types of ratings which are G, PG, PG-13 and R, but 137 movies have no record of them. Also there are five categories for genre, where Drama with 1236, while horror with 298 movies.\n# Loading the packages\rlibrary(magrittr)\rlibrary(tidyverse)\rlibrary(scales)\rlibrary(ggthemr)\rlibrary(knitr)\rlibrary(kableExtra)\rlibrary(lubridate)\r# load the data\rMovie \u0026lt;- read_csv(\u0026quot;movie_profit.csv\u0026quot;, col_types = cols(X1 = col_skip(), release_date = col_date(format = \u0026quot;%m/%d/%Y\u0026quot;)))\r# Load the theme\rggthemr(\u0026quot;flat dark\u0026quot;)\r# looking at dimensions\rdim(Movie)\rattach(Movie)\r# Bar plot to Genre\rggplot(Movie,aes(genre))+\rgeom_bar()+stat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rxlab(\u0026quot;Genre\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;All movies in perspevtive of Genre \u0026quot;)\rThere are five types of ratings but around half of them are R rated, while 1094 are PG-13. While 573 are in category of PG and G rated movies are only 85. Finally, 137 movies do not have any ratings.\n# Bar plot to MPAA Ratings\rggplot(Movie,aes(mpaa_rating))+\rgeom_bar()+geom_bar()+\rstat_count(aes(y=..count.., label=..count..),geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rxlab(\u0026quot;Rating\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+\rggtitle(\u0026quot;All movies in persepvtie of MPAA Rating\u0026quot;)\rComedy movies are mostly R rated(under 17 requires guardian) and PG-13 (some material is inappropriate to under 13). Where the frequencies are respectively 367 and 328. 309 movies of adventure genre could be watched by children with accompanying parents and 67 movies can be watched by all ages.\nYet 645 Drama movies are R-rated.There is only one action movie for general audiences(for all) and obviously no horror film should be watched by children alone, yet there are 7 movies which you can watch with your parents.\n#checking for bias in mpaa rating and genre\rkable(table(mpaa_rating,genre),\u0026quot;html\u0026quot;) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Contingency table in counts for Genre versus MPAA Rating\u0026quot;=6)) \r\r\rContingency table in counts for Genre versus MPAA Rating\r\r\r\r\r\rAction\r\rAdventure\r\rComedy\r\rDrama\r\rHorror\r\r\r\r\r\rG\r\r1\r\r67\r\r6\r\r11\r\r0\r\r\r\rPG\r\r34\r\r309\r\r79\r\r144\r\r7\r\r\r\rPG-13\r\r225\r\r83\r\r328\r\r398\r\r58\r\r\r\rR\r\r286\r\r14\r\r367\r\r645\r\r202\r\r\r\r\rWe think horror movies are mostly R-rated then it is true. But only it is explainable by percentage. Yet considering the amount of horror movies made generally it is very low even in this random sample. Action and Comedy movies have very close percentages for PG-13 ratedness, while 52% are R rated for Action and 47% are comedy.\n# column percentage for above table\rkable(table(mpaa_rating,genre) %\u0026gt;%\rprop.table(margin=2) %\u0026gt;%\rround(digits = 2)) %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;),full_width = T) %\u0026gt;%\radd_header_above(c(\u0026quot;Percentage table for Genre versus MPAA Rating\u0026quot;=6))\r\r\rPercentage table for Genre versus MPAA Rating\r\r\r\r\r\rAction\r\rAdventure\r\rComedy\r\rDrama\r\rHorror\r\r\r\r\r\rG\r\r0.00\r\r0.14\r\r0.01\r\r0.01\r\r0.00\r\r\r\rPG\r\r0.06\r\r0.65\r\r0.10\r\r0.12\r\r0.03\r\r\r\rPG-13\r\r0.41\r\r0.18\r\r0.42\r\r0.33\r\r0.22\r\r\r\rR\r\r0.52\r\r0.03\r\r0.47\r\r0.54\r\r0.76\r\r\r\r\rThis data-set contains the release dates from 1956 to 2019. Even though it is not 2019 there is a movie which has been listed here. This explains the domestic and worldwide gross being zero as zero. Then again we have to be careful because there are movies which might not make profit at all, domestic or other wise.\n\rLets Focus of movies which has zero domestic gross\rNo revenue from 66 movies, that is interesting. So obviously Aqua man has a whopping more than 150 million dollars\nproduction budget and no profit because it was not released yet when this data set was compiled. Second rank is for “Wonder park” with 100 million dollars. This movie will be released in 2019.\n\rZero Domestic gross Point of View\rSurprisingly there are movies without any production budget information because I am very sure No movie is done for free. Specially it is odd to see “12 Angry Men” in this list, which leads to the conclusion not all Movies in this list are to be on-it in the first place. We have 66 movies to consider.\n# domestic gross zero only movies\rMovie_domestic_zero\u0026lt;-subset.data.frame(Movie,c(domestic_gross==0)) # checking dimensions\rdim(Movie_domestic_zero)\rattach(Movie_domestic_zero)\r# Scatterplot for production budget\rggplot(Movie_domestic_zero,aes(x=reorder(movie,production_budget),\ry=production_budget))+\rgeom_point()+theme(axis.text.x =element_text(angle = 90, hjust = 1))+\rscale_y_continuous(labels = dollar_format())+\rylab(\u0026quot;Production budget\u0026quot;)+xlab(\u0026quot;Movie names\u0026quot;)+\rggtitle(\u0026quot;Domestic Gross Zero but how production budget varies in Movies\u0026quot;)\r\rGenre and MPAA Rating Point of View\rSo movies with R ratedness have the most count and they are also action and drama genre movies of count 10. Here also there are 11 movies with have not been classified into any rating. Finally, there is no G rated movie in this graphical representation. Majority of movies (31) are from R rated in related to rating. While considering genre the Drama category is represented by 24. Action, Drama and Horror movies includes missing rating.\n#plotting worldwide gross with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(x=mpaa_rating,fill=genre)) +\rgeom_bar(position = \u0026quot;stack\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;MPAA Rating\u0026quot;)+\rgeom_text(aes(label=..count..),stat=\u0026#39;count\u0026#39;,position=position_stack(0.4))+\rggtitle(\u0026quot;MPAA Rating counts with Genre\u0026quot;)\r#plotting worldwide gross with mpaa rating\rMovie_domestic_zero %\u0026gt;% ggplot(aes(fill=mpaa_rating,x=genre)) +\rgeom_bar(position = \u0026quot;stack\u0026quot;)+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Genre\u0026quot;)+\rgeom_text(aes(label=..count..),stat=\u0026#39;count\u0026#39;,position=position_stack(0.4))+\rggtitle(\u0026quot;Genre counts with MPAA Rating\u0026quot;)\r\rFinding Outliers in Perspective of Genre and MPAA Rating\rConsidering the box-plot there are 3 outliers in Drama while plotting data in perspective of genre. Most amount of production budget is concluded in Action genre, while the least is in Horror.\nIf we focus on the production budget with genre there are 7 outliers and one action movie has spent 100 million dollars, similarly a movie from adventure category spent more than 100 million dollars. Others production budget is way less than 50 million dollars.\n#plotting production budget with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(genre,production_budget)) +\rgeom_boxplot()+ylab(\u0026quot;Production Budget\u0026quot;)+\rxlab(\u0026quot;Genre\u0026quot;)+\rscale_y_continuous(labels = dollar_format())+\rexpand_limits(y=0)+coord_flip()+\rggtitle(\u0026quot;Boxplot for Production budget in perspective of Genre\u0026quot;)\rLeast amount budget is spent on movies of no rating mentioned while most is on PG-13 rated movies and it has one strong outlier. Previously with genre we had 7 outliers but according to MPAA rating there are only 6 outliers.\n#plotting production budget with genre\rMovie_domestic_zero %\u0026gt;% ggplot(aes(mpaa_rating,production_budget)) +\rgeom_boxplot()+ylab(\u0026quot;Production Budget\u0026quot;)+\rxlab(\u0026quot;MPAA rating\u0026quot;)+\rscale_y_continuous(labels = dollar_format())+\rexpand_limits(y=0)+coord_flip()+\rggtitle(\u0026quot;Boxplot for Production budget in perspective of MPAA Rating\u0026quot;)\r\rProduction Budget and Worldwide Gross\rAccording to the ascending order in the list of 10 movies with lowest production budget only 2 have profited. One movie (All the Boys Love Mandy Lane) has considerably done good, but if you consider this list of 10 movies we have 12 angry men as well.\n# ascending order of top ten movies in production budget\rkable(Movie_domestic_zero[order(Movie_domestic_zero$production_budget),-c(1,4)] %\u0026gt;% head(10),\rcol.names=c(\u0026quot;Movie Name\u0026quot;,\u0026quot;Production Budget\u0026quot;,\u0026quot;Wordlwide Gross\u0026quot;,\u0026quot;Distributor\u0026quot;,\r\u0026quot;MPAA Rating\u0026quot;,\u0026quot;Genre\u0026quot;)) %\u0026gt;% kable_styling(full_width = T,font_size = 13) %\u0026gt;%\radd_header_above(c(\u0026quot;Top 10 Least production budget movies for Domestic gross 0\u0026quot;=6))\r\r\rTop 10 Least production budget movies for Domestic gross 0\r\r\r\r\rMovie Name\r\rProduction Budget\r\rWordlwide Gross\r\rDistributor\r\rMPAA Rating\r\rGenre\r\r\r\r\r\r12 Angry Men\r\r340000\r\r0\r\rUnited Artists\r\rNA\r\rDrama\r\r\r\rMy Beautiful Laundrette\r\r400000\r\r0\r\rOrion Classics\r\rNA\r\rDrama\r\r\r\rEverything Put Together\r\r500000\r\r7890\r\rNA\r\rR\r\rDrama\r\r\r\rAll the Boys Love Mandy Lane\r\r750000\r\r1960521\r\rRadius\r\rR\r\rHorror\r\r\r\rJimmy and Judy\r\r1000000\r\r0\r\rOutrider Pictures\r\rR\r\rAction\r\r\r\rThe Poker House\r\r1000000\r\r0\r\rPhase 4 Films\r\rR\r\rDrama\r\r\r\rProud\r\r1000000\r\r0\r\rCastle Hill Product…\r\rPG\r\rDrama\r\r\r\rSteppin: The Movie\r\r1000000\r\r0\r\rWeinstein Co.\r\rPG-13\r\rComedy\r\r\r\rZombies of Mass Destruction\r\r1000000\r\r0\r\rAfter Dark\r\rR\r\rComedy\r\r\r\rGrand Theft Parsons\r\r1200000\r\r0\r\rSwipe Films\r\rPG-13\r\rDrama\r\r\r\r\rThis indicates that we didn’t have records how much of profit in home and away properly, because there is no way that people did not watch that movie and not make any gross. So we conclude that some movies which were released before 1970s did not pertain any information of gross domestic or worldwide.\n\rYears, Months and Days versus Production Budget\rThere are 4 Movies before 1972 with zero for domestic gross which can conclude loss of information. Oddly in year 2014 there are 8 movies with zero domestic gross and most of the movies are after year 2000.\n# plotting years vs movies released\rggplot(Movie_domestic_zero,aes(x=year(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Years\u0026quot;)+\rscale_x_continuous(label=1956:2019,breaks=1956:2019)+\rscale_y_continuous(labels = 0:8,breaks = 0:8)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)+\rtheme(axis.text.x = element_text(angle = 90,hjust = 2))\rConsidering the months there is no specialty most of the counts are in-between 3 and 8. In January there are only two movies.\nggplot(Movie_domestic_zero,aes(x=month(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Months\u0026quot;)+\rscale_x_continuous(label=1:12,breaks=1:12)+\rscale_y_continuous(labels = 0:8,breaks = 0:8)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)\rOnly the movies with release dates 19th and 25th have no domestic gross, while highest count of 6 occurs on 21st. Most of the days have the count of 1 movie.\nggplot(Movie_domestic_zero,aes(x=day(release_date)))+\rgeom_bar()+ylab(\u0026quot;Frequency\u0026quot;)+xlab(\u0026quot;Days\u0026quot;)+\rscale_x_continuous(label=1:31,breaks=1:31)+\rscale_y_continuous(labels = 0:6,breaks = 0:6)+\rstat_count(aes(y=..count.., label=..count..), geom=\u0026quot;text\u0026quot;, vjust=-.5)\r\rConclusion\rMy conclusion of the above plots and tables in point form\n\rIn the complete data set Drama Genre has most (1236) counts and least (298) count goes to Horror. While, most (1514) of the Movies are R rated, least count goes to Grated movies. Considering Genre and Rating, it is true that Horror movies are R rated while it represents 76%, and should not let children watch alone.\n\rWhile there are movies with No domestic gross some have not been released yet (Aqua man and Wonder Park). Further, some Movies do not even have worldwide gross. This causes missing information. Even though a famous movies such as “12 Angry Men”.\n\rThis missing information could be the related to the fact that there are 4 movies which were released before 1972. Oddly in 2014 there are 8 movies which do not contain domestic gross information. Further, most of these movies were released after year 2000.\n\rBox plot indicates Adventure genre have spent more range in production budget, while in perspective of MPAA rating PG-13 movies have most range in production budget with a clear outlier.\n\r\r\rFurther Analysis\r\rSimilarly we can focus on movies of world wide gross equals to zero with other variables.\n\rConduct scrutinized interest with movies of world wide gross zero and domestic gross zero.\n\r\rPlease see that This is my first post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU\n\r","date":1542153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542153600,"objectID":"ecbc820859c89f1bafbf1c1169bd0904","permalink":"/post/week_30/week-30-movie-profit/","publishdate":"2018-11-14T00:00:00Z","relpermalink":"/post/week_30/week-30-movie-profit/","section":"post","summary":"2018 Week 30 TidyTuesday: Movie Profit.","tags":["Movies","R","R package","TidyTuesday"],"title":"Week 30: Movie Profit","type":"post"},{"authors":null,"categories":null,"content":"I started reading books from month of April, 2018. Which is very useful for my life as a believer of self-learning. I hope to share these books with you so that it will be useful. There is no meaning in numbers except that they are just for ordering from 1 to further in no hierarchy. Genre specificity is not my thing therefore these books are not divided into groups.\n Rise the Dark by Michael Koryta The Bourne Enigma by Eric Van LustBader Jack Reacher thriller, A Wanted Man by Lee Child Digital Fortress by Dan Brown The Selfish Gene by Richard Dawkins How Democracies Die by Steven Levitsky and Daniel Ziblatt Elon Musk by Ashlee Vance Roll of Thunder Hear My Cry by Mildred Taylor Boy by Roald Dohl Animal Farm by George Orwell The power of Habit by Charles Duhigg  Books Related to R, Data Science and Statistics\n Text Mining with R : A Tidy Approach by Julia Silge and David Robinson R for Journalists by Andrew Ba Tran Interpretable Machine Learning by Christoph Molnar Advanced R by Hadley Wickham Modern R with the tidyverse by Bruno Rodrigues What They Forgot to Teach You About R by Jennifer Bryan, Jim Hester bookdown: Authoring Books and Technical Documents with R Markdown by Yihui Xie blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill Happy Git and GitHub for the UseR by Jenny Bryan, the STAT 545 TAs, Jim Hester  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"244490054b04706f9ddeb683c046cfbd","permalink":"/projects/books/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/books/","section":"projects","summary":"The names of books with amazon links.","tags":["Books"],"title":"Books I have read","type":"projects"},{"authors":null,"categories":null,"content":"Watching movies for the last six years(from 2012) with much interest in plot, story line and characters. I have watched more than 300 movies but the below mentioned are in my mind. They are from different decades as well, therefore do not hesistate to watch. If you prefer go through the IMDB link so that the ratings and summary will give you more information. 36 Movies will be mentioned below.\n 12 Angry Men Batman Trilogy Casablanca Dan Brown Trilogy Die Hard Quadropolgy Despicable Me Collection Dr. Strangelove Fight Club Forrest Gump Godfather Inception Interstellar Internal Affairs Ip man Lock, Stock and Two Smoking Barrels Lord of The Rings Collection Memento Mr and Mrs Smith One flew over the cuckoos nest Se7en Saving Private Ryan Shutter Island Schindlers List The Departed The Good The Bad and The Ugly The Shawshank Redemption The Silence of the Lambs The Sting The Terminal The Truman Show The Matrix Trilogy The Bourne Collection The Hobbit Collection 35 The Raid The Prestige V for Vendetta Unbreakable Unforgiven Wall-E [Venom]() [The Intouchables]()  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"e6070f9b978f4b08725e8872e93320f3","permalink":"/projects/movies/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/movies/","section":"projects","summary":"The movies wish are very brilliant in every way with their IMDB links.","tags":["Movies"],"title":"Movies","type":"projects"},{"authors":null,"categories":null,"content":"More than 50 TV shows are on my list and I have watched all of them. There are alot of shows in this list where all would agree be great and enticing. There is no specific genre of my interest and I do not mind rewatching these TV shows again.\n [1983]() A bit of Fry and Laurie Altered Carbon Band of Brothers [Barry]() Black Mirror Blue Planet I,II [BodyGuard]() BroadChurch Barry Brookynn Nine Nine Chance Condor Cosmos A space time odyssey [Dynasties]() Doctor Who Fawlty Towers Friends House House of Cards US House of Cards UK How I Met Your Mother Jeeves and Wooster Killing Eve [Koombiyo]() Line of Duty Luther [Manhunt]() Monty Python The Flying Circus [Mindhunter]() Mind Your Language Mr. Robot Narcos [Newsroom]() Not the Nine O\u0026rsquo;Clock News Ozark [Patrick Melrose]() Person of Interest Peaky Blinders Planet Earth I,II Preacher Rick and Morty [Sahodaraya]() Seinfeld Southpark Sherlock The Fresh Prince of Bel-Air The Good Place The Mentalist The Blacklist The Grand Tour The Night Manager [The Punisher]() [The Catherine Tate Show]() The Black Adder The Thin Blue Line The Wire Thick of It Tom Clancys Jack Ryan Top Gear Two and a Half Men Unforgotten Westworld Yes Minister Yes, Prime Minister  ","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"6d07d48d82d1eebc5d6cadd95c933074","permalink":"/projects/tvshows/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/projects/tvshows/","section":"projects","summary":"List of TV shows with their respective IMDB links.","tags":["TVshows"],"title":"TV Shows","type":"projects"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1538265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538265600,"objectID":"13f73cb07ffc7c1c6211e017748215fb","permalink":"/publication/fitodbod-website/","publishdate":"2018-09-30T00:00:00Z","relpermalink":"/publication/fitodbod-website/","section":"publication","summary":" Using [pkgdown](https://pkgdown.r-lib.org/) a genuine website for fitODBOD was generated. There is no extra work to be done here, because pkgdown uses our exisiting man files and vignettes to create this website. It is quie easy and convenient for a user to use this website generated for fitODBOD. Rather than the CRAN or GitHub  versions, which only provides a manual in the website there are clear and concise examples of how functions can be used for researchers","tags":["package","R","CRAN","fitODBOD","GitHub","Website"],"title":"Website-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1536969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536969600,"objectID":"65a60044eeb2d2c5c9baaf4583c37a9a","permalink":"/publication/fitodbod-github/","publishdate":"2018-09-15T00:00:00Z","relpermalink":"/publication/fitodbod-github/","section":"publication","summary":" GitHub version is there for version 1.2.0 with is maintained by me if issues are raised regarding the package. While building under CRAN restrictions some examples and vignettes were not included to submission but there are in R-fitODBOD repository. Readme file includes a badge and a small example of how to fit Binomial outcome data which cannot be modeled by Binomial distribution. Finally, if there are any issues regarding the functions or suggestions you can use the issue tab to report them to me.","tags":["R","GitHub","Package"],"title":"GitHub-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536431400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536431400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+05:30","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["M.Amalan, P.Wijekoon"],"categories":null,"content":"","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"8ead533fab86338ce8e8811de7b71ffb","permalink":"/publication/fitodbod-cran/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/publication/fitodbod-cran/","section":"publication","summary":"Binomial Outcome Data has vast amount use in the field of biology, medicine and epidemiology. Modeling such data relative to Binomial Mixture Distributions were discussed because Binomial distribution fails to model the data. The reason is actual observed variance of the data is greater than the assumed theoretical variance, which is defined as over dispersion. fitODBOD package has two versions until October 2018, but in the future there will be updates regularly each year. Whenever there are new distributions to model BOD the new versions will include these distributions. Further, if there are new techniques to make these distributions more useful they will be also added as functions. Version 1.1.0 was published in February 2018 and Version 1.20 was released in September 2018 with two new distributions.","tags":["package","R","CRAN","fitODBOD"],"title":"CRAN-fitODBOD: fitting Over Dispersed Binomial Outcome Data","type":"publication"},{"authors":["M.Amalan"],"categories":null,"content":"","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"60b94c34fc45f1e50db023bee5de657a","permalink":"/publication/4th-year/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/publication/4th-year/","section":"publication","summary":"This is a least viable product of R package for fitting Binomial outcome data, and an abstract, extended articles were written in related to this minimum viable product.","tags":["package","R","CRAN","fitODBOD"],"title":"R Package Development to Model Over Dispersed Binomial Outcome Data with the use of Binomial Mixture Distributions and Alternate Binomial Distributions.","type":"publication"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483209000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483209000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00+05:30","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]